diff --git a/experiments/learning/sample_test.py b/experiments/learning/sample_test.py
index a9edcf0..174aab5 100644
--- a/experiments/learning/sample_test.py
+++ b/experiments/learning/sample_test.py
@@ -1,121 +1,213 @@
-from __future__ import annotations
-
-import random
-import sys
-from typing import Any, Dict, Optional
-
-import gymnasium as gym
-import numpy as np
-
-from sample_factory.algo.utils.context import global_model_factory
-from sample_factory.cfg.arguments import parse_full_cfg, parse_sf_args
-from sample_factory.envs.env_utils import RewardShapingInterface, TrainingInfoInterface, register_env
-from sample_factory.train import run_rl
-from sf_examples.train_custom_env_custom_model import make_custom_encoder, override_default_params
-
-from gym_pybullet_drones.envs.BaseAviary import DroneModel, Physics
-from gym_pybullet_drones.envs.multi_agent_rl.FlockAviary import FlockAviary
-from gym_pybullet_drones.envs.multi_agent_rl.LeaderFollowerAviary import LeaderFollowerAviary
-from gym_pybullet_drones.envs.multi_agent_rl.MeetupAviary import MeetupAviary
-from gym_pybullet_drones.envs.single_agent_rl.BaseSingleAgentAviary import ActionType, ObservationType
-from gym_pybullet_drones.utils.Logger import Logger
-
-class CustomMultiEnv(LeaderFollowerAviary, TrainingInfoInterface, RewardShapingInterface):
-    """
-    Custom multi-agent environment integrating gym_pybullet_drones with Sample Factory.
-    Implements a leader-follower scenario with two drones.
-    """
-
-    def __init__(self, full_env_name, cfg, render_mode: Optional[str] = None):
-        TrainingInfoInterface.__init__(self)
-        RewardShapingInterface.__init__(self)
-
-        self.name = full_env_name  # optional
-        self.cfg = cfg
-        self.curr_episode_steps = 0
-        self.res = 8  # 8x8 images
-        self.channels = 1  # it's easier when the channel dimension is present, even if it's 1
-
-        # Define observation and action spaces
-        self.observation_space = gym.spaces.Box(0, 1, (self.channels, self.res, self.res), dtype=np.float32)
-        self.action_space = gym.spaces.Discrete(2)
-
-        self.num_agents = 2
-        self.is_multiagent = True
-
-        self.inactive_steps = [3] * self.num_agents
-
-        self.episode_rewards = [[] for _ in range(self.num_agents)]
-
-        self.reward_shaping = [dict(rew=-1.0) for _ in range(self.num_agents)]
-
-        self.obs = self._computeObs()
-
-        self.render_mode = render_mode
-
-    def _obs(self):
-        if self.obs is None:
-            self.obs = [np.float32(np.random.rand(self.channels, self.res, self.res)) for _ in range(self.num_agents)]
-        return self.obs
-
-    def reset(self, **kwargs):
-        self.curr_episode_steps = 0
-        self.episode_rewards = [[] for _ in range(self.num_agents)]
-        return self._computeObs(), [dict() for _ in range(self.num_agents)]
-
-    def step(self, action):
-        """Steps the environment with the provided action."""
-        obs, reward, terminated, truncated, info = super().step(action)
-        self.curr_episode_steps += 1
-
-        # Optionally, implement custom reward shaping here
-        # Example: Modify rewards based on some criteria
-        # for agent_idx in range(self.num_agents):
-        #     self.episode_rewards[agent_idx].append(reward[agent_idx])
-
-        return obs, reward, terminated, truncated, info
-
-    def get_default_reward_shaping(self) -> Optional[Dict[str, Any]]:
-        return self.reward_shaping[0]
-
-    def set_reward_shaping(self, reward_shaping: Dict[str, Any], agent_idx: int | slice) -> None:
-        if isinstance(agent_idx, int):
-            agent_idx = slice(agent_idx, agent_idx + 1)
-        for idx in range(agent_idx.start, agent_idx.stop):
-            self.reward_shaping[idx] = reward_shaping
-
-    def render(self, mode='human'):
-        """Renders the environment."""
-        return super().render(mode)
-
-def make_custom_multi_env_func(full_env_name, cfg=None, _env_config=None, render_mode: Optional[str] = None):
-    return CustomMultiEnv(full_env_name, cfg, render_mode=render_mode)
-
-def add_extra_params_func(parser):
-    """
-    Specify any additional command line arguments for this family of custom environments.
-    """
-    p = parser
-    p.add_argument("--custom_env_episode_len", default=10, type=int, help="Number of steps in the episode")
-
-def register_custom_components():
-    register_env("my_custom_multi_env_v1", make_custom_multi_env_func)
-    global_model_factory().register_encoder_factory(make_custom_encoder)
-
-def parse_custom_args(argv=None, evaluation=False):
-    parser, cfg = parse_sf_args(argv=argv, evaluation=evaluation)
-    add_extra_params_func(parser)
-    override_default_params(parser)
-    # Second parsing pass yields the final configuration
-    cfg = parse_full_cfg(parser, argv)
-    return cfg
-
-def main():
-    """Script entry point."""
-    register_custom_components()
-    cfg = parse_custom_args()
-    status = run_rl(cfg)
-    return status
-
-if __name__ == "__main__":
-    sys.exit(main())
+from __future__ import annotations
+
+import sys
+from typing import Any, Dict, Optional
+
+import gymnasium as gym
+import numpy as np
+import torch
+import torch.nn as nn
+from sample_factory.algo.utils.context import global_model_factory
+from sample_factory.cfg.arguments import parse_full_cfg, parse_sf_args
+from sample_factory.envs.env_utils import RewardShapingInterface, TrainingInfoInterface, register_env
+from sample_factory.train import run_rl
+from sf_examples.train_custom_env_custom_model import override_default_params
+
+from gym_pybullet_drones.envs.BaseAviary import DroneModel, Physics
+from gym_pybullet_drones.envs.multi_agent_rl.FlockAviary import FlockAviary
+from gym_pybullet_drones.envs.multi_agent_rl.LeaderFollowerAviary import LeaderFollowerAviary
+from gym_pybullet_drones.envs.multi_agent_rl.MeetupAviary import MeetupAviary
+from gym_pybullet_drones.envs.single_agent_rl.BaseSingleAgentAviary import ActionType, ObservationType
+from gym_pybullet_drones.utils.Logger import Logger
+
+class CustomEncoder(nn.Module):
+    def __init__(self, cfg, obs_space):
+        super().__init__()
+        self.conv = nn.Sequential(
+            nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1), 
+            nn.ReLU(),
+            nn.Flatten(),  
+            nn.Linear(8 * 4 * 6, 64),  
+            nn.ReLU()
+        )
+        self.output_dim = 64  
+
+    def forward(self, x):
+        """
+        Forward pass of the encoder. Extracts the observation tensor from TensorDict.
+        
+        Parameters:
+        x : TensorDict
+            Input data dictionary containing observations and other metadata.
+        """
+        obs_tensor = x["obs"]  
+        
+        obs_tensor = obs_tensor.view(-1, 1, 4, 6)
+        
+        return self.conv(obs_tensor)
+
+    
+    def get_out_size(self):
+        """
+        Return the output size (feature dimension) of the encoder.
+        This is required by Sample Factory.
+        """
+        return self.output_dim
+    def device_for_input_tensor(self, input_tensor_name):
+        """
+        Returns the device (CPU/GPU) where the input tensor should be placed.
+        This ensures compatibility with the Sample Factory framework.
+        """
+        return next(self.parameters()).device
+    def type_for_input_tensor(self, input_tensor_name):
+        """
+        Returns the data type (e.g., torch.float32) for the input tensor.
+        """
+        return torch.float32
+
+class CustomMultiEnv(LeaderFollowerAviary, TrainingInfoInterface, RewardShapingInterface):
+    """
+    Custom multi-agent environment with a leader-follower structure.
+    """
+
+    def __init__(self, full_env_name, cfg, render_mode: Optional[str] = None):
+        super().__init__(
+            drone_model=None,
+            num_drones=2,
+            neighbourhood_radius=10,
+            initial_xyzs=None,
+            initial_rpys=None,
+            physics=None,
+            freq=240,
+            aggregate_phy_steps=1,
+            gui=render_mode == "human",
+            record=False,
+        )
+
+        self.name = full_env_name
+        self.cfg = cfg
+        self.num_agents = 2
+        self.obs_dim = 12  
+        self.action_dim = 4
+
+        # Define per-agent observation bounds
+        self.obs_lower_bound = np.array([
+            -1, -1,  0,  
+            -1, -1, -1, 
+            -1, -1, -1,  
+            -1, -1, -1   
+        ])
+        self.obs_upper_bound = np.array([
+            1,  1,  1,   
+            1,  1,  1,   
+            1,  1,  1,   
+            1,  1,  1    
+        ])
+
+        # Combine bounds for two agents (12 + 12 = 24)
+        combined_obs_size = self.obs_dim * self.num_agents
+        self.combined_low = np.tile(self.obs_lower_bound, self.num_agents)  # Repeats for 2 agents
+        self.combined_high = np.tile(self.obs_upper_bound, self.num_agents)  # Repeats for 2 agents
+        self.reward_shaping: Dict[str, Any] = dict(action_rew_coeff=0.01)
+        # Correct observation space
+        self.observation_space = gym.spaces.Box(
+            low=self.combined_low,
+            high=self.combined_high,
+            dtype=np.float32
+        )
+
+        # Action space for one agent
+        self.action_space = gym.spaces.Box(
+            low=-1, high=1, shape=(self.action_dim,), dtype=np.float32
+        )
+
+
+    def step(self, action):
+        """Step function to execute actions and provide joint observations."""
+        action = np.array(action)
+
+        action_dict = {i: action[i].reshape(4,) for i in range(self.num_agents)} 
+
+        obs_dict, reward, terminated, truncated, info = super().step(action_dict)
+        obs = [
+            np.clip(np.concatenate([obs_dict[0], obs_dict[1]]), self.combined_low, self.combined_high),
+            np.clip(np.concatenate([obs_dict[1], obs_dict[0]]), self.combined_low, self.combined_high),
+        ]
+
+        reward = self._compute_rewards(obs_dict[0], obs_dict[1])
+
+        return obs, reward, terminated, truncated, info
+
+    def reset(self, **kwargs):
+        """Reset environment and return combined initial observations."""
+        self.curr_episode_steps = 0
+
+        obs_dict = super().reset()
+
+        obs_0 = obs_dict[0]  
+        obs_1 = obs_dict[1]  
+
+        combined_obs = [
+            np.clip(np.concatenate([obs_0, obs_1]), self.combined_low, self.combined_high),
+            np.clip(np.concatenate([obs_1, obs_0]), self.combined_low, self.combined_high),
+        ]
+
+        return combined_obs, {}
+
+
+    def get_default_reward_shaping(self) -> Optional[Dict[str, Any]]:
+        """
+        Provide a default reward shaping scheme.
+
+        This specifies the weights for the leader's and followers' rewards based on
+        distance to their respective target positions.
+        """
+        return {
+            "leader_target_penalty": -1.0,   
+            "follower_target_penalty": -0.5  
+        }
+    def set_reward_shaping(self, reward_shaping: Dict[str, Any], agent_idx: int | slice) -> None:
+        self.reward_shaping = reward_shaping
+
+
+def make_custom_multi_env_func(full_env_name, cfg=None, _env_config=None, render_mode: Optional[str] = None):
+    """Factory function to create the custom multi-agent environment."""
+    return CustomMultiEnv(full_env_name, cfg, render_mode=render_mode)
+
+
+def add_extra_params_func(parser):
+    """
+    Specify any additional command line arguments for this family of custom environments.
+    """
+    p = parser
+    p.add_argument("--custom_env_episode_len", default=10, type=int, help="Number of steps in the episode")
+
+def make_custom_encoder(cfg, obs_space):
+    """Factory function to create the CustomEncoder."""
+    return CustomEncoder(cfg, obs_space)
+
+def register_custom_components():
+    register_env("my_custom_multi_env_v1", make_custom_multi_env_func)
+    global_model_factory().register_encoder_factory(make_custom_encoder)
+
+
+
+def parse_custom_args(argv=None, evaluation=False):
+    """Parse custom arguments."""
+    parser, cfg = parse_sf_args(argv=argv, evaluation=evaluation)
+    add_extra_params_func(parser)
+    override_default_params(parser)
+    cfg = parse_full_cfg(parser, argv)
+    return cfg
+
+
+def main():
+    """Script entry point."""
+    register_custom_components()
+    cfg = parse_custom_args()
+    status = run_rl(cfg)
+    return status
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff --git a/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733767326.LAPTOP-S6TM0USI b/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733767326.LAPTOP-S6TM0USI
deleted file mode 100644
index 5d844d5..0000000
Binary files a/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733767326.LAPTOP-S6TM0USI and /dev/null differ
diff --git a/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733767367.LAPTOP-S6TM0USI b/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733767367.LAPTOP-S6TM0USI
deleted file mode 100644
index 291f862..0000000
Binary files a/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733767367.LAPTOP-S6TM0USI and /dev/null differ
diff --git a/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733767519.LAPTOP-S6TM0USI b/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733767519.LAPTOP-S6TM0USI
deleted file mode 100644
index 8c797bd..0000000
Binary files a/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733767519.LAPTOP-S6TM0USI and /dev/null differ
diff --git a/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733768525.LAPTOP-S6TM0USI b/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733768525.LAPTOP-S6TM0USI
deleted file mode 100644
index b27b55a..0000000
Binary files a/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733768525.LAPTOP-S6TM0USI and /dev/null differ
diff --git a/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733768618.LAPTOP-S6TM0USI b/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733768618.LAPTOP-S6TM0USI
deleted file mode 100644
index bc9bf1d..0000000
Binary files a/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733768618.LAPTOP-S6TM0USI and /dev/null differ
diff --git a/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733768944.LAPTOP-S6TM0USI b/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733768944.LAPTOP-S6TM0USI
deleted file mode 100644
index f5bbe75..0000000
Binary files a/experiments/learning/train_dir/example_multi/.summary/0/events.out.tfevents.1733768944.LAPTOP-S6TM0USI and /dev/null differ
diff --git a/experiments/learning/train_dir/example_multi/checkpoint_p0/best_000000601_615424_reward_0.000.pth b/experiments/learning/train_dir/example_multi/checkpoint_p0/best_000000601_615424_reward_0.000.pth
deleted file mode 100644
index 353c107..0000000
Binary files a/experiments/learning/train_dir/example_multi/checkpoint_p0/best_000000601_615424_reward_0.000.pth and /dev/null differ
diff --git a/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000001117_1143808.pth b/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000001117_1143808.pth
deleted file mode 100644
index 56e58db..0000000
Binary files a/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000001117_1143808.pth and /dev/null differ
diff --git a/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000001119_1145856.pth b/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000001119_1145856.pth
deleted file mode 100644
index de2b33f..0000000
Binary files a/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000001119_1145856.pth and /dev/null differ
diff --git a/experiments/learning/train_dir/example_multi/config.json b/experiments/learning/train_dir/example_multi/config.json
deleted file mode 100644
index 650bdf5..0000000
--- a/experiments/learning/train_dir/example_multi/config.json
+++ /dev/null
@@ -1,134 +0,0 @@
-{
-  "help": false,
-  "algo": "APPO",
-  "env": "my_custom_multi_env_v1",
-  "experiment": "example_multi",
-  "train_dir": "/mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir",
-  "restart_behavior": "resume",
-  "device": "cpu",
-  "seed": null,
-  "num_policies": 1,
-  "async_rl": true,
-  "serial_mode": false,
-  "batched_sampling": false,
-  "num_batches_to_accumulate": 2,
-  "worker_num_splits": 2,
-  "policy_workers_per_policy": 1,
-  "max_policy_lag": 1000,
-  "num_workers": 16,
-  "num_envs_per_worker": 2,
-  "batch_size": 1024,
-  "num_batches_per_epoch": 1,
-  "num_epochs": 1,
-  "rollout": 32,
-  "recurrence": 32,
-  "shuffle_minibatches": false,
-  "gamma": 0.99,
-  "reward_scale": 1.0,
-  "reward_clip": 1000.0,
-  "value_bootstrap": false,
-  "normalize_returns": true,
-  "exploration_loss_coeff": 0.003,
-  "value_loss_coeff": 0.5,
-  "kl_loss_coeff": 0.0,
-  "exploration_loss": "entropy",
-  "gae_lambda": 0.95,
-  "ppo_clip_ratio": 0.1,
-  "ppo_clip_value": 1.0,
-  "with_vtrace": false,
-  "vtrace_rho": 1.0,
-  "vtrace_c": 1.0,
-  "optimizer": "adam",
-  "adam_eps": 1e-06,
-  "adam_beta1": 0.9,
-  "adam_beta2": 0.999,
-  "max_grad_norm": 4.0,
-  "learning_rate": 0.0001,
-  "lr_schedule": "constant",
-  "lr_schedule_kl_threshold": 0.008,
-  "lr_adaptive_min": 1e-06,
-  "lr_adaptive_max": 0.01,
-  "obs_subtract_mean": 0.0,
-  "obs_scale": 1.0,
-  "normalize_input": true,
-  "normalize_input_keys": null,
-  "decorrelate_experience_max_seconds": 0,
-  "decorrelate_envs_on_one_worker": true,
-  "actor_worker_gpus": [],
-  "set_workers_cpu_affinity": true,
-  "force_envs_single_thread": false,
-  "default_niceness": 0,
-  "log_to_file": true,
-  "experiment_summaries_interval": 10,
-  "flush_summaries_interval": 30,
-  "stats_avg": 100,
-  "summaries_use_frameskip": true,
-  "heartbeat_interval": 20,
-  "heartbeat_reporting_interval": 180,
-  "train_for_env_steps": 10000000000,
-  "train_for_seconds": 10000000000,
-  "save_every_sec": 5,
-  "keep_checkpoints": 2,
-  "load_checkpoint_kind": "latest",
-  "save_milestones_sec": -1,
-  "save_best_every_sec": 5,
-  "save_best_metric": "reward",
-  "save_best_after": 100000,
-  "benchmark": false,
-  "encoder_mlp_layers": [
-    512,
-    512
-  ],
-  "encoder_conv_architecture": "convnet_simple",
-  "encoder_conv_mlp_layers": [
-    512
-  ],
-  "use_rnn": true,
-  "rnn_size": 128,
-  "rnn_type": "gru",
-  "rnn_num_layers": 1,
-  "decoder_mlp_layers": [],
-  "nonlinearity": "elu",
-  "policy_initialization": "orthogonal",
-  "policy_init_gain": 1.0,
-  "actor_critic_share_weights": true,
-  "adaptive_stddev": true,
-  "continuous_tanh_scale": 0.0,
-  "initial_stddev": 1.0,
-  "use_env_info_cache": false,
-  "env_gpu_actions": false,
-  "env_gpu_observations": true,
-  "env_frameskip": 1,
-  "env_framestack": 1,
-  "pixel_format": "CHW",
-  "use_record_episode_statistics": false,
-  "with_wandb": false,
-  "wandb_user": null,
-  "wandb_project": "sample_factory",
-  "wandb_group": null,
-  "wandb_job_type": "SF",
-  "wandb_tags": [],
-  "with_pbt": false,
-  "pbt_mix_policies_in_one_env": true,
-  "pbt_period_env_steps": 5000000,
-  "pbt_start_mutation": 20000000,
-  "pbt_replace_fraction": 0.3,
-  "pbt_mutation_rate": 0.15,
-  "pbt_replace_reward_gap": 0.1,
-  "pbt_replace_reward_gap_absolute": 1e-06,
-  "pbt_optimize_gamma": false,
-  "pbt_target_objective": "true_objective",
-  "pbt_perturb_min": 1.1,
-  "pbt_perturb_max": 1.5,
-  "custom_env_episode_len": 10,
-  "command_line": "--algo=APPO --env=my_custom_multi_env_v1 --experiment=example_multi --save_every_sec=5 --experiment_summaries_interval=10",
-  "cli_args": {
-    "algo": "APPO",
-    "env": "my_custom_multi_env_v1",
-    "experiment": "example_multi",
-    "experiment_summaries_interval": 10,
-    "save_every_sec": 5
-  },
-  "git_hash": "24cfdf341e61dae08bbb625761f4cee0699e23d0",
-  "git_repo_name": "https://github.com/DanielCarneiro123/gym-pybullet-drones.git"
-}
\ No newline at end of file
diff --git a/experiments/learning/train_dir/example_multi/git.diff b/experiments/learning/train_dir/example_multi/git.diff
deleted file mode 100644
index e69de29..0000000
diff --git a/experiments/learning/train_dir/example_multi/sf_log.txt b/experiments/learning/train_dir/example_multi/sf_log.txt
deleted file mode 100644
index f71f19a..0000000
--- a/experiments/learning/train_dir/example_multi/sf_log.txt
+++ /dev/null
@@ -1,1306 +0,0 @@
-[2024-12-09 18:02:14,908][11026] Saving configuration to /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/config.json...
-[2024-12-09 18:02:15,948][11026] Rollout worker 0 uses device cpu
-[2024-12-09 18:02:15,949][11026] Rollout worker 1 uses device cpu
-[2024-12-09 18:02:15,950][11026] Rollout worker 2 uses device cpu
-[2024-12-09 18:02:15,951][11026] Rollout worker 3 uses device cpu
-[2024-12-09 18:02:15,952][11026] Rollout worker 4 uses device cpu
-[2024-12-09 18:02:15,953][11026] Rollout worker 5 uses device cpu
-[2024-12-09 18:02:15,954][11026] Rollout worker 6 uses device cpu
-[2024-12-09 18:02:15,955][11026] Rollout worker 7 uses device cpu
-[2024-12-09 18:02:15,957][11026] Rollout worker 8 uses device cpu
-[2024-12-09 18:02:15,959][11026] Rollout worker 9 uses device cpu
-[2024-12-09 18:02:15,961][11026] Rollout worker 10 uses device cpu
-[2024-12-09 18:02:15,962][11026] Rollout worker 11 uses device cpu
-[2024-12-09 18:02:15,963][11026] Rollout worker 12 uses device cpu
-[2024-12-09 18:02:15,964][11026] Rollout worker 13 uses device cpu
-[2024-12-09 18:02:15,965][11026] Rollout worker 14 uses device cpu
-[2024-12-09 18:02:15,966][11026] Rollout worker 15 uses device cpu
-[2024-12-09 18:02:55,611][11276] Saving configuration to /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/config.json...
-[2024-12-09 18:02:56,643][11276] Rollout worker 0 uses device cpu
-[2024-12-09 18:02:56,645][11276] Rollout worker 1 uses device cpu
-[2024-12-09 18:02:56,646][11276] Rollout worker 2 uses device cpu
-[2024-12-09 18:02:56,647][11276] Rollout worker 3 uses device cpu
-[2024-12-09 18:02:56,648][11276] Rollout worker 4 uses device cpu
-[2024-12-09 18:02:56,649][11276] Rollout worker 5 uses device cpu
-[2024-12-09 18:02:56,651][11276] Rollout worker 6 uses device cpu
-[2024-12-09 18:02:56,652][11276] Rollout worker 7 uses device cpu
-[2024-12-09 18:02:56,654][11276] Rollout worker 8 uses device cpu
-[2024-12-09 18:02:56,655][11276] Rollout worker 9 uses device cpu
-[2024-12-09 18:02:56,656][11276] Rollout worker 10 uses device cpu
-[2024-12-09 18:02:56,657][11276] Rollout worker 11 uses device cpu
-[2024-12-09 18:02:56,659][11276] Rollout worker 12 uses device cpu
-[2024-12-09 18:02:56,660][11276] Rollout worker 13 uses device cpu
-[2024-12-09 18:02:56,661][11276] Rollout worker 14 uses device cpu
-[2024-12-09 18:02:56,662][11276] Rollout worker 15 uses device cpu
-[2024-12-09 18:05:24,459][11982] Saving configuration to /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/config.json...
-[2024-12-09 18:05:25,505][11982] Rollout worker 0 uses device cpu
-[2024-12-09 18:05:25,506][11982] Rollout worker 1 uses device cpu
-[2024-12-09 18:05:25,508][11982] Rollout worker 2 uses device cpu
-[2024-12-09 18:05:25,509][11982] Rollout worker 3 uses device cpu
-[2024-12-09 18:05:25,510][11982] Rollout worker 4 uses device cpu
-[2024-12-09 18:05:25,511][11982] Rollout worker 5 uses device cpu
-[2024-12-09 18:05:25,513][11982] Rollout worker 6 uses device cpu
-[2024-12-09 18:05:25,514][11982] Rollout worker 7 uses device cpu
-[2024-12-09 18:05:25,515][11982] Rollout worker 8 uses device cpu
-[2024-12-09 18:05:25,517][11982] Rollout worker 9 uses device cpu
-[2024-12-09 18:05:25,518][11982] Rollout worker 10 uses device cpu
-[2024-12-09 18:05:25,520][11982] Rollout worker 11 uses device cpu
-[2024-12-09 18:05:25,522][11982] Rollout worker 12 uses device cpu
-[2024-12-09 18:05:25,524][11982] Rollout worker 13 uses device cpu
-[2024-12-09 18:05:25,525][11982] Rollout worker 14 uses device cpu
-[2024-12-09 18:05:25,528][11982] Rollout worker 15 uses device cpu
-[2024-12-09 18:05:25,620][11982] InferenceWorker_p0-w0: min num requests: 5
-[2024-12-09 18:05:25,723][11982] Starting all processes...
-[2024-12-09 18:05:25,725][11982] Starting process learner_proc0
-[2024-12-09 18:05:26,860][11982] Starting all processes...
-[2024-12-09 18:05:26,888][11982] Starting process inference_proc0-0
-[2024-12-09 18:05:26,888][11982] Starting process rollout_proc0
-[2024-12-09 18:05:26,889][11982] Starting process rollout_proc1
-[2024-12-09 18:05:26,890][11982] Starting process rollout_proc2
-[2024-12-09 18:05:26,890][11982] Starting process rollout_proc3
-[2024-12-09 18:05:26,891][11982] Starting process rollout_proc4
-[2024-12-09 18:05:26,892][11982] Starting process rollout_proc5
-[2024-12-09 18:05:26,900][11982] Starting process rollout_proc6
-[2024-12-09 18:05:26,900][11982] Starting process rollout_proc7
-[2024-12-09 18:05:26,901][11982] Starting process rollout_proc8
-[2024-12-09 18:05:26,907][11982] Starting process rollout_proc9
-[2024-12-09 18:05:26,907][11982] Starting process rollout_proc10
-[2024-12-09 18:05:26,909][11982] Starting process rollout_proc11
-[2024-12-09 18:05:26,909][11982] Starting process rollout_proc12
-[2024-12-09 18:05:26,910][11982] Starting process rollout_proc13
-[2024-12-09 18:05:26,911][11982] Starting process rollout_proc14
-[2024-12-09 18:05:26,912][11982] Starting process rollout_proc15
-[2024-12-09 18:05:36,976][12103] Worker 4 uses CPU cores [4]
-[2024-12-09 18:05:37,065][12100] Worker 1 uses CPU cores [1]
-[2024-12-09 18:05:37,094][12102] Worker 3 uses CPU cores [3]
-[2024-12-09 18:05:37,179][12099] Worker 0 uses CPU cores [0]
-[2024-12-09 18:05:37,222][12105] Worker 6 uses CPU cores [6]
-[2024-12-09 18:05:37,232][12111] Worker 12 uses CPU cores [12]
-[2024-12-09 18:05:37,292][12115] Worker 15 uses CPU cores [15]
-[2024-12-09 18:05:37,295][12101] Worker 2 uses CPU cores [2]
-[2024-12-09 18:05:37,311][12106] Worker 8 uses CPU cores [8]
-[2024-12-09 18:05:37,345][12114] Worker 13 uses CPU cores [13]
-[2024-12-09 18:05:37,353][12113] Worker 14 uses CPU cores [14]
-[2024-12-09 18:05:37,388][12109] Worker 9 uses CPU cores [9]
-[2024-12-09 18:05:37,502][12104] Worker 5 uses CPU cores [5]
-[2024-12-09 18:05:37,527][12078] Starting seed is not provided
-[2024-12-09 18:05:37,528][12078] Initializing actor-critic model on device cpu
-[2024-12-09 18:05:37,530][12078] RunningMeanStd input shape: (1, 8, 8)
-[2024-12-09 18:05:37,541][12078] RunningMeanStd input shape: (1,)
-[2024-12-09 18:05:37,552][12108] Worker 7 uses CPU cores [7]
-[2024-12-09 18:05:37,646][12110] Worker 10 uses CPU cores [10]
-[2024-12-09 18:05:37,695][12078] Created Actor Critic model with architecture:
-[2024-12-09 18:05:37,696][12078] ActorCriticSharedWeights(
-  (obs_normalizer): ObservationNormalizer(
-    (running_mean_std): RunningMeanStdDictInPlace(
-      (running_mean_std): ModuleDict(
-        (obs): RunningMeanStdInPlace()
-      )
-    )
-  )
-  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
-  (encoder): CustomEncoder(
-    (conv_head): Sequential(
-      (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2))
-      (1): ELU(alpha=1.0)
-      (2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1))
-      (3): ELU(alpha=1.0)
-    )
-  )
-  (core): ModelCoreRNN(
-    (core): GRU(64, 128)
-  )
-  (decoder): MlpDecoder(
-    (mlp): Identity()
-  )
-  (critic_linear): Linear(in_features=128, out_features=1, bias=True)
-  (action_parameterization): ActionParameterizationDefault(
-    (distribution_linear): Linear(in_features=128, out_features=2, bias=True)
-  )
-)
-[2024-12-09 18:05:37,756][12112] Worker 11 uses CPU cores [11]
-[2024-12-09 18:05:38,524][12078] Using optimizer <class 'torch.optim.adam.Adam'>
-[2024-12-09 18:05:40,618][12078] No checkpoints found
-[2024-12-09 18:05:40,619][12078] Did not load from checkpoint, starting from scratch!
-[2024-12-09 18:05:40,621][12078] Initialized policy 0 weights for model version 0
-[2024-12-09 18:05:40,626][12078] LearnerWorker_p0 finished initialization!
-[2024-12-09 18:05:40,632][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000000_0.pth...
-[2024-12-09 18:05:40,636][12098] RunningMeanStd input shape: (1, 8, 8)
-[2024-12-09 18:05:40,642][12098] RunningMeanStd input shape: (1,)
-[2024-12-09 18:05:40,671][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000000_0.pth...
-[2024-12-09 18:05:40,684][11982] Inference worker 0-0 is ready!
-[2024-12-09 18:05:40,686][11982] All inference workers are ready! Signal rollout workers to start!
-[2024-12-09 18:05:40,695][12101] Decorrelating experience for 0 frames...
-[2024-12-09 18:05:40,695][12106] Decorrelating experience for 0 frames...
-[2024-12-09 18:05:40,695][12103] Decorrelating experience for 0 frames...
-[2024-12-09 18:05:40,696][12108] Decorrelating experience for 0 frames...
-[2024-12-09 18:05:40,696][12111] Decorrelating experience for 0 frames...
-[2024-12-09 18:05:40,696][12104] Decorrelating experience for 0 frames...
-[2024-12-09 18:05:40,696][12102] Decorrelating experience for 0 frames...
-[2024-12-09 18:05:40,696][12109] Decorrelating experience for 0 frames...
-[2024-12-09 18:05:40,696][12113] Decorrelating experience for 0 frames...
-[2024-12-09 18:05:40,696][12099] Decorrelating experience for 0 frames...
-[2024-12-09 18:05:40,700][12104] Decorrelating experience for 32 frames...
-[2024-12-09 18:05:40,700][12102] Decorrelating experience for 32 frames...
-[2024-12-09 18:05:40,701][12114] Decorrelating experience for 0 frames...
-[2024-12-09 18:05:40,701][12115] Decorrelating experience for 0 frames...
-[2024-12-09 18:05:40,702][12112] Decorrelating experience for 0 frames...
-[2024-12-09 18:05:40,703][12109] Decorrelating experience for 32 frames...
-[2024-12-09 18:05:40,704][12111] Decorrelating experience for 32 frames...
-[2024-12-09 18:05:40,704][12106] Decorrelating experience for 32 frames...
-[2024-12-09 18:05:40,704][12108] Decorrelating experience for 32 frames...
-[2024-12-09 18:05:40,705][12110] Decorrelating experience for 0 frames...
-[2024-12-09 18:05:40,706][12105] Decorrelating experience for 0 frames...
-[2024-12-09 18:05:40,707][12113] Decorrelating experience for 32 frames...
-[2024-12-09 18:05:40,708][12100] Decorrelating experience for 0 frames...
-[2024-12-09 18:05:40,710][12099] Decorrelating experience for 32 frames...
-[2024-12-09 18:05:40,710][12114] Decorrelating experience for 32 frames...
-[2024-12-09 18:05:40,711][12101] Decorrelating experience for 32 frames...
-[2024-12-09 18:05:40,711][12112] Decorrelating experience for 32 frames...
-[2024-12-09 18:05:40,713][12115] Decorrelating experience for 32 frames...
-[2024-12-09 18:05:40,713][12105] Decorrelating experience for 32 frames...
-[2024-12-09 18:05:40,714][12103] Decorrelating experience for 32 frames...
-[2024-12-09 18:05:40,714][12110] Decorrelating experience for 32 frames...
-[2024-12-09 18:05:40,723][12100] Decorrelating experience for 32 frames...
-[2024-12-09 18:05:40,744][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000000_0.pth...
-[2024-12-09 18:05:40,780][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000000_0.pth...
-[2024-12-09 18:05:40,806][12102] Worker 3-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:40,806][12104] Worker 5-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:40,806][12109] Worker 9-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:40,806][12111] Worker 12-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:40,806][12108] Worker 7-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:40,807][12113] Worker 14-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:40,813][12113] Worker 14-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:40,814][12114] Worker 13-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:40,814][12103] Worker 4-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:40,814][12112] Worker 11-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:40,814][12105] Worker 6-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:40,814][12115] Worker 15-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:40,816][12101] Worker 2-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:40,816][12110] Worker 10-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:40,816][12099] Worker 0-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:40,817][12100] Worker 1-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:40,823][12106] Worker 8-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:40,971][12113] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:05:41,086][12104] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:05:41,092][12108] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:05:41,099][12112] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:05:41,099][12115] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:05:41,101][12103] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:05:41,107][12109] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:05:41,107][12102] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:05:41,107][12106] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:05:41,107][12111] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:05:41,112][12110] Worker 10-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:41,113][12114] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:05:41,113][12105] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:05:41,115][12110] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:05:41,120][12099] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:05:41,128][12101] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:05:41,144][12100] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:05:41,170][12078] Signal inference workers to stop experience collection...
-[2024-12-09 18:05:41,182][12098] InferenceWorker_p0-w0: stopping experience collection
-[2024-12-09 18:05:41,399][12078] Signal inference workers to resume experience collection...
-[2024-12-09 18:05:41,401][12098] InferenceWorker_p0-w0: resuming experience collection
-[2024-12-09 18:05:41,879][12111] Worker 12-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:42,059][12102] Worker 3-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:42,568][12103] Worker 4-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:42,893][12098] Updated weights for policy 0, policy_version 10 (0.0016)
-[2024-12-09 18:05:43,122][12101] Worker 2-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:44,439][12098] Updated weights for policy 0, policy_version 20 (0.0011)
-[2024-12-09 18:05:44,688][11982] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 21504. Throughput: 0: nan. Samples: 15243. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)
-[2024-12-09 18:05:44,690][11982] Avg episode reward: [(0, '-5.050')]
-[2024-12-09 18:05:44,751][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000022_22528.pth...
-[2024-12-09 18:05:45,173][12104] Worker 5-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:45,602][11982] Heartbeat connected on Batcher_0
-[2024-12-09 18:05:45,609][11982] Heartbeat connected on LearnerWorker_p0
-[2024-12-09 18:05:45,627][11982] Heartbeat connected on InferenceWorker_p0-w0
-[2024-12-09 18:05:45,629][11982] Heartbeat connected on RolloutWorker_w0
-[2024-12-09 18:05:45,634][11982] Heartbeat connected on RolloutWorker_w1
-[2024-12-09 18:05:45,640][11982] Heartbeat connected on RolloutWorker_w2
-[2024-12-09 18:05:45,647][11982] Heartbeat connected on RolloutWorker_w3
-[2024-12-09 18:05:45,653][11982] Heartbeat connected on RolloutWorker_w4
-[2024-12-09 18:05:45,659][11982] Heartbeat connected on RolloutWorker_w5
-[2024-12-09 18:05:45,665][11982] Heartbeat connected on RolloutWorker_w6
-[2024-12-09 18:05:45,671][11982] Heartbeat connected on RolloutWorker_w7
-[2024-12-09 18:05:45,677][11982] Heartbeat connected on RolloutWorker_w8
-[2024-12-09 18:05:45,683][11982] Heartbeat connected on RolloutWorker_w9
-[2024-12-09 18:05:45,689][11982] Heartbeat connected on RolloutWorker_w10
-[2024-12-09 18:05:45,696][11982] Heartbeat connected on RolloutWorker_w11
-[2024-12-09 18:05:45,702][11982] Heartbeat connected on RolloutWorker_w12
-[2024-12-09 18:05:45,708][11982] Heartbeat connected on RolloutWorker_w13
-[2024-12-09 18:05:45,714][11982] Heartbeat connected on RolloutWorker_w14
-[2024-12-09 18:05:45,720][11982] Heartbeat connected on RolloutWorker_w15
-[2024-12-09 18:05:46,015][12098] Updated weights for policy 0, policy_version 30 (0.0012)
-[2024-12-09 18:05:47,559][12098] Updated weights for policy 0, policy_version 40 (0.0012)
-[2024-12-09 18:05:47,980][12100] Worker 1-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:48,650][12114] Worker 13-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:49,094][12098] Updated weights for policy 0, policy_version 50 (0.0013)
-[2024-12-09 18:05:49,280][12109] Worker 9-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:49,589][12112] Worker 11-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:49,689][11982] Fps is (10 sec: 6348.5, 60 sec: 6348.5, 300 sec: 6348.5). Total num frames: 53248. Throughput: 0: 3627.5. Samples: 33381. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:05:49,691][11982] Avg episode reward: [(0, '-4.020')]
-[2024-12-09 18:05:49,754][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000053_54272.pth...
-[2024-12-09 18:05:49,821][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000000_0.pth
-[2024-12-09 18:05:50,907][12098] Updated weights for policy 0, policy_version 60 (0.0013)
-[2024-12-09 18:05:52,474][12098] Updated weights for policy 0, policy_version 70 (0.0012)
-[2024-12-09 18:05:53,783][12106] Worker 8-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:54,019][12098] Updated weights for policy 0, policy_version 80 (0.0011)
-[2024-12-09 18:05:54,688][11982] Fps is (10 sec: 6451.2, 60 sec: 6451.2, 300 sec: 6451.2). Total num frames: 86016. Throughput: 0: 5230.8. Samples: 67551. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:05:54,690][11982] Avg episode reward: [(0, '-4.250')]
-[2024-12-09 18:05:54,695][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000084_86016.pth...
-[2024-12-09 18:05:54,761][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000022_22528.pth
-[2024-12-09 18:05:55,574][12098] Updated weights for policy 0, policy_version 90 (0.0011)
-[2024-12-09 18:05:56,051][12115] Worker 15-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:56,147][12108] Worker 7-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:57,085][12098] Updated weights for policy 0, policy_version 100 (0.0012)
-[2024-12-09 18:05:58,108][12105] Worker 6-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:05:58,668][12098] Updated weights for policy 0, policy_version 110 (0.0012)
-[2024-12-09 18:05:59,689][11982] Fps is (10 sec: 6553.7, 60 sec: 6485.3, 300 sec: 6485.3). Total num frames: 118784. Throughput: 0: 5882.1. Samples: 103475. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:05:59,690][11982] Avg episode reward: [(0, '-4.050')]
-[2024-12-09 18:05:59,702][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000117_119808.pth...
-[2024-12-09 18:05:59,769][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000053_54272.pth
-[2024-12-09 18:05:59,780][12078] Saving new best policy, reward=-4.050!
-[2024-12-09 18:06:00,143][12098] Updated weights for policy 0, policy_version 120 (0.0011)
-[2024-12-09 18:06:01,130][12099] Worker 0-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:06:01,740][12098] Updated weights for policy 0, policy_version 130 (0.0010)
-[2024-12-09 18:06:03,271][12098] Updated weights for policy 0, policy_version 140 (0.0011)
-[2024-12-09 18:06:04,689][11982] Fps is (10 sec: 6655.9, 60 sec: 6553.6, 300 sec: 6553.6). Total num frames: 152576. Throughput: 0: 5324.6. Samples: 121736. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)
-[2024-12-09 18:06:04,690][11982] Avg episode reward: [(0, '-2.530')]
-[2024-12-09 18:06:04,695][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000149_152576.pth...
-[2024-12-09 18:06:04,760][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000084_86016.pth
-[2024-12-09 18:06:04,772][12078] Saving new best policy, reward=-2.530!
-[2024-12-09 18:06:04,917][12098] Updated weights for policy 0, policy_version 150 (0.0012)
-[2024-12-09 18:06:06,339][12098] Updated weights for policy 0, policy_version 160 (0.0012)
-[2024-12-09 18:06:06,866][12111] Worker 12-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:06:07,858][12098] Updated weights for policy 0, policy_version 170 (0.0013)
-[2024-12-09 18:06:09,360][12098] Updated weights for policy 0, policy_version 180 (0.0012)
-[2024-12-09 18:06:09,689][11982] Fps is (10 sec: 6758.2, 60 sec: 6594.5, 300 sec: 6594.5). Total num frames: 186368. Throughput: 0: 5697.1. Samples: 157673. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:06:09,690][11982] Avg episode reward: [(0, '-1.780')]
-[2024-12-09 18:06:09,704][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000182_186368.pth...
-[2024-12-09 18:06:09,765][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000117_119808.pth
-[2024-12-09 18:06:09,779][12078] Saving new best policy, reward=-1.730!
-[2024-12-09 18:06:10,906][12098] Updated weights for policy 0, policy_version 190 (0.0010)
-[2024-12-09 18:06:12,462][12098] Updated weights for policy 0, policy_version 200 (0.0012)
-[2024-12-09 18:06:13,999][12098] Updated weights for policy 0, policy_version 210 (0.0011)
-[2024-12-09 18:06:14,689][11982] Fps is (10 sec: 6655.9, 60 sec: 6587.7, 300 sec: 6587.7). Total num frames: 219136. Throughput: 0: 5936.6. Samples: 193344. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:06:14,690][11982] Avg episode reward: [(0, '-1.100')]
-[2024-12-09 18:06:14,696][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000214_219136.pth...
-[2024-12-09 18:06:14,759][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000149_152576.pth
-[2024-12-09 18:06:14,770][12078] Saving new best policy, reward=-1.100!
-[2024-12-09 18:06:15,549][12098] Updated weights for policy 0, policy_version 220 (0.0012)
-[2024-12-09 18:06:17,094][12103] Worker 4-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:06:17,110][12098] Updated weights for policy 0, policy_version 230 (0.0012)
-[2024-12-09 18:06:18,599][12098] Updated weights for policy 0, policy_version 240 (0.0012)
-[2024-12-09 18:06:19,689][11982] Fps is (10 sec: 6553.8, 60 sec: 6582.8, 300 sec: 6582.8). Total num frames: 251904. Throughput: 0: 5605.8. Samples: 211446. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:06:19,691][11982] Avg episode reward: [(0, '-0.930')]
-[2024-12-09 18:06:19,712][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000247_252928.pth...
-[2024-12-09 18:06:19,771][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000182_186368.pth
-[2024-12-09 18:06:19,783][12078] Saving new best policy, reward=-0.930!
-[2024-12-09 18:06:20,211][12098] Updated weights for policy 0, policy_version 250 (0.0012)
-[2024-12-09 18:06:21,702][12098] Updated weights for policy 0, policy_version 260 (0.0011)
-[2024-12-09 18:06:23,217][12098] Updated weights for policy 0, policy_version 270 (0.0011)
-[2024-12-09 18:06:24,688][11982] Fps is (10 sec: 6656.4, 60 sec: 6604.8, 300 sec: 6604.8). Total num frames: 285696. Throughput: 0: 5826.7. Samples: 248311. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:06:24,689][11982] Avg episode reward: [(0, '-0.520')]
-[2024-12-09 18:06:24,695][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000279_285696.pth...
-[2024-12-09 18:06:24,756][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000214_219136.pth
-[2024-12-09 18:06:24,768][12078] Saving new best policy, reward=-0.520!
-[2024-12-09 18:06:24,903][12098] Updated weights for policy 0, policy_version 280 (0.0012)
-[2024-12-09 18:06:26,337][12098] Updated weights for policy 0, policy_version 290 (0.0012)
-[2024-12-09 18:06:27,892][12098] Updated weights for policy 0, policy_version 300 (0.0012)
-[2024-12-09 18:06:29,459][12098] Updated weights for policy 0, policy_version 310 (0.0012)
-[2024-12-09 18:06:29,689][11982] Fps is (10 sec: 6656.1, 60 sec: 6599.1, 300 sec: 6599.1). Total num frames: 318464. Throughput: 0: 5977.9. Samples: 284251. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:06:29,690][11982] Avg episode reward: [(0, '-0.490')]
-[2024-12-09 18:06:29,703][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000311_318464.pth...
-[2024-12-09 18:06:29,767][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000247_252928.pth
-[2024-12-09 18:06:29,781][12078] Saving new best policy, reward=-0.490!
-[2024-12-09 18:06:31,105][12098] Updated weights for policy 0, policy_version 320 (0.0012)
-[2024-12-09 18:06:32,653][12098] Updated weights for policy 0, policy_version 330 (0.0012)
-[2024-12-09 18:06:34,214][12098] Updated weights for policy 0, policy_version 340 (0.0012)
-[2024-12-09 18:06:34,689][11982] Fps is (10 sec: 6450.9, 60 sec: 6574.0, 300 sec: 6574.0). Total num frames: 350208. Throughput: 0: 5960.6. Samples: 301607. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)
-[2024-12-09 18:06:34,691][11982] Avg episode reward: [(0, '-0.370')]
-[2024-12-09 18:06:34,713][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000343_351232.pth...
-[2024-12-09 18:06:34,794][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000279_285696.pth
-[2024-12-09 18:06:34,812][12078] Saving new best policy, reward=-0.370!
-[2024-12-09 18:06:35,832][12101] Worker 2-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:06:35,894][12098] Updated weights for policy 0, policy_version 350 (0.0012)
-[2024-12-09 18:06:37,230][12110] Worker 10-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:06:37,557][12098] Updated weights for policy 0, policy_version 360 (0.0013)
-[2024-12-09 18:06:37,944][12111] Worker 12-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:06:38,356][12101] Worker 2-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:06:39,064][12098] Updated weights for policy 0, policy_version 370 (0.0011)
-[2024-12-09 18:06:39,707][11982] Fps is (10 sec: 6439.6, 60 sec: 6570.1, 300 sec: 6570.1). Total num frames: 382976. Throughput: 0: 5958.9. Samples: 335809. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:06:39,709][11982] Avg episode reward: [(0, '-0.420')]
-[2024-12-09 18:06:39,722][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000374_382976.pth...
-[2024-12-09 18:06:39,781][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000311_318464.pth
-[2024-12-09 18:06:40,364][12105] Worker 6-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:06:40,613][12098] Updated weights for policy 0, policy_version 380 (0.0012)
-[2024-12-09 18:06:42,168][12098] Updated weights for policy 0, policy_version 390 (0.0012)
-[2024-12-09 18:06:43,501][12108] Worker 7-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:06:43,693][12098] Updated weights for policy 0, policy_version 400 (0.0012)
-[2024-12-09 18:06:44,689][11982] Fps is (10 sec: 6553.7, 60 sec: 6570.7, 300 sec: 6570.7). Total num frames: 415744. Throughput: 0: 5968.2. Samples: 372043. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:06:44,690][11982] Avg episode reward: [(0, '-0.200')]
-[2024-12-09 18:06:44,696][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000406_415744.pth...
-[2024-12-09 18:06:44,763][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000343_351232.pth
-[2024-12-09 18:06:44,776][12078] Saving new best policy, reward=-0.200!
-[2024-12-09 18:06:45,270][12098] Updated weights for policy 0, policy_version 410 (0.0011)
-[2024-12-09 18:06:46,819][12098] Updated weights for policy 0, policy_version 420 (0.0011)
-[2024-12-09 18:06:47,565][12114] Worker 13-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:06:48,345][12098] Updated weights for policy 0, policy_version 430 (0.0012)
-[2024-12-09 18:06:49,689][11982] Fps is (10 sec: 6565.2, 60 sec: 6587.7, 300 sec: 6569.3). Total num frames: 448512. Throughput: 0: 5960.2. Samples: 389948. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:06:49,690][11982] Avg episode reward: [(0, '-0.220')]
-[2024-12-09 18:06:49,754][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000439_449536.pth...
-[2024-12-09 18:06:49,795][12110] Worker 10-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:06:49,814][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000374_382976.pth
-[2024-12-09 18:06:49,896][12098] Updated weights for policy 0, policy_version 440 (0.0011)
-[2024-12-09 18:06:51,384][12098] Updated weights for policy 0, policy_version 450 (0.0011)
-[2024-12-09 18:06:53,005][12098] Updated weights for policy 0, policy_version 460 (0.0012)
-[2024-12-09 18:06:54,196][12100] Worker 1-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:06:54,485][12098] Updated weights for policy 0, policy_version 470 (0.0010)
-[2024-12-09 18:06:54,688][11982] Fps is (10 sec: 6656.1, 60 sec: 6604.8, 300 sec: 6582.9). Total num frames: 482304. Throughput: 0: 5959.4. Samples: 425842. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:06:54,690][11982] Avg episode reward: [(0, '-0.020')]
-[2024-12-09 18:06:54,696][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000471_482304.pth...
-[2024-12-09 18:06:54,754][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000406_415744.pth
-[2024-12-09 18:06:54,766][12078] Saving new best policy, reward=-0.020!
-[2024-12-09 18:06:56,051][12098] Updated weights for policy 0, policy_version 480 (0.0011)
-[2024-12-09 18:06:56,684][12102] Worker 3-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:06:57,647][12098] Updated weights for policy 0, policy_version 490 (0.0012)
-[2024-12-09 18:06:59,129][12098] Updated weights for policy 0, policy_version 500 (0.0012)
-[2024-12-09 18:06:59,689][11982] Fps is (10 sec: 6656.2, 60 sec: 6604.8, 300 sec: 6580.9). Total num frames: 515072. Throughput: 0: 5979.7. Samples: 462428. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:06:59,690][11982] Avg episode reward: [(0, '-0.060')]
-[2024-12-09 18:06:59,744][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000504_516096.pth...
-[2024-12-09 18:06:59,804][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000439_449536.pth
-[2024-12-09 18:07:00,657][12098] Updated weights for policy 0, policy_version 510 (0.0011)
-[2024-12-09 18:07:00,949][12099] Worker 0-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:07:02,188][12098] Updated weights for policy 0, policy_version 520 (0.0012)
-[2024-12-09 18:07:02,324][12106] Worker 8-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:07:02,540][12106] Worker 8-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:07:03,768][12098] Updated weights for policy 0, policy_version 530 (0.0018)
-[2024-12-09 18:07:04,473][12115] Worker 15-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:07:04,689][11982] Fps is (10 sec: 6655.8, 60 sec: 6604.8, 300 sec: 6592.0). Total num frames: 548864. Throughput: 0: 5976.7. Samples: 480397. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)
-[2024-12-09 18:07:04,690][11982] Avg episode reward: [(0, '-0.020')]
-[2024-12-09 18:07:04,695][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000536_548864.pth...
-[2024-12-09 18:07:04,758][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000471_482304.pth
-[2024-12-09 18:07:05,303][12098] Updated weights for policy 0, policy_version 540 (0.0011)
-[2024-12-09 18:07:06,788][12098] Updated weights for policy 0, policy_version 550 (0.0011)
-[2024-12-09 18:07:07,892][12108] Worker 7-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:07:08,364][12098] Updated weights for policy 0, policy_version 560 (0.0012)
-[2024-12-09 18:07:09,689][11982] Fps is (10 sec: 6655.9, 60 sec: 6587.8, 300 sec: 6589.7). Total num frames: 581632. Throughput: 0: 5956.1. Samples: 516339. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)
-[2024-12-09 18:07:09,691][11982] Avg episode reward: [(0, '-0.020')]
-[2024-12-09 18:07:09,708][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000568_581632.pth...
-[2024-12-09 18:07:09,770][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000504_516096.pth
-[2024-12-09 18:07:09,981][12098] Updated weights for policy 0, policy_version 570 (0.0013)
-[2024-12-09 18:07:11,550][12098] Updated weights for policy 0, policy_version 580 (0.0013)
-[2024-12-09 18:07:11,979][12099] Worker 0-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:07:13,103][12098] Updated weights for policy 0, policy_version 590 (0.0012)
-[2024-12-09 18:07:14,603][12098] Updated weights for policy 0, policy_version 600 (0.0012)
-[2024-12-09 18:07:14,688][11982] Fps is (10 sec: 6553.7, 60 sec: 6587.8, 300 sec: 6587.7). Total num frames: 614400. Throughput: 0: 5938.4. Samples: 551480. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:07:14,690][11982] Avg episode reward: [(0, '0.000')]
-[2024-12-09 18:07:14,747][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000601_615424.pth...
-[2024-12-09 18:07:14,807][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000536_548864.pth
-[2024-12-09 18:07:14,818][12078] Saving new best policy, reward=0.000!
-[2024-12-09 18:07:16,119][12098] Updated weights for policy 0, policy_version 610 (0.0011)
-[2024-12-09 18:07:16,409][12111] Worker 12-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker (200 times)
-[2024-12-09 18:07:17,633][12098] Updated weights for policy 0, policy_version 620 (0.0012)
-[2024-12-09 18:07:19,183][12098] Updated weights for policy 0, policy_version 630 (0.0011)
-[2024-12-09 18:07:19,689][11982] Fps is (10 sec: 6655.9, 60 sec: 6604.8, 300 sec: 6596.7). Total num frames: 648192. Throughput: 0: 5957.3. Samples: 569685. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:07:19,690][11982] Avg episode reward: [(0, '0.000')]
-[2024-12-09 18:07:19,707][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000633_648192.pth...
-[2024-12-09 18:07:19,769][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000568_581632.pth
-[2024-12-09 18:07:20,731][12098] Updated weights for policy 0, policy_version 640 (0.0012)
-[2024-12-09 18:07:22,288][12098] Updated weights for policy 0, policy_version 650 (0.0011)
-[2024-12-09 18:07:22,544][12100] Worker 1-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:07:23,799][12098] Updated weights for policy 0, policy_version 660 (0.0012)
-[2024-12-09 18:07:24,689][11982] Fps is (10 sec: 6656.0, 60 sec: 6587.7, 300 sec: 6594.6). Total num frames: 680960. Throughput: 0: 6011.7. Samples: 606229. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)
-[2024-12-09 18:07:24,690][11982] Avg episode reward: [(0, '0.000')]
-[2024-12-09 18:07:24,736][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000666_681984.pth...
-[2024-12-09 18:07:24,795][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000601_615424.pth
-[2024-12-09 18:07:25,350][12098] Updated weights for policy 0, policy_version 670 (0.0013)
-[2024-12-09 18:07:26,067][12112] Worker 11-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:07:26,651][12113] Worker 14-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker (100 times)
-[2024-12-09 18:07:26,913][12098] Updated weights for policy 0, policy_version 680 (0.0010)
-[2024-12-09 18:07:28,432][12098] Updated weights for policy 0, policy_version 690 (0.0013)
-[2024-12-09 18:07:29,689][11982] Fps is (10 sec: 6656.0, 60 sec: 6604.8, 300 sec: 6602.4). Total num frames: 714752. Throughput: 0: 6007.0. Samples: 642360. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)
-[2024-12-09 18:07:29,691][11982] Avg episode reward: [(0, '0.000')]
-[2024-12-09 18:07:29,706][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000698_714752.pth...
-[2024-12-09 18:07:29,768][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000633_648192.pth
-[2024-12-09 18:07:30,006][12098] Updated weights for policy 0, policy_version 700 (0.0014)
-[2024-12-09 18:07:31,552][12098] Updated weights for policy 0, policy_version 710 (0.0011)
-[2024-12-09 18:07:33,023][12098] Updated weights for policy 0, policy_version 720 (0.0012)
-[2024-12-09 18:07:34,668][12098] Updated weights for policy 0, policy_version 730 (0.0011)
-[2024-12-09 18:07:34,688][11982] Fps is (10 sec: 6656.0, 60 sec: 6621.9, 300 sec: 6600.1). Total num frames: 747520. Throughput: 0: 6012.0. Samples: 660484. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)
-[2024-12-09 18:07:34,690][11982] Avg episode reward: [(0, '-0.060')]
-[2024-12-09 18:07:34,695][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000730_747520.pth...
-[2024-12-09 18:07:34,758][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000666_681984.pth
-[2024-12-09 18:07:36,217][12098] Updated weights for policy 0, policy_version 740 (0.0011)
-[2024-12-09 18:07:37,977][12098] Updated weights for policy 0, policy_version 750 (0.0013)
-[2024-12-09 18:07:38,539][12105] Worker 6-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker (200 times)
-[2024-12-09 18:07:39,689][11982] Fps is (10 sec: 6246.3, 60 sec: 6572.6, 300 sec: 6571.4). Total num frames: 777216. Throughput: 0: 5977.0. Samples: 694810. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:07:39,690][11982] Avg episode reward: [(0, '0.000')]
-[2024-12-09 18:07:39,708][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000759_777216.pth...
-[2024-12-09 18:07:39,797][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000698_714752.pth
-[2024-12-09 18:07:39,909][12098] Updated weights for policy 0, policy_version 760 (0.0014)
-[2024-12-09 18:07:41,613][12098] Updated weights for policy 0, policy_version 770 (0.0014)
-[2024-12-09 18:07:43,162][12098] Updated weights for policy 0, policy_version 780 (0.0012)
-[2024-12-09 18:07:43,218][11982] Keyboard interrupt detected in the event loop EvtLoop [Runner_EvtLoop, process=main process 11982], exiting...
-[2024-12-09 18:07:43,219][12105] Stopping RolloutWorker_w6...
-[2024-12-09 18:07:43,219][12114] Stopping RolloutWorker_w13...
-[2024-12-09 18:07:43,219][12102] Stopping RolloutWorker_w3...
-[2024-12-09 18:07:43,220][12113] Stopping RolloutWorker_w14...
-[2024-12-09 18:07:43,220][12100] Stopping RolloutWorker_w1...
-[2024-12-09 18:07:43,219][12108] Stopping RolloutWorker_w7...
-[2024-12-09 18:07:43,220][12112] Stopping RolloutWorker_w11...
-[2024-12-09 18:07:43,220][12103] Stopping RolloutWorker_w4...
-[2024-12-09 18:07:43,220][11982] Runner profile tree view:
-main_loop: 137.4975
-[2024-12-09 18:07:43,220][12106] Stopping RolloutWorker_w8...
-[2024-12-09 18:07:43,221][12111] Stopping RolloutWorker_w12...
-[2024-12-09 18:07:43,221][12101] Stopping RolloutWorker_w2...
-[2024-12-09 18:07:43,221][12115] Stopping RolloutWorker_w15...
-[2024-12-09 18:07:43,221][12109] Stopping RolloutWorker_w9...
-[2024-12-09 18:07:43,221][12114] Loop rollout_proc13_evt_loop terminating...
-[2024-12-09 18:07:43,221][12105] Loop rollout_proc6_evt_loop terminating...
-[2024-12-09 18:07:43,222][12113] Loop rollout_proc14_evt_loop terminating...
-[2024-12-09 18:07:43,222][12102] Loop rollout_proc3_evt_loop terminating...
-[2024-12-09 18:07:43,221][12078] Stopping Batcher_0...
-[2024-12-09 18:07:43,222][12108] Loop rollout_proc7_evt_loop terminating...
-[2024-12-09 18:07:43,222][12112] Loop rollout_proc11_evt_loop terminating...
-[2024-12-09 18:07:43,222][12104] Stopping RolloutWorker_w5...
-[2024-12-09 18:07:43,222][11982] Collected {0: 798720}, FPS: 5809.0
-[2024-12-09 18:07:43,222][12110] Stopping RolloutWorker_w10...
-[2024-12-09 18:07:43,223][12103] Loop rollout_proc4_evt_loop terminating...
-[2024-12-09 18:07:43,223][12109] Loop rollout_proc9_evt_loop terminating...
-[2024-12-09 18:07:43,223][12101] Loop rollout_proc2_evt_loop terminating...
-[2024-12-09 18:07:43,223][12115] Loop rollout_proc15_evt_loop terminating...
-[2024-12-09 18:07:43,223][12106] Loop rollout_proc8_evt_loop terminating...
-[2024-12-09 18:07:43,224][12111] Loop rollout_proc12_evt_loop terminating...
-[2024-12-09 18:07:43,224][12078] Loop batcher_evt_loop terminating...
-[2024-12-09 18:07:43,222][12100] Loop rollout_proc1_evt_loop terminating...
-[2024-12-09 18:07:43,226][12104] Loop rollout_proc5_evt_loop terminating...
-[2024-12-09 18:07:43,225][12110] Loop rollout_proc10_evt_loop terminating...
-[2024-12-09 18:07:43,224][12099] Stopping RolloutWorker_w0...
-[2024-12-09 18:07:43,236][12099] Loop rollout_proc0_evt_loop terminating...
-[2024-12-09 18:07:43,239][12078] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000780_798720.pth...
-[2024-12-09 18:07:43,353][12098] Weights refcount: 2 0
-[2024-12-09 18:07:43,358][12098] Stopping InferenceWorker_p0-w0...
-[2024-12-09 18:07:43,359][12098] Loop inference_proc0-0_evt_loop terminating...
-[2024-12-09 18:07:43,363][12078] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000730_747520.pth
-[2024-12-09 18:07:43,387][12078] Stopping LearnerWorker_p0...
-[2024-12-09 18:07:43,389][12078] Loop learner_proc0_evt_loop terminating...
-[2024-12-09 18:22:10,588][16664] Saving configuration to /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/config.json...
-[2024-12-09 18:22:11,629][16664] Rollout worker 0 uses device cpu
-[2024-12-09 18:22:11,630][16664] Rollout worker 1 uses device cpu
-[2024-12-09 18:22:11,631][16664] Rollout worker 2 uses device cpu
-[2024-12-09 18:22:11,632][16664] Rollout worker 3 uses device cpu
-[2024-12-09 18:22:11,633][16664] Rollout worker 4 uses device cpu
-[2024-12-09 18:22:11,635][16664] Rollout worker 5 uses device cpu
-[2024-12-09 18:22:11,636][16664] Rollout worker 6 uses device cpu
-[2024-12-09 18:22:11,637][16664] Rollout worker 7 uses device cpu
-[2024-12-09 18:22:11,638][16664] Rollout worker 8 uses device cpu
-[2024-12-09 18:22:11,639][16664] Rollout worker 9 uses device cpu
-[2024-12-09 18:22:11,640][16664] Rollout worker 10 uses device cpu
-[2024-12-09 18:22:11,641][16664] Rollout worker 11 uses device cpu
-[2024-12-09 18:22:11,642][16664] Rollout worker 12 uses device cpu
-[2024-12-09 18:22:11,643][16664] Rollout worker 13 uses device cpu
-[2024-12-09 18:22:11,645][16664] Rollout worker 14 uses device cpu
-[2024-12-09 18:22:11,646][16664] Rollout worker 15 uses device cpu
-[2024-12-09 18:22:11,743][16664] InferenceWorker_p0-w0: min num requests: 5
-[2024-12-09 18:22:11,845][16664] Starting all processes...
-[2024-12-09 18:22:11,846][16664] Starting process learner_proc0
-[2024-12-09 18:22:12,943][16664] Starting all processes...
-[2024-12-09 18:22:12,962][16664] Starting process inference_proc0-0
-[2024-12-09 18:22:12,963][16664] Starting process rollout_proc0
-[2024-12-09 18:22:12,964][16664] Starting process rollout_proc1
-[2024-12-09 18:22:12,964][16664] Starting process rollout_proc2
-[2024-12-09 18:22:12,965][16664] Starting process rollout_proc3
-[2024-12-09 18:22:12,965][16664] Starting process rollout_proc4
-[2024-12-09 18:22:12,966][16664] Starting process rollout_proc5
-[2024-12-09 18:22:12,972][16664] Starting process rollout_proc6
-[2024-12-09 18:22:12,987][16664] Starting process rollout_proc11
-[2024-12-09 18:22:12,977][16664] Starting process rollout_proc8
-[2024-12-09 18:22:12,981][16664] Starting process rollout_proc9
-[2024-12-09 18:22:12,983][16664] Starting process rollout_proc10
-[2024-12-09 18:22:12,974][16664] Starting process rollout_proc7
-[2024-12-09 18:22:12,988][16664] Starting process rollout_proc12
-[2024-12-09 18:22:12,989][16664] Starting process rollout_proc13
-[2024-12-09 18:22:12,990][16664] Starting process rollout_proc14
-[2024-12-09 18:22:12,991][16664] Starting process rollout_proc15
-[2024-12-09 18:22:23,992][16797] Worker 8 uses CPU cores [8]
-[2024-12-09 18:22:24,056][16789] Worker 4 uses CPU cores [4]
-[2024-12-09 18:22:24,191][16786] Worker 1 uses CPU cores [1]
-[2024-12-09 18:22:24,244][16788] Worker 3 uses CPU cores [3]
-[2024-12-09 18:22:24,272][16792] Worker 11 uses CPU cores [11]
-[2024-12-09 18:22:24,284][16794] Worker 6 uses CPU cores [6]
-[2024-12-09 18:22:24,372][16787] Worker 2 uses CPU cores [2]
-[2024-12-09 18:22:24,534][16785] Worker 0 uses CPU cores [0]
-[2024-12-09 18:22:24,611][16764] Starting seed is not provided
-[2024-12-09 18:22:24,613][16764] Initializing actor-critic model on device cpu
-[2024-12-09 18:22:24,615][16764] RunningMeanStd input shape: (1, 8, 8)
-[2024-12-09 18:22:24,627][16764] RunningMeanStd input shape: (1,)
-[2024-12-09 18:22:24,639][16793] Worker 9 uses CPU cores [9]
-[2024-12-09 18:22:24,646][16800] Worker 13 uses CPU cores [13]
-[2024-12-09 18:22:24,725][16801] Worker 15 uses CPU cores [15]
-[2024-12-09 18:22:24,772][16790] Worker 5 uses CPU cores [5]
-[2024-12-09 18:22:24,793][16796] Worker 7 uses CPU cores [7]
-[2024-12-09 18:22:24,804][16764] Created Actor Critic model with architecture:
-[2024-12-09 18:22:24,805][16764] ActorCriticSharedWeights(
-  (obs_normalizer): ObservationNormalizer(
-    (running_mean_std): RunningMeanStdDictInPlace(
-      (running_mean_std): ModuleDict(
-        (obs): RunningMeanStdInPlace()
-      )
-    )
-  )
-  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
-  (encoder): CustomEncoder(
-    (conv_head): Sequential(
-      (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2))
-      (1): ELU(alpha=1.0)
-      (2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1))
-      (3): ELU(alpha=1.0)
-    )
-  )
-  (core): ModelCoreRNN(
-    (core): GRU(64, 128)
-  )
-  (decoder): MlpDecoder(
-    (mlp): Identity()
-  )
-  (critic_linear): Linear(in_features=128, out_features=1, bias=True)
-  (action_parameterization): ActionParameterizationDefault(
-    (distribution_linear): Linear(in_features=128, out_features=2, bias=True)
-  )
-)
-[2024-12-09 18:22:24,958][16799] Worker 14 uses CPU cores [14]
-[2024-12-09 18:22:24,977][16798] Worker 12 uses CPU cores [12]
-[2024-12-09 18:22:25,129][16795] Worker 10 uses CPU cores [10]
-[2024-12-09 18:22:25,926][16764] Using optimizer <class 'torch.optim.adam.Adam'>
-[2024-12-09 18:22:28,300][16764] Loading state from checkpoint /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000780_798720.pth...
-[2024-12-09 18:22:28,349][16764] Loading model from checkpoint
-[2024-12-09 18:22:28,351][16764] Loaded experiment state at self.train_step=780, self.env_steps=798720
-[2024-12-09 18:22:28,354][16764] Initialized policy 0 weights for model version 780
-[2024-12-09 18:22:28,359][16764] LearnerWorker_p0 finished initialization!
-[2024-12-09 18:22:28,368][16764] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000780_798720.pth...
-[2024-12-09 18:22:28,371][16784] RunningMeanStd input shape: (1, 8, 8)
-[2024-12-09 18:22:28,376][16784] RunningMeanStd input shape: (1,)
-[2024-12-09 18:22:28,428][16764] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000780_798720.pth...
-[2024-12-09 18:22:28,428][16664] Inference worker 0-0 is ready!
-[2024-12-09 18:22:28,430][16664] All inference workers are ready! Signal rollout workers to start!
-[2024-12-09 18:22:28,440][16787] Decorrelating experience for 0 frames...
-[2024-12-09 18:22:28,440][16789] Decorrelating experience for 0 frames...
-[2024-12-09 18:22:28,440][16795] Decorrelating experience for 0 frames...
-[2024-12-09 18:22:28,441][16798] Decorrelating experience for 0 frames...
-[2024-12-09 18:22:28,442][16790] Decorrelating experience for 0 frames...
-[2024-12-09 18:22:28,446][16798] Decorrelating experience for 32 frames...
-[2024-12-09 18:22:28,446][16789] Decorrelating experience for 32 frames...
-[2024-12-09 18:22:28,446][16799] Decorrelating experience for 0 frames...
-[2024-12-09 18:22:28,440][16794] Decorrelating experience for 0 frames...
-[2024-12-09 18:22:28,440][16800] Decorrelating experience for 0 frames...
-[2024-12-09 18:22:28,449][16785] Decorrelating experience for 0 frames...
-[2024-12-09 18:22:28,441][16786] Decorrelating experience for 0 frames...
-[2024-12-09 18:22:28,451][16792] Decorrelating experience for 0 frames...
-[2024-12-09 18:22:28,453][16795] Decorrelating experience for 32 frames...
-[2024-12-09 18:22:28,454][16787] Decorrelating experience for 32 frames...
-[2024-12-09 18:22:28,456][16790] Decorrelating experience for 32 frames...
-[2024-12-09 18:22:28,456][16799] Decorrelating experience for 32 frames...
-[2024-12-09 18:22:28,456][16785] Decorrelating experience for 32 frames...
-[2024-12-09 18:22:28,457][16788] Decorrelating experience for 0 frames...
-[2024-12-09 18:22:28,458][16794] Decorrelating experience for 32 frames...
-[2024-12-09 18:22:28,463][16800] Decorrelating experience for 32 frames...
-[2024-12-09 18:22:28,463][16786] Decorrelating experience for 32 frames...
-[2024-12-09 18:22:28,464][16796] Decorrelating experience for 0 frames...
-[2024-12-09 18:22:28,464][16793] Decorrelating experience for 0 frames...
-[2024-12-09 18:22:28,464][16788] Decorrelating experience for 32 frames...
-[2024-12-09 18:22:28,468][16796] Decorrelating experience for 32 frames...
-[2024-12-09 18:22:28,472][16793] Decorrelating experience for 32 frames...
-[2024-12-09 18:22:28,472][16801] Decorrelating experience for 0 frames...
-[2024-12-09 18:22:28,474][16792] Decorrelating experience for 32 frames...
-[2024-12-09 18:22:28,480][16801] Decorrelating experience for 32 frames...
-[2024-12-09 18:22:28,482][16797] Decorrelating experience for 0 frames...
-[2024-12-09 18:22:28,486][16797] Decorrelating experience for 32 frames...
-[2024-12-09 18:22:28,530][16764] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000780_798720.pth...
-[2024-12-09 18:22:28,570][16800] Worker 13-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:28,571][16789] Worker 4-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:28,580][16798] Worker 12-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:28,580][16785] Worker 0-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:28,580][16790] Worker 5-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:28,580][16786] Worker 1-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:28,581][16788] Worker 3-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:28,581][16796] Worker 7-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:28,581][16795] Worker 10-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:28,582][16794] Worker 6-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:28,582][16799] Worker 14-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:28,582][16792] Worker 11-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:28,582][16797] Worker 8-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:28,582][16787] Worker 2-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:28,584][16793] Worker 9-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:28,584][16801] Worker 15-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:28,591][16764] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000780_798720.pth...
-[2024-12-09 18:22:28,904][16792] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:22:28,913][16796] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:22:28,914][16794] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:22:28,915][16786] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:22:28,915][16793] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:22:28,922][16800] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:22:28,922][16801] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:22:28,922][16795] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:22:28,929][16797] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:22:28,930][16790] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:22:28,937][16799] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:22:28,938][16789] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:22:28,945][16798] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:22:28,953][16785] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:22:28,976][16787] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:22:28,976][16788] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:22:28,999][16764] Signal inference workers to stop experience collection...
-[2024-12-09 18:22:29,009][16784] InferenceWorker_p0-w0: stopping experience collection
-[2024-12-09 18:22:29,279][16764] Signal inference workers to resume experience collection...
-[2024-12-09 18:22:29,281][16784] InferenceWorker_p0-w0: resuming experience collection
-[2024-12-09 18:22:30,180][16790] Worker 5-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:30,292][16794] Worker 6-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:30,664][16789] Worker 4-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:30,741][16784] Updated weights for policy 0, policy_version 790 (0.0018)
-[2024-12-09 18:22:30,781][16664] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 808960. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
-[2024-12-09 18:22:30,783][16664] Avg episode reward: [(0, '0.000')]
-[2024-12-09 18:22:30,788][16764] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000790_808960.pth...
-[2024-12-09 18:22:30,847][16764] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000759_777216.pth
-[2024-12-09 18:22:31,235][16792] Worker 11-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:31,729][16664] Heartbeat connected on Batcher_0
-[2024-12-09 18:22:31,735][16664] Heartbeat connected on LearnerWorker_p0
-[2024-12-09 18:22:31,751][16664] Heartbeat connected on RolloutWorker_w0
-[2024-12-09 18:22:31,753][16664] Heartbeat connected on InferenceWorker_p0-w0
-[2024-12-09 18:22:31,757][16664] Heartbeat connected on RolloutWorker_w1
-[2024-12-09 18:22:31,764][16664] Heartbeat connected on RolloutWorker_w2
-[2024-12-09 18:22:31,769][16664] Heartbeat connected on RolloutWorker_w3
-[2024-12-09 18:22:31,775][16664] Heartbeat connected on RolloutWorker_w4
-[2024-12-09 18:22:31,781][16664] Heartbeat connected on RolloutWorker_w5
-[2024-12-09 18:22:31,788][16664] Heartbeat connected on RolloutWorker_w6
-[2024-12-09 18:22:31,794][16664] Heartbeat connected on RolloutWorker_w7
-[2024-12-09 18:22:31,806][16664] Heartbeat connected on RolloutWorker_w9
-[2024-12-09 18:22:31,813][16664] Heartbeat connected on RolloutWorker_w10
-[2024-12-09 18:22:31,815][16664] Heartbeat connected on RolloutWorker_w8
-[2024-12-09 18:22:31,819][16664] Heartbeat connected on RolloutWorker_w11
-[2024-12-09 18:22:31,825][16664] Heartbeat connected on RolloutWorker_w12
-[2024-12-09 18:22:31,831][16664] Heartbeat connected on RolloutWorker_w13
-[2024-12-09 18:22:31,837][16664] Heartbeat connected on RolloutWorker_w14
-[2024-12-09 18:22:31,843][16664] Heartbeat connected on RolloutWorker_w15
-[2024-12-09 18:22:32,356][16784] Updated weights for policy 0, policy_version 800 (0.0011)
-[2024-12-09 18:22:33,660][16798] Worker 12-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:33,924][16784] Updated weights for policy 0, policy_version 810 (0.0012)
-[2024-12-09 18:22:35,502][16784] Updated weights for policy 0, policy_version 820 (0.0013)
-[2024-12-09 18:22:35,781][16664] Fps is (10 sec: 6348.4, 60 sec: 6348.4, 300 sec: 6348.4). Total num frames: 840704. Throughput: 0: 6331.2. Samples: 31658. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:22:35,783][16664] Avg episode reward: [(0, '0.000')]
-[2024-12-09 18:22:35,820][16764] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000822_841728.pth...
-[2024-12-09 18:22:35,882][16764] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000780_798720.pth
-[2024-12-09 18:22:37,079][16784] Updated weights for policy 0, policy_version 830 (0.0012)
-[2024-12-09 18:22:37,137][16785] Worker 0-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:37,606][16795] Worker 10-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:38,403][16787] Worker 2-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:38,611][16784] Updated weights for policy 0, policy_version 840 (0.0013)
-[2024-12-09 18:22:40,099][16799] Worker 14-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:40,133][16784] Updated weights for policy 0, policy_version 850 (0.0011)
-[2024-12-09 18:22:40,781][16664] Fps is (10 sec: 6451.1, 60 sec: 6451.1, 300 sec: 6451.1). Total num frames: 873472. Throughput: 0: 6712.2. Samples: 67123. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)
-[2024-12-09 18:22:40,783][16664] Avg episode reward: [(0, '0.000')]
-[2024-12-09 18:22:40,832][16764] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000854_874496.pth...
-[2024-12-09 18:22:40,893][16764] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000790_808960.pth
-[2024-12-09 18:22:40,903][16788] Worker 3-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:41,787][16784] Updated weights for policy 0, policy_version 860 (0.0013)
-[2024-12-09 18:22:42,785][16793] Worker 9-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:22:42,823][16664] Keyboard interrupt detected in the event loop EvtLoop [Runner_EvtLoop, process=main process 16664], exiting...
-[2024-12-09 18:22:42,825][16800] Stopping RolloutWorker_w13...
-[2024-12-09 18:22:42,826][16795] Stopping RolloutWorker_w10...
-[2024-12-09 18:22:42,826][16796] Stopping RolloutWorker_w7...
-[2024-12-09 18:22:42,826][16788] Stopping RolloutWorker_w3...
-[2024-12-09 18:22:42,826][16786] Stopping RolloutWorker_w1...
-[2024-12-09 18:22:42,826][16798] Stopping RolloutWorker_w12...
-[2024-12-09 18:22:42,826][16790] Stopping RolloutWorker_w5...
-[2024-12-09 18:22:42,826][16801] Stopping RolloutWorker_w15...
-[2024-12-09 18:22:42,827][16793] Stopping RolloutWorker_w9...
-[2024-12-09 18:22:42,827][16794] Stopping RolloutWorker_w6...
-[2024-12-09 18:22:42,826][16664] Runner profile tree view:
-main_loop: 30.9818
-[2024-12-09 18:22:42,827][16787] Stopping RolloutWorker_w2...
-[2024-12-09 18:22:42,827][16764] Stopping Batcher_0...
-[2024-12-09 18:22:42,828][16789] Stopping RolloutWorker_w4...
-[2024-12-09 18:22:42,828][16797] Stopping RolloutWorker_w8...
-[2024-12-09 18:22:42,828][16795] Loop rollout_proc10_evt_loop terminating...
-[2024-12-09 18:22:42,828][16800] Loop rollout_proc13_evt_loop terminating...
-[2024-12-09 18:22:42,828][16796] Loop rollout_proc7_evt_loop terminating...
-[2024-12-09 18:22:42,828][16788] Loop rollout_proc3_evt_loop terminating...
-[2024-12-09 18:22:42,828][16664] Collected {0: 886784}, FPS: 2842.4
-[2024-12-09 18:22:42,829][16801] Loop rollout_proc15_evt_loop terminating...
-[2024-12-09 18:22:42,829][16798] Loop rollout_proc12_evt_loop terminating...
-[2024-12-09 18:22:42,829][16787] Loop rollout_proc2_evt_loop terminating...
-[2024-12-09 18:22:42,829][16794] Loop rollout_proc6_evt_loop terminating...
-[2024-12-09 18:22:42,829][16764] Loop batcher_evt_loop terminating...
-[2024-12-09 18:22:42,829][16790] Loop rollout_proc5_evt_loop terminating...
-[2024-12-09 18:22:42,830][16793] Loop rollout_proc9_evt_loop terminating...
-[2024-12-09 18:22:42,829][16786] Loop rollout_proc1_evt_loop terminating...
-[2024-12-09 18:22:42,831][16789] Loop rollout_proc4_evt_loop terminating...
-[2024-12-09 18:22:42,832][16797] Loop rollout_proc8_evt_loop terminating...
-[2024-12-09 18:22:42,832][16785] Stopping RolloutWorker_w0...
-[2024-12-09 18:22:42,834][16792] Stopping RolloutWorker_w11...
-[2024-12-09 18:22:42,836][16785] Loop rollout_proc0_evt_loop terminating...
-[2024-12-09 18:22:42,842][16799] Stopping RolloutWorker_w14...
-[2024-12-09 18:22:42,859][16799] Loop rollout_proc14_evt_loop terminating...
-[2024-12-09 18:22:42,863][16792] Loop rollout_proc11_evt_loop terminating...
-[2024-12-09 18:22:42,938][16764] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000867_887808.pth...
-[2024-12-09 18:22:42,946][16784] Weights refcount: 2 0
-[2024-12-09 18:22:42,950][16784] Stopping InferenceWorker_p0-w0...
-[2024-12-09 18:22:42,953][16784] Loop inference_proc0-0_evt_loop terminating...
-[2024-12-09 18:22:43,023][16764] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000822_841728.pth
-[2024-12-09 18:22:43,035][16764] Stopping LearnerWorker_p0...
-[2024-12-09 18:22:43,037][16764] Loop learner_proc0_evt_loop terminating...
-[2024-12-09 18:23:42,589][17689] Saving configuration to /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/config.json...
-[2024-12-09 18:23:43,632][17689] Rollout worker 0 uses device cpu
-[2024-12-09 18:23:43,633][17689] Rollout worker 1 uses device cpu
-[2024-12-09 18:23:43,635][17689] Rollout worker 2 uses device cpu
-[2024-12-09 18:23:43,636][17689] Rollout worker 3 uses device cpu
-[2024-12-09 18:23:43,637][17689] Rollout worker 4 uses device cpu
-[2024-12-09 18:23:43,638][17689] Rollout worker 5 uses device cpu
-[2024-12-09 18:23:43,639][17689] Rollout worker 6 uses device cpu
-[2024-12-09 18:23:43,640][17689] Rollout worker 7 uses device cpu
-[2024-12-09 18:23:43,641][17689] Rollout worker 8 uses device cpu
-[2024-12-09 18:23:43,642][17689] Rollout worker 9 uses device cpu
-[2024-12-09 18:23:43,643][17689] Rollout worker 10 uses device cpu
-[2024-12-09 18:23:43,645][17689] Rollout worker 11 uses device cpu
-[2024-12-09 18:23:43,646][17689] Rollout worker 12 uses device cpu
-[2024-12-09 18:23:43,648][17689] Rollout worker 13 uses device cpu
-[2024-12-09 18:23:43,649][17689] Rollout worker 14 uses device cpu
-[2024-12-09 18:23:43,650][17689] Rollout worker 15 uses device cpu
-[2024-12-09 18:23:43,701][17689] InferenceWorker_p0-w0: min num requests: 5
-[2024-12-09 18:23:43,804][17689] Starting all processes...
-[2024-12-09 18:23:43,805][17689] Starting process learner_proc0
-[2024-12-09 18:23:44,338][17689] Starting all processes...
-[2024-12-09 18:23:44,355][17689] Starting process inference_proc0-0
-[2024-12-09 18:23:44,356][17689] Starting process rollout_proc0
-[2024-12-09 18:23:44,356][17689] Starting process rollout_proc1
-[2024-12-09 18:23:44,357][17689] Starting process rollout_proc2
-[2024-12-09 18:23:44,357][17689] Starting process rollout_proc3
-[2024-12-09 18:23:44,358][17689] Starting process rollout_proc4
-[2024-12-09 18:23:44,359][17689] Starting process rollout_proc5
-[2024-12-09 18:23:44,359][17689] Starting process rollout_proc6
-[2024-12-09 18:23:44,360][17689] Starting process rollout_proc7
-[2024-12-09 18:23:44,360][17689] Starting process rollout_proc8
-[2024-12-09 18:23:44,361][17689] Starting process rollout_proc9
-[2024-12-09 18:23:44,362][17689] Starting process rollout_proc10
-[2024-12-09 18:23:44,362][17689] Starting process rollout_proc11
-[2024-12-09 18:23:44,363][17689] Starting process rollout_proc12
-[2024-12-09 18:23:44,364][17689] Starting process rollout_proc13
-[2024-12-09 18:23:44,378][17689] Starting process rollout_proc14
-[2024-12-09 18:23:44,381][17689] Starting process rollout_proc15
-[2024-12-09 18:23:52,892][17806] Worker 5 uses CPU cores [5]
-[2024-12-09 18:23:53,004][17810] Worker 10 uses CPU cores [10]
-[2024-12-09 18:23:53,182][17804] Worker 2 uses CPU cores [2]
-[2024-12-09 18:23:53,337][17812] Worker 12 uses CPU cores [12]
-[2024-12-09 18:23:53,340][17814] Worker 14 uses CPU cores [14]
-[2024-12-09 18:23:53,374][17778] Starting seed is not provided
-[2024-12-09 18:23:53,375][17778] Initializing actor-critic model on device cpu
-[2024-12-09 18:23:53,379][17778] RunningMeanStd input shape: (1, 8, 8)
-[2024-12-09 18:23:53,383][17778] RunningMeanStd input shape: (1,)
-[2024-12-09 18:23:53,413][17813] Worker 13 uses CPU cores [13]
-[2024-12-09 18:23:53,474][17799] Worker 0 uses CPU cores [0]
-[2024-12-09 18:23:53,478][17778] Created Actor Critic model with architecture:
-[2024-12-09 18:23:53,482][17778] ActorCriticSharedWeights(
-  (obs_normalizer): ObservationNormalizer(
-    (running_mean_std): RunningMeanStdDictInPlace(
-      (running_mean_std): ModuleDict(
-        (obs): RunningMeanStdInPlace()
-      )
-    )
-  )
-  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
-  (encoder): CustomEncoder(
-    (conv_head): Sequential(
-      (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2))
-      (1): ELU(alpha=1.0)
-      (2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1))
-      (3): ELU(alpha=1.0)
-    )
-  )
-  (core): ModelCoreRNN(
-    (core): GRU(64, 128)
-  )
-  (decoder): MlpDecoder(
-    (mlp): Identity()
-  )
-  (critic_linear): Linear(in_features=128, out_features=1, bias=True)
-  (action_parameterization): ActionParameterizationDefault(
-    (distribution_linear): Linear(in_features=128, out_features=2, bias=True)
-  )
-)
-[2024-12-09 18:23:53,512][17808] Worker 8 uses CPU cores [8]
-[2024-12-09 18:23:53,612][17815] Worker 15 uses CPU cores [15]
-[2024-12-09 18:23:53,632][17811] Worker 11 uses CPU cores [11]
-[2024-12-09 18:23:53,694][17809] Worker 9 uses CPU cores [9]
-[2024-12-09 18:23:53,817][17802] Worker 3 uses CPU cores [3]
-[2024-12-09 18:23:54,017][17800] Worker 1 uses CPU cores [1]
-[2024-12-09 18:23:54,022][17807] Worker 7 uses CPU cores [7]
-[2024-12-09 18:23:54,049][17803] Worker 4 uses CPU cores [4]
-[2024-12-09 18:23:54,242][17805] Worker 6 uses CPU cores [6]
-[2024-12-09 18:23:54,729][17778] Using optimizer <class 'torch.optim.adam.Adam'>
-[2024-12-09 18:23:56,100][17778] Loading state from checkpoint /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000867_887808.pth...
-[2024-12-09 18:23:56,145][17778] Loading model from checkpoint
-[2024-12-09 18:23:56,148][17778] Loaded experiment state at self.train_step=867, self.env_steps=887808
-[2024-12-09 18:23:56,150][17778] Initialized policy 0 weights for model version 867
-[2024-12-09 18:23:56,155][17778] LearnerWorker_p0 finished initialization!
-[2024-12-09 18:23:56,167][17778] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000867_887808.pth...
-[2024-12-09 18:23:56,167][17798] RunningMeanStd input shape: (1, 8, 8)
-[2024-12-09 18:23:56,170][17798] RunningMeanStd input shape: (1,)
-[2024-12-09 18:23:56,220][17689] Inference worker 0-0 is ready!
-[2024-12-09 18:23:56,222][17689] All inference workers are ready! Signal rollout workers to start!
-[2024-12-09 18:23:56,224][17778] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000867_887808.pth...
-[2024-12-09 18:23:56,231][17802] Decorrelating experience for 0 frames...
-[2024-12-09 18:23:56,242][17811] Decorrelating experience for 0 frames...
-[2024-12-09 18:23:56,243][17809] Decorrelating experience for 0 frames...
-[2024-12-09 18:23:56,243][17805] Decorrelating experience for 0 frames...
-[2024-12-09 18:23:56,243][17813] Decorrelating experience for 0 frames...
-[2024-12-09 18:23:56,245][17807] Decorrelating experience for 0 frames...
-[2024-12-09 18:23:56,247][17806] Decorrelating experience for 0 frames...
-[2024-12-09 18:23:56,246][17810] Decorrelating experience for 0 frames...
-[2024-12-09 18:23:56,247][17803] Decorrelating experience for 0 frames...
-[2024-12-09 18:23:56,244][17804] Decorrelating experience for 0 frames...
-[2024-12-09 18:23:56,248][17808] Decorrelating experience for 0 frames...
-[2024-12-09 18:23:56,249][17800] Decorrelating experience for 0 frames...
-[2024-12-09 18:23:56,254][17802] Decorrelating experience for 32 frames...
-[2024-12-09 18:23:56,250][17814] Decorrelating experience for 0 frames...
-[2024-12-09 18:23:56,256][17809] Decorrelating experience for 32 frames...
-[2024-12-09 18:23:56,258][17811] Decorrelating experience for 32 frames...
-[2024-12-09 18:23:56,258][17805] Decorrelating experience for 32 frames...
-[2024-12-09 18:23:56,259][17813] Decorrelating experience for 32 frames...
-[2024-12-09 18:23:56,260][17807] Decorrelating experience for 32 frames...
-[2024-12-09 18:23:56,261][17799] Decorrelating experience for 0 frames...
-[2024-12-09 18:23:56,261][17800] Decorrelating experience for 32 frames...
-[2024-12-09 18:23:56,261][17814] Decorrelating experience for 32 frames...
-[2024-12-09 18:23:56,268][17799] Decorrelating experience for 32 frames...
-[2024-12-09 18:23:56,270][17806] Decorrelating experience for 32 frames...
-[2024-12-09 18:23:56,270][17810] Decorrelating experience for 32 frames...
-[2024-12-09 18:23:56,270][17808] Decorrelating experience for 32 frames...
-[2024-12-09 18:23:56,259][17815] Decorrelating experience for 0 frames...
-[2024-12-09 18:23:56,274][17804] Decorrelating experience for 32 frames...
-[2024-12-09 18:23:56,277][17812] Decorrelating experience for 0 frames...
-[2024-12-09 18:23:56,278][17815] Decorrelating experience for 32 frames...
-[2024-12-09 18:23:56,282][17812] Decorrelating experience for 32 frames...
-[2024-12-09 18:23:56,283][17802] Worker 3-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:56,282][17807] Worker 7-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:56,286][17805] Worker 6-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:56,293][17814] Worker 14-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:56,294][17811] Worker 11-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:56,294][17808] Worker 8-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:56,294][17813] Worker 13-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:56,296][17800] Worker 1-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:56,296][17810] Worker 10-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:56,296][17809] Worker 9-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:56,297][17803] Decorrelating experience for 32 frames...
-[2024-12-09 18:23:56,298][17799] Worker 0-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:56,304][17804] Worker 2-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:56,306][17815] Worker 15-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:56,307][17812] Worker 12-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:56,309][17806] Worker 5-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:56,332][17803] Worker 4-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:56,359][17778] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000867_887808.pth...
-[2024-12-09 18:23:56,606][17807] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:23:56,611][17813] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:23:56,612][17800] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:23:56,620][17802] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:23:56,620][17810] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:23:56,625][17811] Worker 11-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:56,626][17812] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:23:56,629][17811] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:23:56,632][17809] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:23:56,634][17805] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:23:56,639][17808] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:23:56,639][17806] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:23:56,643][17799] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:23:56,649][17804] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:23:56,657][17815] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:23:56,659][17803] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:23:56,663][17814] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:23:56,688][17778] Signal inference workers to stop experience collection...
-[2024-12-09 18:23:56,700][17798] InferenceWorker_p0-w0: stopping experience collection
-[2024-12-09 18:23:56,895][17778] Signal inference workers to resume experience collection...
-[2024-12-09 18:23:56,897][17798] InferenceWorker_p0-w0: resuming experience collection
-[2024-12-09 18:23:56,998][17810] Worker 10-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:57,138][17804] Worker 2-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:57,548][17799] Worker 0-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:58,307][17798] Updated weights for policy 0, policy_version 877 (0.0015)
-[2024-12-09 18:23:58,470][17813] Worker 13-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:58,577][17689] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 899072. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
-[2024-12-09 18:23:58,578][17689] Avg episode reward: [(0, '0.000')]
-[2024-12-09 18:23:58,645][17778] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000879_900096.pth...
-[2024-12-09 18:23:58,706][17778] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000854_874496.pth
-[2024-12-09 18:23:58,891][17807] Worker 7-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:23:59,928][17798] Updated weights for policy 0, policy_version 887 (0.0012)
-[2024-12-09 18:24:00,240][17803] Worker 4-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:24:00,375][17806] Worker 5-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:24:00,927][17800] Worker 1-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:24:01,450][17814] Worker 14-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:24:01,557][17798] Updated weights for policy 0, policy_version 897 (0.0012)
-[2024-12-09 18:24:02,140][17808] Worker 8-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:24:03,153][17798] Updated weights for policy 0, policy_version 907 (0.0013)
-[2024-12-09 18:24:03,577][17689] Fps is (10 sec: 6348.6, 60 sec: 6348.6, 300 sec: 6348.6). Total num frames: 930816. Throughput: 0: 6553.3. Samples: 32768. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:24:03,579][17689] Avg episode reward: [(0, '0.000')]
-[2024-12-09 18:24:03,598][17778] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000910_931840.pth...
-[2024-12-09 18:24:03,656][17778] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000867_887808.pth
-[2024-12-09 18:24:03,685][17689] Heartbeat connected on Batcher_0
-[2024-12-09 18:24:03,693][17689] Heartbeat connected on LearnerWorker_p0
-[2024-12-09 18:24:03,710][17689] Heartbeat connected on RolloutWorker_w0
-[2024-12-09 18:24:03,715][17689] Heartbeat connected on InferenceWorker_p0-w0
-[2024-12-09 18:24:03,717][17689] Heartbeat connected on RolloutWorker_w1
-[2024-12-09 18:24:03,724][17689] Heartbeat connected on RolloutWorker_w2
-[2024-12-09 18:24:03,730][17689] Heartbeat connected on RolloutWorker_w3
-[2024-12-09 18:24:03,736][17689] Heartbeat connected on RolloutWorker_w4
-[2024-12-09 18:24:03,750][17689] Heartbeat connected on RolloutWorker_w6
-[2024-12-09 18:24:03,755][17689] Heartbeat connected on RolloutWorker_w5
-[2024-12-09 18:24:03,757][17689] Heartbeat connected on RolloutWorker_w7
-[2024-12-09 18:24:03,762][17689] Heartbeat connected on RolloutWorker_w8
-[2024-12-09 18:24:03,768][17689] Heartbeat connected on RolloutWorker_w9
-[2024-12-09 18:24:03,779][17689] Heartbeat connected on RolloutWorker_w11
-[2024-12-09 18:24:03,785][17689] Heartbeat connected on RolloutWorker_w12
-[2024-12-09 18:24:03,791][17689] Heartbeat connected on RolloutWorker_w13
-[2024-12-09 18:24:03,795][17689] Heartbeat connected on RolloutWorker_w10
-[2024-12-09 18:24:03,797][17689] Heartbeat connected on RolloutWorker_w14
-[2024-12-09 18:24:03,802][17689] Heartbeat connected on RolloutWorker_w15
-[2024-12-09 18:24:04,737][17798] Updated weights for policy 0, policy_version 917 (0.0012)
-[2024-12-09 18:24:06,265][17798] Updated weights for policy 0, policy_version 927 (0.0012)
-[2024-12-09 18:24:07,810][17798] Updated weights for policy 0, policy_version 937 (0.0011)
-[2024-12-09 18:24:08,577][17689] Fps is (10 sec: 6451.1, 60 sec: 6451.1, 300 sec: 6451.1). Total num frames: 963584. Throughput: 0: 6864.7. Samples: 68648. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:24:08,578][17689] Avg episode reward: [(0, '0.000')]
-[2024-12-09 18:24:08,638][17778] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000942_964608.pth...
-[2024-12-09 18:24:08,653][17805] Worker 6-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:24:08,691][17778] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000879_900096.pth
-[2024-12-09 18:24:09,406][17798] Updated weights for policy 0, policy_version 947 (0.0012)
-[2024-12-09 18:24:09,711][17812] Worker 12-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:24:10,936][17798] Updated weights for policy 0, policy_version 957 (0.0012)
-[2024-12-09 18:24:12,031][17802] Worker 3-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:24:12,478][17798] Updated weights for policy 0, policy_version 967 (0.0012)
-[2024-12-09 18:24:13,577][17689] Fps is (10 sec: 6656.1, 60 sec: 6553.6, 300 sec: 6553.6). Total num frames: 997376. Throughput: 0: 5759.6. Samples: 86394. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:24:13,578][17689] Avg episode reward: [(0, '0.000')]
-[2024-12-09 18:24:13,583][17778] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000974_997376.pth...
-[2024-12-09 18:24:13,648][17778] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000910_931840.pth
-[2024-12-09 18:24:14,012][17798] Updated weights for policy 0, policy_version 977 (0.0011)
-[2024-12-09 18:24:15,534][17798] Updated weights for policy 0, policy_version 987 (0.0013)
-[2024-12-09 18:24:17,252][17798] Updated weights for policy 0, policy_version 997 (0.0013)
-[2024-12-09 18:24:17,265][17689] Keyboard interrupt detected in the event loop EvtLoop [Runner_EvtLoop, process=main process 17689], exiting...
-[2024-12-09 18:24:17,267][17810] Stopping RolloutWorker_w10...
-[2024-12-09 18:24:17,267][17807] Stopping RolloutWorker_w7...
-[2024-12-09 18:24:17,268][17799] Stopping RolloutWorker_w0...
-[2024-12-09 18:24:17,268][17800] Stopping RolloutWorker_w1...
-[2024-12-09 18:24:17,268][17811] Stopping RolloutWorker_w11...
-[2024-12-09 18:24:17,269][17778] Stopping Batcher_0...
-[2024-12-09 18:24:17,269][17689] Runner profile tree view:
-main_loop: 33.4653
-[2024-12-09 18:24:17,268][17812] Stopping RolloutWorker_w12...
-[2024-12-09 18:24:17,269][17804] Stopping RolloutWorker_w2...
-[2024-12-09 18:24:17,269][17802] Stopping RolloutWorker_w3...
-[2024-12-09 18:24:17,269][17806] Stopping RolloutWorker_w5...
-[2024-12-09 18:24:17,269][17800] Loop rollout_proc1_evt_loop terminating...
-[2024-12-09 18:24:17,268][17803] Stopping RolloutWorker_w4...
-[2024-12-09 18:24:17,270][17811] Loop rollout_proc11_evt_loop terminating...
-[2024-12-09 18:24:17,270][17809] Stopping RolloutWorker_w9...
-[2024-12-09 18:24:17,269][17807] Loop rollout_proc7_evt_loop terminating...
-[2024-12-09 18:24:17,270][17808] Stopping RolloutWorker_w8...
-[2024-12-09 18:24:17,270][17815] Stopping RolloutWorker_w15...
-[2024-12-09 18:24:17,271][17805] Stopping RolloutWorker_w6...
-[2024-12-09 18:24:17,271][17689] Collected {0: 1020928}, FPS: 3977.9
-[2024-12-09 18:24:17,271][17778] Loop batcher_evt_loop terminating...
-[2024-12-09 18:24:17,270][17799] Loop rollout_proc0_evt_loop terminating...
-[2024-12-09 18:24:17,271][17803] Loop rollout_proc4_evt_loop terminating...
-[2024-12-09 18:24:17,272][17804] Loop rollout_proc2_evt_loop terminating...
-[2024-12-09 18:24:17,272][17802] Loop rollout_proc3_evt_loop terminating...
-[2024-12-09 18:24:17,269][17810] Loop rollout_proc10_evt_loop terminating...
-[2024-12-09 18:24:17,272][17806] Loop rollout_proc5_evt_loop terminating...
-[2024-12-09 18:24:17,272][17809] Loop rollout_proc9_evt_loop terminating...
-[2024-12-09 18:24:17,272][17812] Loop rollout_proc12_evt_loop terminating...
-[2024-12-09 18:24:17,272][17808] Loop rollout_proc8_evt_loop terminating...
-[2024-12-09 18:24:17,273][17815] Loop rollout_proc15_evt_loop terminating...
-[2024-12-09 18:24:17,274][17805] Loop rollout_proc6_evt_loop terminating...
-[2024-12-09 18:24:17,276][17813] Stopping RolloutWorker_w13...
-[2024-12-09 18:24:17,284][17814] Stopping RolloutWorker_w14...
-[2024-12-09 18:24:17,284][17813] Loop rollout_proc13_evt_loop terminating...
-[2024-12-09 18:24:17,283][17778] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000997_1020928.pth...
-[2024-12-09 18:24:17,303][17814] Loop rollout_proc14_evt_loop terminating...
-[2024-12-09 18:24:17,369][17798] Weights refcount: 2 0
-[2024-12-09 18:24:17,372][17798] Stopping InferenceWorker_p0-w0...
-[2024-12-09 18:24:17,374][17798] Loop inference_proc0-0_evt_loop terminating...
-[2024-12-09 18:24:17,389][17778] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000942_964608.pth
-[2024-12-09 18:24:17,409][17778] Stopping LearnerWorker_p0...
-[2024-12-09 18:24:17,412][17778] Loop learner_proc0_evt_loop terminating...
-[2024-12-09 18:29:08,873][19590] Saving configuration to /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/config.json...
-[2024-12-09 18:29:09,903][19590] Rollout worker 0 uses device cpu
-[2024-12-09 18:29:09,904][19590] Rollout worker 1 uses device cpu
-[2024-12-09 18:29:09,905][19590] Rollout worker 2 uses device cpu
-[2024-12-09 18:29:09,906][19590] Rollout worker 3 uses device cpu
-[2024-12-09 18:29:09,907][19590] Rollout worker 4 uses device cpu
-[2024-12-09 18:29:09,909][19590] Rollout worker 5 uses device cpu
-[2024-12-09 18:29:09,910][19590] Rollout worker 6 uses device cpu
-[2024-12-09 18:29:09,911][19590] Rollout worker 7 uses device cpu
-[2024-12-09 18:29:09,912][19590] Rollout worker 8 uses device cpu
-[2024-12-09 18:29:09,914][19590] Rollout worker 9 uses device cpu
-[2024-12-09 18:29:09,915][19590] Rollout worker 10 uses device cpu
-[2024-12-09 18:29:09,917][19590] Rollout worker 11 uses device cpu
-[2024-12-09 18:29:09,919][19590] Rollout worker 12 uses device cpu
-[2024-12-09 18:29:09,920][19590] Rollout worker 13 uses device cpu
-[2024-12-09 18:29:09,922][19590] Rollout worker 14 uses device cpu
-[2024-12-09 18:29:09,924][19590] Rollout worker 15 uses device cpu
-[2024-12-09 18:29:09,976][19590] InferenceWorker_p0-w0: min num requests: 5
-[2024-12-09 18:29:10,091][19590] Starting all processes...
-[2024-12-09 18:29:10,092][19590] Starting process learner_proc0
-[2024-12-09 18:29:10,682][19590] Starting all processes...
-[2024-12-09 18:29:10,704][19590] Starting process inference_proc0-0
-[2024-12-09 18:29:10,704][19590] Starting process rollout_proc0
-[2024-12-09 18:29:10,705][19590] Starting process rollout_proc1
-[2024-12-09 18:29:10,706][19590] Starting process rollout_proc2
-[2024-12-09 18:29:10,706][19590] Starting process rollout_proc3
-[2024-12-09 18:29:10,707][19590] Starting process rollout_proc4
-[2024-12-09 18:29:10,708][19590] Starting process rollout_proc5
-[2024-12-09 18:29:10,708][19590] Starting process rollout_proc6
-[2024-12-09 18:29:10,709][19590] Starting process rollout_proc7
-[2024-12-09 18:29:10,709][19590] Starting process rollout_proc8
-[2024-12-09 18:29:10,710][19590] Starting process rollout_proc9
-[2024-12-09 18:29:10,711][19590] Starting process rollout_proc10
-[2024-12-09 18:29:10,712][19590] Starting process rollout_proc11
-[2024-12-09 18:29:10,712][19590] Starting process rollout_proc12
-[2024-12-09 18:29:10,713][19590] Starting process rollout_proc13
-[2024-12-09 18:29:10,714][19590] Starting process rollout_proc14
-[2024-12-09 18:29:10,734][19590] Starting process rollout_proc15
-[2024-12-09 18:29:20,232][19698] Worker 2 uses CPU cores [2]
-[2024-12-09 18:29:20,402][19699] Worker 4 uses CPU cores [4]
-[2024-12-09 18:29:20,484][19702] Worker 5 uses CPU cores [5]
-[2024-12-09 18:29:20,504][19674] Starting seed is not provided
-[2024-12-09 18:29:20,509][19674] Initializing actor-critic model on device cpu
-[2024-12-09 18:29:20,519][19674] RunningMeanStd input shape: (1, 8, 8)
-[2024-12-09 18:29:20,528][19674] RunningMeanStd input shape: (1,)
-[2024-12-09 18:29:20,532][19707] Worker 11 uses CPU cores [11]
-[2024-12-09 18:29:20,567][19710] Worker 13 uses CPU cores [13]
-[2024-12-09 18:29:20,666][19674] Created Actor Critic model with architecture:
-[2024-12-09 18:29:20,669][19674] ActorCriticSharedWeights(
-  (obs_normalizer): ObservationNormalizer(
-    (running_mean_std): RunningMeanStdDictInPlace(
-      (running_mean_std): ModuleDict(
-        (obs): RunningMeanStdInPlace()
-      )
-    )
-  )
-  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
-  (encoder): CustomEncoder(
-    (conv_head): Sequential(
-      (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2))
-      (1): ELU(alpha=1.0)
-      (2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1))
-      (3): ELU(alpha=1.0)
-    )
-  )
-  (core): ModelCoreRNN(
-    (core): GRU(64, 128)
-  )
-  (decoder): MlpDecoder(
-    (mlp): Identity()
-  )
-  (critic_linear): Linear(in_features=128, out_features=1, bias=True)
-  (action_parameterization): ActionParameterizationDefault(
-    (distribution_linear): Linear(in_features=128, out_features=2, bias=True)
-  )
-)
-[2024-12-09 18:29:20,702][19709] Worker 14 uses CPU cores [14]
-[2024-12-09 18:29:20,752][19701] Worker 6 uses CPU cores [6]
-[2024-12-09 18:29:20,927][19708] Worker 12 uses CPU cores [12]
-[2024-12-09 18:29:21,001][19696] Worker 1 uses CPU cores [1]
-[2024-12-09 18:29:21,023][19705] Worker 9 uses CPU cores [9]
-[2024-12-09 18:29:21,313][19704] Worker 7 uses CPU cores [7]
-[2024-12-09 18:29:21,342][19703] Worker 8 uses CPU cores [8]
-[2024-12-09 18:29:21,352][19697] Worker 3 uses CPU cores [3]
-[2024-12-09 18:29:21,354][19695] Worker 0 uses CPU cores [0]
-[2024-12-09 18:29:21,505][19706] Worker 10 uses CPU cores [10]
-[2024-12-09 18:29:21,741][19711] Worker 15 uses CPU cores [15]
-[2024-12-09 18:29:22,114][19674] Using optimizer <class 'torch.optim.adam.Adam'>
-[2024-12-09 18:29:23,807][19674] Loading state from checkpoint /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000997_1020928.pth...
-[2024-12-09 18:29:23,856][19674] Loading model from checkpoint
-[2024-12-09 18:29:23,858][19674] Loaded experiment state at self.train_step=997, self.env_steps=1020928
-[2024-12-09 18:29:23,860][19674] Initialized policy 0 weights for model version 997
-[2024-12-09 18:29:23,865][19674] LearnerWorker_p0 finished initialization!
-[2024-12-09 18:29:23,873][19674] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000997_1020928.pth...
-[2024-12-09 18:29:23,878][19694] RunningMeanStd input shape: (1, 8, 8)
-[2024-12-09 18:29:23,881][19694] RunningMeanStd input shape: (1,)
-[2024-12-09 18:29:23,932][19590] Inference worker 0-0 is ready!
-[2024-12-09 18:29:23,935][19674] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000997_1020928.pth...
-[2024-12-09 18:29:23,935][19590] All inference workers are ready! Signal rollout workers to start!
-[2024-12-09 18:29:23,943][19711] Decorrelating experience for 0 frames...
-[2024-12-09 18:29:23,943][19706] Decorrelating experience for 0 frames...
-[2024-12-09 18:29:23,944][19708] Decorrelating experience for 0 frames...
-[2024-12-09 18:29:23,944][19699] Decorrelating experience for 0 frames...
-[2024-12-09 18:29:23,945][19697] Decorrelating experience for 0 frames...
-[2024-12-09 18:29:23,945][19698] Decorrelating experience for 0 frames...
-[2024-12-09 18:29:23,943][19696] Decorrelating experience for 0 frames...
-[2024-12-09 18:29:23,950][19704] Decorrelating experience for 0 frames...
-[2024-12-09 18:29:23,952][19702] Decorrelating experience for 0 frames...
-[2024-12-09 18:29:23,943][19695] Decorrelating experience for 0 frames...
-[2024-12-09 18:29:23,957][19696] Decorrelating experience for 32 frames...
-[2024-12-09 18:29:23,957][19699] Decorrelating experience for 32 frames...
-[2024-12-09 18:29:23,957][19711] Decorrelating experience for 32 frames...
-[2024-12-09 18:29:23,957][19704] Decorrelating experience for 32 frames...
-[2024-12-09 18:29:23,957][19698] Decorrelating experience for 32 frames...
-[2024-12-09 18:29:23,958][19697] Decorrelating experience for 32 frames...
-[2024-12-09 18:29:23,958][19708] Decorrelating experience for 32 frames...
-[2024-12-09 18:29:23,959][19705] Decorrelating experience for 0 frames...
-[2024-12-09 18:29:23,959][19703] Decorrelating experience for 0 frames...
-[2024-12-09 18:29:23,960][19706] Decorrelating experience for 32 frames...
-[2024-12-09 18:29:23,964][19702] Decorrelating experience for 32 frames...
-[2024-12-09 18:29:23,964][19695] Decorrelating experience for 32 frames...
-[2024-12-09 18:29:23,964][19707] Decorrelating experience for 0 frames...
-[2024-12-09 18:29:23,967][19710] Decorrelating experience for 0 frames...
-[2024-12-09 18:29:23,969][19709] Decorrelating experience for 0 frames...
-[2024-12-09 18:29:23,970][19703] Decorrelating experience for 32 frames...
-[2024-12-09 18:29:23,972][19707] Decorrelating experience for 32 frames...
-[2024-12-09 18:29:23,974][19710] Decorrelating experience for 32 frames...
-[2024-12-09 18:29:23,976][19709] Decorrelating experience for 32 frames...
-[2024-12-09 18:29:23,974][19701] Decorrelating experience for 0 frames...
-[2024-12-09 18:29:23,980][19701] Decorrelating experience for 32 frames...
-[2024-12-09 18:29:23,991][19697] Worker 3-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:23,991][19698] Worker 2-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:23,991][19704] Worker 7-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:23,995][19705] Decorrelating experience for 32 frames...
-[2024-12-09 18:29:24,001][19708] Worker 12-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:24,002][19711] Worker 15-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:24,002][19703] Worker 8-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:24,002][19702] Worker 5-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:24,003][19709] Worker 14-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:24,003][19699] Worker 4-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:24,004][19701] Worker 6-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:24,004][19696] Worker 1-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:24,005][19707] Worker 11-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:24,005][19710] Worker 13-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:24,007][19695] Worker 0-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:24,013][19706] Worker 10-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:24,016][19705] Worker 9-0 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:24,040][19674] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000997_1020928.pth...
-[2024-12-09 18:29:24,279][19706] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:29:24,321][19701] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:29:24,322][19698] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:29:24,328][19697] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:29:24,329][19696] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:29:24,329][19710] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:29:24,334][19705] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:29:24,335][19711] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:29:24,336][19709] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:29:24,343][19707] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:29:24,344][19704] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:29:24,350][19708] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:29:24,356][19703] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:29:24,376][19702] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:29:24,378][19695] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:29:24,398][19699] Multiple policies in trajectory buffer: [-1  0] (-1 means inactive agent)
-[2024-12-09 18:29:24,419][19674] Signal inference workers to stop experience collection...
-[2024-12-09 18:29:24,427][19694] InferenceWorker_p0-w0: stopping experience collection
-[2024-12-09 18:29:24,526][19590] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 1020928. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
-[2024-12-09 18:29:24,527][19590] Avg episode reward: [(0, '0.000')]
-[2024-12-09 18:29:24,596][19674] Signal inference workers to resume experience collection...
-[2024-12-09 18:29:24,598][19694] InferenceWorker_p0-w0: resuming experience collection
-[2024-12-09 18:29:24,692][19674] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000999_1022976.pth...
-[2024-12-09 18:29:24,762][19674] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000974_997376.pth
-[2024-12-09 18:29:24,888][19705] Worker 9-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:25,853][19695] Worker 0-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:26,044][19710] Worker 13-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:26,114][19694] Updated weights for policy 0, policy_version 1007 (0.0017)
-[2024-12-09 18:29:26,400][19709] Worker 14-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:26,949][19707] Worker 11-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:27,676][19706] Worker 10-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:27,757][19694] Updated weights for policy 0, policy_version 1017 (0.0012)
-[2024-12-09 18:29:29,377][19694] Updated weights for policy 0, policy_version 1027 (0.0012)
-[2024-12-09 18:29:29,526][19590] Fps is (10 sec: 6347.9, 60 sec: 6347.9, 300 sec: 6347.9). Total num frames: 1052672. Throughput: 0: 2993.4. Samples: 14969. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)
-[2024-12-09 18:29:29,528][19590] Avg episode reward: [(0, '0.000')]
-[2024-12-09 18:29:29,533][19674] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000001028_1052672.pth...
-[2024-12-09 18:29:29,546][19703] Worker 8-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:29,596][19674] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000997_1020928.pth
-[2024-12-09 18:29:29,960][19590] Heartbeat connected on Batcher_0
-[2024-12-09 18:29:29,986][19590] Heartbeat connected on InferenceWorker_p0-w0
-[2024-12-09 18:29:29,987][19590] Heartbeat connected on RolloutWorker_w0
-[2024-12-09 18:29:29,994][19590] Heartbeat connected on RolloutWorker_w1
-[2024-12-09 18:29:30,007][19590] Heartbeat connected on RolloutWorker_w3
-[2024-12-09 18:29:30,014][19590] Heartbeat connected on RolloutWorker_w2
-[2024-12-09 18:29:30,021][19590] Heartbeat connected on RolloutWorker_w5
-[2024-12-09 18:29:30,026][19590] Heartbeat connected on RolloutWorker_w4
-[2024-12-09 18:29:30,028][19590] Heartbeat connected on RolloutWorker_w6
-[2024-12-09 18:29:30,061][19590] Heartbeat connected on LearnerWorker_p0
-[2024-12-09 18:29:30,064][19590] Heartbeat connected on RolloutWorker_w7
-[2024-12-09 18:29:30,065][19590] Heartbeat connected on RolloutWorker_w8
-[2024-12-09 18:29:30,067][19590] Heartbeat connected on RolloutWorker_w9
-[2024-12-09 18:29:30,068][19590] Heartbeat connected on RolloutWorker_w10
-[2024-12-09 18:29:30,069][19590] Heartbeat connected on RolloutWorker_w11
-[2024-12-09 18:29:30,072][19590] Heartbeat connected on RolloutWorker_w12
-[2024-12-09 18:29:30,076][19590] Heartbeat connected on RolloutWorker_w13
-[2024-12-09 18:29:30,084][19590] Heartbeat connected on RolloutWorker_w14
-[2024-12-09 18:29:30,091][19590] Heartbeat connected on RolloutWorker_w15
-[2024-12-09 18:29:30,602][19702] Worker 5-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:30,846][19698] Worker 2-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:31,101][19694] Updated weights for policy 0, policy_version 1037 (0.0014)
-[2024-12-09 18:29:32,922][19694] Updated weights for policy 0, policy_version 1047 (0.0015)
-[2024-12-09 18:29:34,009][19704] Worker 7-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:34,438][19697] Worker 3-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:34,526][19590] Fps is (10 sec: 5836.6, 60 sec: 5836.6, 300 sec: 5836.6). Total num frames: 1079296. Throughput: 0: 4732.0. Samples: 47321. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)
-[2024-12-09 18:29:34,528][19590] Avg episode reward: [(0, '0.000')]
-[2024-12-09 18:29:34,552][19674] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000001055_1080320.pth...
-[2024-12-09 18:29:34,644][19674] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000000999_1022976.pth
-[2024-12-09 18:29:35,011][19694] Updated weights for policy 0, policy_version 1057 (0.0016)
-[2024-12-09 18:29:36,669][19694] Updated weights for policy 0, policy_version 1067 (0.0014)
-[2024-12-09 18:29:36,723][19701] Worker 6-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:37,032][19696] Worker 1-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:38,275][19694] Updated weights for policy 0, policy_version 1077 (0.0012)
-[2024-12-09 18:29:39,525][19590] Fps is (10 sec: 5837.3, 60 sec: 6007.5, 300 sec: 6007.5). Total num frames: 1111040. Throughput: 0: 5252.3. Samples: 78784. Policy #0 lag: (min: 0.0, avg: 1.6, max: 3.0)
-[2024-12-09 18:29:39,527][19590] Avg episode reward: [(0, '0.000')]
-[2024-12-09 18:29:39,533][19674] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000001085_1111040.pth...
-[2024-12-09 18:29:39,600][19674] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000001028_1052672.pth
-[2024-12-09 18:29:39,827][19694] Updated weights for policy 0, policy_version 1087 (0.0011)
-[2024-12-09 18:29:41,354][19694] Updated weights for policy 0, policy_version 1097 (0.0014)
-[2024-12-09 18:29:41,907][19699] Worker 4-1 has no active agents... We immediately continue to the next iteration without notifying the inference worker
-[2024-12-09 18:29:42,963][19694] Updated weights for policy 0, policy_version 1107 (0.0012)
-[2024-12-09 18:29:44,526][19590] Fps is (10 sec: 6348.8, 60 sec: 6092.7, 300 sec: 6092.7). Total num frames: 1142784. Throughput: 0: 4825.4. Samples: 96509. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)
-[2024-12-09 18:29:44,527][19590] Avg episode reward: [(0, '0.000')]
-[2024-12-09 18:29:44,616][19694] Updated weights for policy 0, policy_version 1117 (0.0012)
-[2024-12-09 18:29:44,619][19674] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000001117_1143808.pth...
-[2024-12-09 18:29:44,688][19674] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000001055_1080320.pth
-[2024-12-09 18:29:45,109][19590] Keyboard interrupt detected in the event loop EvtLoop [Runner_EvtLoop, process=main process 19590], exiting...
-[2024-12-09 18:29:45,115][19699] Stopping RolloutWorker_w4...
-[2024-12-09 18:29:45,116][19711] Stopping RolloutWorker_w15...
-[2024-12-09 18:29:45,116][19710] Stopping RolloutWorker_w13...
-[2024-12-09 18:29:45,117][19707] Stopping RolloutWorker_w11...
-[2024-12-09 18:29:45,118][19698] Stopping RolloutWorker_w2...
-[2024-12-09 18:29:45,116][19674] Stopping Batcher_0...
-[2024-12-09 18:29:45,117][19709] Stopping RolloutWorker_w14...
-[2024-12-09 18:29:45,120][19699] Loop rollout_proc4_evt_loop terminating...
-[2024-12-09 18:29:45,120][19711] Loop rollout_proc15_evt_loop terminating...
-[2024-12-09 18:29:45,121][19698] Loop rollout_proc2_evt_loop terminating...
-[2024-12-09 18:29:45,122][19709] Loop rollout_proc14_evt_loop terminating...
-[2024-12-09 18:29:45,123][19705] Stopping RolloutWorker_w9...
-[2024-12-09 18:29:45,123][19707] Loop rollout_proc11_evt_loop terminating...
-[2024-12-09 18:29:45,124][19702] Stopping RolloutWorker_w5...
-[2024-12-09 18:29:45,124][19674] Loop batcher_evt_loop terminating...
-[2024-12-09 18:29:45,125][19590] Runner profile tree view:
-main_loop: 35.0341
-[2024-12-09 18:29:45,126][19697] Stopping RolloutWorker_w3...
-[2024-12-09 18:29:45,126][19696] Stopping RolloutWorker_w1...
-[2024-12-09 18:29:45,120][19710] Loop rollout_proc13_evt_loop terminating...
-[2024-12-09 18:29:45,129][19705] Loop rollout_proc9_evt_loop terminating...
-[2024-12-09 18:29:45,129][19674] Saving /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000001119_1145856.pth...
-[2024-12-09 18:29:45,129][19708] Stopping RolloutWorker_w12...
-[2024-12-09 18:29:45,130][19696] Loop rollout_proc1_evt_loop terminating...
-[2024-12-09 18:29:45,131][19697] Loop rollout_proc3_evt_loop terminating...
-[2024-12-09 18:29:45,133][19708] Loop rollout_proc12_evt_loop terminating...
-[2024-12-09 18:29:45,129][19702] Loop rollout_proc5_evt_loop terminating...
-[2024-12-09 18:29:45,126][19701] Stopping RolloutWorker_w6...
-[2024-12-09 18:29:45,136][19590] Collected {0: 1145856}, FPS: 3565.9
-[2024-12-09 18:29:45,132][19703] Stopping RolloutWorker_w8...
-[2024-12-09 18:29:45,144][19695] Stopping RolloutWorker_w0...
-[2024-12-09 18:29:45,143][19706] Stopping RolloutWorker_w10...
-[2024-12-09 18:29:45,145][19703] Loop rollout_proc8_evt_loop terminating...
-[2024-12-09 18:29:45,143][19704] Stopping RolloutWorker_w7...
-[2024-12-09 18:29:45,163][19704] Loop rollout_proc7_evt_loop terminating...
-[2024-12-09 18:29:45,163][19706] Loop rollout_proc10_evt_loop terminating...
-[2024-12-09 18:29:45,163][19695] Loop rollout_proc0_evt_loop terminating...
-[2024-12-09 18:29:45,173][19701] Loop rollout_proc6_evt_loop terminating...
-[2024-12-09 18:29:45,470][19674] Removing /mnt/c/Users/Daniel/OneDrive/Ambiente de Trabalho/MIA/TSI/proj/gym-pybullet-drones/experiments/learning/train_dir/example_multi/checkpoint_p0/checkpoint_000001085_1111040.pth
-[2024-12-09 18:29:45,496][19674] Stopping LearnerWorker_p0...
-[2024-12-09 18:29:45,499][19674] Loop learner_proc0_evt_loop terminating...
-[2024-12-09 18:29:45,533][19694] Weights refcount: 2 0
-[2024-12-09 18:29:45,551][19694] Stopping InferenceWorker_p0-w0...
-[2024-12-09 18:29:45,567][19694] Loop inference_proc0-0_evt_loop terminating...
diff --git a/gym_pybullet_drones/envs/BaseAviary.py b/gym_pybullet_drones/envs/BaseAviary.py
index 42ccd57..cf10547 100644
--- a/gym_pybullet_drones/envs/BaseAviary.py
+++ b/gym_pybullet_drones/envs/BaseAviary.py
@@ -1,1095 +1,1101 @@
-import os
-from sys import platform
-import time
-import collections
-from datetime import datetime
-from enum import Enum
-import xml.etree.ElementTree as etxml
-from PIL import Image
-import pkgutil
-egl = pkgutil.get_loader('eglRenderer')
-import numpy as np
-import pybullet as p
-import pybullet_data
-import gymnasium as gym
-
-class DroneModel(Enum):
-    """Drone models enumeration class."""
-
-    CF2X = "cf2x"   # Bitcraze Craziflie 2.0 in the X configuration
-    CF2P = "cf2p"   # Bitcraze Craziflie 2.0 in the + configuration
-    HB = "hb"       # Generic quadrotor (with AscTec Hummingbird inertial properties)
-
-################################################################################
-
-class Physics(Enum):
-    """Physics implementations enumeration class."""
-
-    PYB = "pyb"                         # Base PyBullet physics update
-    DYN = "dyn"                         # Update with an explicit model of the dynamics
-    PYB_GND = "pyb_gnd"                 # PyBullet physics update with ground effect
-    PYB_DRAG = "pyb_drag"               # PyBullet physics update with drag
-    PYB_DW = "pyb_dw"                   # PyBullet physics update with downwash
-    PYB_GND_DRAG_DW = "pyb_gnd_drag_dw" # PyBullet physics update with ground effect, drag, and downwash
-
-################################################################################
-
-class ImageType(Enum):
-    """Camera capture image type enumeration class."""
-
-    RGB = 0     # Red, green, blue (and alpha)
-    DEP = 1     # Depth
-    SEG = 2     # Segmentation by object id
-    BW = 3      # Black and white
-
-################################################################################
-
-class BaseAviary(gym.Env):
-    """Base class for "drone aviary" Gym environments."""
-
-    metadata = {'render.modes': ['human']}
-    
-    ################################################################################
-
-    def __init__(self,
-                 drone_model: DroneModel=DroneModel.CF2X,
-                 num_drones: int=1,
-                 neighbourhood_radius: float=np.inf,
-                 initial_xyzs=None,
-                 initial_rpys=None,
-                 physics: Physics=Physics.PYB,
-                 freq: int=240,
-                 aggregate_phy_steps: int=1,
-                 gui=False,
-                 record=False,
-                 obstacles=False,
-                 user_debug_gui=True,
-                 vision_attributes=False,
-                 dynamics_attributes=False
-                 ):
-        """Initialization of a generic aviary environment.
-
-        Parameters
-        ----------
-        drone_model : DroneModel, optional
-            The desired drone type (detailed in an .urdf file in folder `assets`).
-        num_drones : int, optional
-            The desired number of drones in the aviary.
-        neighbourhood_radius : float, optional
-            Radius used to compute the drones' adjacency matrix, in meters.
-        initial_xyzs: ndarray | None, optional
-            (NUM_DRONES, 3)-shaped array containing the initial XYZ position of the drones.
-        initial_rpys: ndarray | None, optional
-            (NUM_DRONES, 3)-shaped array containing the initial orientations of the drones (in radians).
-        physics : Physics, optional
-            The desired implementation of PyBullet physics/custom dynamics.
-        freq : int, optional
-            The frequency (Hz) at which the physics engine steps.
-        aggregate_phy_steps : int, optional
-            The number of physics steps within one call to `BaseAviary.step()`.
-        gui : bool, optional
-            Whether to use PyBullet's GUI.
-        record : bool, optional
-            Whether to save a video of the simulation in folder `files/videos/`.
-        obstacles : bool, optional
-            Whether to add obstacles to the simulation.
-        user_debug_gui : bool, optional
-            Whether to draw the drones' axes and the GUI RPMs sliders.
-        vision_attributes : bool, optional
-            Whether to allocate the attributes needed by vision-based aviary subclasses.
-        dynamics_attributes : bool, optional
-            Whether to allocate the attributes needed by subclasses accepting thrust and torques inputs.
-
-        """
-        #### Constants #############################################
-        self.G = 9.8
-        self.RAD2DEG = 180/np.pi
-        self.DEG2RAD = np.pi/180
-        self.SIM_FREQ = freq
-        self.TIMESTEP = 1./self.SIM_FREQ
-        self.AGGR_PHY_STEPS = aggregate_phy_steps
-        #### Parameters ############################################
-        self.NUM_DRONES = num_drones
-        self.NEIGHBOURHOOD_RADIUS = neighbourhood_radius
-        #### Options ###############################################
-        self.DRONE_MODEL = drone_model
-        self.GUI = gui
-        self.RECORD = record
-        self.PHYSICS = physics
-        self.OBSTACLES = obstacles
-        self.USER_DEBUG = user_debug_gui
-        self.URDF = "cf2x" + ".urdf"
-        #### Load the drone properties from the .urdf file #########
-        self.M, \
-        self.L, \
-        self.THRUST2WEIGHT_RATIO, \
-        self.J, \
-        self.J_INV, \
-        self.KF, \
-        self.KM, \
-        self.COLLISION_H,\
-        self.COLLISION_R, \
-        self.COLLISION_Z_OFFSET, \
-        self.MAX_SPEED_KMH, \
-        self.GND_EFF_COEFF, \
-        self.PROP_RADIUS, \
-        self.DRAG_COEFF, \
-        self.DW_COEFF_1, \
-        self.DW_COEFF_2, \
-        self.DW_COEFF_3 = self._parseURDFParameters()
-        print("[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:\n[INFO] m {:f}, L {:f},\n[INFO] ixx {:f}, iyy {:f}, izz {:f},\n[INFO] kf {:f}, km {:f},\n[INFO] t2w {:f}, max_speed_kmh {:f},\n[INFO] gnd_eff_coeff {:f}, prop_radius {:f},\n[INFO] drag_xy_coeff {:f}, drag_z_coeff {:f},\n[INFO] dw_coeff_1 {:f}, dw_coeff_2 {:f}, dw_coeff_3 {:f}".format(
-            self.M, self.L, self.J[0,0], self.J[1,1], self.J[2,2], self.KF, self.KM, self.THRUST2WEIGHT_RATIO, self.MAX_SPEED_KMH, self.GND_EFF_COEFF, self.PROP_RADIUS, self.DRAG_COEFF[0], self.DRAG_COEFF[2], self.DW_COEFF_1, self.DW_COEFF_2, self.DW_COEFF_3))
-        #### Compute constants #####################################
-        self.GRAVITY = self.G*self.M
-        self.HOVER_RPM = np.sqrt(self.GRAVITY / (4*self.KF))
-        self.MAX_RPM = np.sqrt((self.THRUST2WEIGHT_RATIO*self.GRAVITY) / (4*self.KF))
-        self.MAX_THRUST = (4*self.KF*self.MAX_RPM**2)
-        if self.DRONE_MODEL == DroneModel.CF2X:
-            self.MAX_XY_TORQUE = (2*self.L*self.KF*self.MAX_RPM**2)/np.sqrt(2)
-        elif self.DRONE_MODEL in [DroneModel.CF2P, DroneModel.HB]:
-            self.MAX_XY_TORQUE = (self.L*self.KF*self.MAX_RPM**2)
-        self.MAX_Z_TORQUE = (2*self.KM*self.MAX_RPM**2)
-        self.GND_EFF_H_CLIP = 0.25 * self.PROP_RADIUS * np.sqrt((15 * self.MAX_RPM**2 * self.KF * self.GND_EFF_COEFF) / self.MAX_THRUST)
-        #### Create attributes for vision tasks ####################
-        self.VISION_ATTR = vision_attributes
-        if self.VISION_ATTR:
-            self.IMG_RES = np.array([64, 48])
-            self.IMG_FRAME_PER_SEC = 24
-            self.IMG_CAPTURE_FREQ = int(self.SIM_FREQ/self.IMG_FRAME_PER_SEC)
-            self.rgb = np.zeros(((self.NUM_DRONES, self.IMG_RES[1], self.IMG_RES[0], 4)))
-            self.dep = np.ones(((self.NUM_DRONES, self.IMG_RES[1], self.IMG_RES[0])))
-            self.seg = np.zeros(((self.NUM_DRONES, self.IMG_RES[1], self.IMG_RES[0])))
-            if self.IMG_CAPTURE_FREQ%self.AGGR_PHY_STEPS != 0:
-                print("[ERROR] in BaseAviary.__init__(), aggregate_phy_steps incompatible with the desired video capture frame rate ({:f}Hz)".format(self.IMG_FRAME_PER_SEC))
-                exit()
-            if self.RECORD:
-                self.ONBOARD_IMG_PATH = os.path.dirname(os.path.abspath(__file__))+"/../../files/videos/onboard-"+datetime.now().strftime("%m.%d.%Y_%H.%M.%S")+"/"
-                os.makedirs(os.path.dirname(self.ONBOARD_IMG_PATH), exist_ok=True)
-        #### Create attributes for dynamics control inputs #########
-        self.DYNAMICS_ATTR = dynamics_attributes
-        if self.DYNAMICS_ATTR:
-            if self.DRONE_MODEL == DroneModel.CF2X:
-                self.A = np.array([ [1, 1, 1, 1], [1/np.sqrt(2), 1/np.sqrt(2), -1/np.sqrt(2), -1/np.sqrt(2)], [-1/np.sqrt(2), 1/np.sqrt(2), 1/np.sqrt(2), -1/np.sqrt(2)], [-1, 1, -1, 1] ])
-            elif self.DRONE_MODEL in [DroneModel.CF2P, DroneModel.HB]:
-                self.A = np.array([ [1, 1, 1, 1], [0, 1, 0, -1], [-1, 0, 1, 0], [-1, 1, -1, 1] ])
-            self.INV_A = np.linalg.inv(self.A)
-            self.B_COEFF = np.array([1/self.KF, 1/(self.KF*self.L), 1/(self.KF*self.L), 1/self.KM])
-        #### Connect to PyBullet ###################################
-        if self.GUI:
-            #### With debug GUI ########################################
-            self.CLIENT = p.connect(p.GUI) # p.connect(p.GUI, options="--opengl2")
-            for i in [p.COV_ENABLE_RGB_BUFFER_PREVIEW, p.COV_ENABLE_DEPTH_BUFFER_PREVIEW, p.COV_ENABLE_SEGMENTATION_MARK_PREVIEW]:
-                p.configureDebugVisualizer(i, 0, physicsClientId=self.CLIENT)
-            p.resetDebugVisualizerCamera(cameraDistance=3,
-                                         cameraYaw=-30,
-                                         cameraPitch=-30,
-                                         cameraTargetPosition=[0, 0, 0],
-                                         physicsClientId=self.CLIENT
-                                         )
-            ret = p.getDebugVisualizerCamera(physicsClientId=self.CLIENT)
-            print("viewMatrix", ret[2])
-            print("projectionMatrix", ret[3])
-            if self.USER_DEBUG:
-                #### Add input sliders to the GUI ##########################
-                self.SLIDERS = -1*np.ones(4)
-                for i in range(4):
-                    self.SLIDERS[i] = p.addUserDebugParameter("Propeller "+str(i)+" RPM", 0, self.MAX_RPM, self.HOVER_RPM, physicsClientId=self.CLIENT)
-                self.INPUT_SWITCH = p.addUserDebugParameter("Use GUI RPM", 9999, -1, 0, physicsClientId=self.CLIENT)
-        else:
-            #### Without debug GUI #####################################
-            self.CLIENT = p.connect(p.DIRECT)
-            #### Uncomment the following line to use EGL Render Plugin #
-            #### Instead of TinyRender (CPU-based) in PYB's Direct mode
-            # if platform == "linux": p.setAdditionalSearchPath(pybullet_data.getDataPath()); plugin = p.loadPlugin(egl.get_filename(), "_eglRendererPlugin"); print("plugin=", plugin)
-            if self.RECORD:
-                #### Set the camera parameters to save frames in DIRECT mode
-                self.VID_WIDTH=int(640)
-                self.VID_HEIGHT=int(480)
-                self.FRAME_PER_SEC = 24
-                self.CAPTURE_FREQ = int(self.SIM_FREQ/self.FRAME_PER_SEC)
-                self.CAM_VIEW = p.computeViewMatrixFromYawPitchRoll(distance=3,
-                                                                    yaw=-30,
-                                                                    pitch=-30,
-                                                                    roll=0,
-                                                                    cameraTargetPosition=[0, 0, 0],
-                                                                    upAxisIndex=2,
-                                                                    physicsClientId=self.CLIENT
-                                                                    )
-                self.CAM_PRO = p.computeProjectionMatrixFOV(fov=60.0,
-                                                            aspect=self.VID_WIDTH/self.VID_HEIGHT,
-                                                            nearVal=0.1,
-                                                            farVal=1000.0
-                                                            )
-        #### Set initial poses #####################################
-        if initial_xyzs is None:
-            self.INIT_XYZS = np.vstack([np.array([x*4*self.L for x in range(self.NUM_DRONES)]), \
-                                        np.array([y*4*self.L for y in range(self.NUM_DRONES)]), \
-                                        np.ones(self.NUM_DRONES) * (self.COLLISION_H/2-self.COLLISION_Z_OFFSET+.1)]).transpose().reshape(self.NUM_DRONES, 3)
-        elif np.array(initial_xyzs).shape == (self.NUM_DRONES,3):
-            self.INIT_XYZS = initial_xyzs
-        else:
-            print("[ERROR] invalid initial_xyzs in BaseAviary.__init__(), try initial_xyzs.reshape(NUM_DRONES,3)")
-        if initial_rpys is None:
-            self.INIT_RPYS = np.zeros((self.NUM_DRONES, 3))
-        elif np.array(initial_rpys).shape == (self.NUM_DRONES, 3):
-            self.INIT_RPYS = initial_rpys
-        else:
-            print("[ERROR] invalid initial_rpys in BaseAviary.__init__(), try initial_rpys.reshape(NUM_DRONES,3)")
-        #### Create action and observation spaces ##################
-        self.action_space = self._actionSpace()
-        self.observation_space = self._observationSpace()
-        #### Housekeeping ##########################################
-        self._housekeeping()
-        #### Update and store the drones kinematic information #####
-        self._updateAndStoreKinematicInformation()
-        #### Start video recording #################################
-        self._startVideoRecording()
-    
-    ################################################################################
-
-    def reset(self):
-        """Resets the environment.
-
-        Returns
-        -------
-        ndarray | dict[..]
-            The initial observation, check the specific implementation of `_computeObs()`
-            in each subclass for its format.
-
-        """
-        p.resetSimulation(physicsClientId=self.CLIENT)
-        #### Housekeeping ##########################################
-        self._housekeeping()
-        #### Update and store the drones kinematic information #####
-        self._updateAndStoreKinematicInformation()
-        #### Start video recording #################################
-        self._startVideoRecording()
-        #### Return the initial observation ########################
-        return self._computeObs()
-    
-    ################################################################################
-
-    def step(self,
-             action
-             ):
-        """Advances the environment by one simulation step.
-
-        Parameters
-        ----------
-        action : ndarray | dict[..]
-            The input action for one or more drones, translated into RPMs by
-            the specific implementation of `_preprocessAction()` in each subclass.
-
-        Returns
-        -------
-        ndarray | dict[..]
-            The step's observation, check the specific implementation of `_computeObs()`
-            in each subclass for its format.
-        float | dict[..]
-            The step's reward value(s), check the specific implementation of `_computeReward()`
-            in each subclass for its format.
-        bool | dict[..]
-            Whether the current epoisode is over, check the specific implementation of `_computeDone()`
-            in each subclass for its format.
-        dict[..]
-            Additional information as a dictionary, check the specific implementation of `_computeInfo()`
-            in each subclass for its format.
-
-        """
-        #### Save PNG video frames if RECORD=True and GUI=False ####
-        if self.RECORD and not self.GUI and self.step_counter%self.CAPTURE_FREQ == 0:
-            [w, h, rgb, dep, seg] = p.getCameraImage(width=self.VID_WIDTH,
-                                                     height=self.VID_HEIGHT,
-                                                     shadow=1,
-                                                     viewMatrix=self.CAM_VIEW,
-                                                     projectionMatrix=self.CAM_PRO,
-                                                     renderer=p.ER_TINY_RENDERER,
-                                                     flags=p.ER_SEGMENTATION_MASK_OBJECT_AND_LINKINDEX,
-                                                     physicsClientId=self.CLIENT
-                                                     )
-            (Image.fromarray(np.reshape(rgb, (h, w, 4)), 'RGBA')).save(self.IMG_PATH+"frame_"+str(self.FRAME_NUM)+".png")
-            #### Save the depth or segmentation view instead #######
-            # dep = ((dep-np.min(dep)) * 255 / (np.max(dep)-np.min(dep))).astype('uint8')
-            # (Image.fromarray(np.reshape(dep, (h, w)))).save(self.IMG_PATH+"frame_"+str(self.FRAME_NUM)+".png")
-            # seg = ((seg-np.min(seg)) * 255 / (np.max(seg)-np.min(seg))).astype('uint8')
-            # (Image.fromarray(np.reshape(seg, (h, w)))).save(self.IMG_PATH+"frame_"+str(self.FRAME_NUM)+".png")
-            self.FRAME_NUM += 1
-        #### Read the GUI's input parameters #######################
-        if self.GUI and self.USER_DEBUG:
-            current_input_switch = p.readUserDebugParameter(self.INPUT_SWITCH, physicsClientId=self.CLIENT)
-            if current_input_switch > self.last_input_switch:
-                self.last_input_switch = current_input_switch
-                self.USE_GUI_RPM = True if self.USE_GUI_RPM == False else False
-        if self.USE_GUI_RPM:
-            for i in range(4):
-                self.gui_input[i] = p.readUserDebugParameter(int(self.SLIDERS[i]), physicsClientId=self.CLIENT)
-            clipped_action = np.tile(self.gui_input, (self.NUM_DRONES, 1))
-            if self.step_counter%(self.SIM_FREQ/2) == 0:
-                self.GUI_INPUT_TEXT = [p.addUserDebugText("Using GUI RPM",
-                                                          textPosition=[0, 0, 0],
-                                                          textColorRGB=[1, 0, 0],
-                                                          lifeTime=1,
-                                                          textSize=2,
-                                                          parentObjectUniqueId=self.DRONE_IDS[i],
-                                                          parentLinkIndex=-1,
-                                                          replaceItemUniqueId=int(self.GUI_INPUT_TEXT[i]),
-                                                          physicsClientId=self.CLIENT
-                                                          ) for i in range(self.NUM_DRONES)]
-        #### Save, preprocess, and clip the action to the max. RPM #
-        else:
-            self._saveLastAction(action)
-            clipped_action = np.reshape(self._preprocessAction(action), (self.NUM_DRONES, 4))
-        #### Repeat for as many as the aggregate physics steps #####
-        for _ in range(self.AGGR_PHY_STEPS):
-            #### Update and store the drones kinematic info for certain
-            #### Between aggregate steps for certain types of update ###
-            if self.AGGR_PHY_STEPS > 1 and self.PHYSICS in [Physics.DYN, Physics.PYB_GND, Physics.PYB_DRAG, Physics.PYB_DW, Physics.PYB_GND_DRAG_DW]:
-                self._updateAndStoreKinematicInformation()
-            #### Step the simulation using the desired physics update ##
-            for i in range (self.NUM_DRONES):
-                if self.PHYSICS == Physics.PYB:
-                    self._physics(clipped_action[i, :], i)
-                elif self.PHYSICS == Physics.DYN:
-                    self._dynamics(clipped_action[i, :], i)
-                elif self.PHYSICS == Physics.PYB_GND:
-                    self._physics(clipped_action[i, :], i)
-                    self._groundEffect(clipped_action[i, :], i)
-                elif self.PHYSICS == Physics.PYB_DRAG:
-                    self._physics(clipped_action[i, :], i)
-                    self._drag(self.last_clipped_action[i, :], i)
-                elif self.PHYSICS == Physics.PYB_DW:
-                    self._physics(clipped_action[i, :], i)
-                    self._downwash(i)
-                elif self.PHYSICS == Physics.PYB_GND_DRAG_DW:
-                    self._physics(clipped_action[i, :], i)
-                    self._groundEffect(clipped_action[i, :], i)
-                    self._drag(self.last_clipped_action[i, :], i)
-                    self._downwash(i)
-            #### PyBullet computes the new state, unless Physics.DYN ###
-            if self.PHYSICS != Physics.DYN:
-                p.stepSimulation(physicsClientId=self.CLIENT)
-            #### Save the last applied action (e.g. to compute drag) ###
-            self.last_clipped_action = clipped_action
-        #### Update and store the drones kinematic information #####
-        self._updateAndStoreKinematicInformation()
-        #### Prepare the return values #############################
-        obs = self._computeObs()
-        reward = self._computeReward()
-        done = self._computeDone()
-        info = self._computeInfo()
-        #### Advance the step counter ##############################
-        self.step_counter = self.step_counter + (1 * self.AGGR_PHY_STEPS)
-        return obs, reward, done, info
-    
-    ################################################################################
-    
-    def render(self,
-               mode='human',
-               close=False
-               ):
-        """Prints a textual output of the environment.
-
-        Parameters
-        ----------
-        mode : str, optional
-            Unused.
-        close : bool, optional
-            Unused.
-
-        """
-        if self.first_render_call and not self.GUI:
-            print("[WARNING] BaseAviary.render() is implemented as text-only, re-initialize the environment using Aviary(gui=True) to use PyBullet's graphical interface")
-            self.first_render_call = False
-        print("\n[INFO] BaseAviary.render() ——— it {:04d}".format(self.step_counter),
-              "——— wall-clock time {:.1f}s,".format(time.time()-self.RESET_TIME),
-              "simulation time {:.1f}s@{:d}Hz ({:.2f}x)".format(self.step_counter*self.TIMESTEP, self.SIM_FREQ, (self.step_counter*self.TIMESTEP)/(time.time()-self.RESET_TIME)))
-        for i in range (self.NUM_DRONES):
-            print("[INFO] BaseAviary.render() ——— drone {:d}".format(i),
-                  "——— x {:+06.2f}, y {:+06.2f}, z {:+06.2f}".format(self.pos[i, 0], self.pos[i, 1], self.pos[i, 2]),
-                  "——— velocity {:+06.2f}, {:+06.2f}, {:+06.2f}".format(self.vel[i, 0], self.vel[i, 1], self.vel[i, 2]),
-                  "——— roll {:+06.2f}, pitch {:+06.2f}, yaw {:+06.2f}".format(self.rpy[i, 0]*self.RAD2DEG, self.rpy[i, 1]*self.RAD2DEG, self.rpy[i, 2]*self.RAD2DEG),
-                  "——— angular velocity {:+06.4f}, {:+06.4f}, {:+06.4f} ——— ".format(self.ang_v[i, 0], self.ang_v[i, 1], self.ang_v[i, 2]))
-    
-    ################################################################################
-
-    def close(self):
-        """Terminates the environment.
-        """
-        if self.RECORD and self.GUI:
-            p.stopStateLogging(self.VIDEO_ID, physicsClientId=self.CLIENT)
-        p.disconnect(physicsClientId=self.CLIENT)
-    
-    ################################################################################
-
-    def getPyBulletClient(self):
-        """Returns the PyBullet Client Id.
-
-        Returns
-        -------
-        int:
-            The PyBullet Client Id.
-
-        """
-        return self.CLIENT
-    
-    ################################################################################
-
-    def getDroneIds(self):
-        """Return the Drone Ids.
-
-        Returns
-        -------
-        ndarray:
-            (NUM_DRONES,)-shaped array of ints containing the drones' ids.
-
-        """
-        return self.DRONE_IDS
-    
-    ################################################################################
-
-    def _housekeeping(self):
-        """Housekeeping function.
-
-        Allocation and zero-ing of the variables and PyBullet's parameters/objects
-        in the `reset()` function.
-
-        """
-        #### Initialize/reset counters and zero-valued variables ###
-        self.RESET_TIME = time.time()
-        self.step_counter = 0
-        self.first_render_call = True
-        self.X_AX = -1*np.ones(self.NUM_DRONES)
-        self.Y_AX = -1*np.ones(self.NUM_DRONES)
-        self.Z_AX = -1*np.ones(self.NUM_DRONES);
-        self.GUI_INPUT_TEXT = -1*np.ones(self.NUM_DRONES)
-        self.USE_GUI_RPM=False
-        self.last_input_switch = 0
-        self.last_action = -1*np.ones((self.NUM_DRONES, 4))
-        self.last_clipped_action = np.zeros((self.NUM_DRONES, 4))
-        self.gui_input = np.zeros(4)
-        #### Initialize the drones kinemaatic information ##########
-        self.pos = np.zeros((self.NUM_DRONES, 3))
-        self.quat = np.zeros((self.NUM_DRONES, 4))
-        self.rpy = np.zeros((self.NUM_DRONES, 3))
-        self.vel = np.zeros((self.NUM_DRONES, 3))
-        self.ang_v = np.zeros((self.NUM_DRONES, 3))
-        if self.PHYSICS == Physics.DYN:
-            self.rpy_rates = np.zeros((self.NUM_DRONES, 3))
-        #### Set PyBullet's parameters #############################
-        p.setGravity(0, 0, -self.G, physicsClientId=self.CLIENT)
-        p.setRealTimeSimulation(0, physicsClientId=self.CLIENT)
-        p.setTimeStep(self.TIMESTEP, physicsClientId=self.CLIENT)
-        p.setAdditionalSearchPath(pybullet_data.getDataPath(), physicsClientId=self.CLIENT)
-        #### Load ground plane, drone and obstacles models #########
-        self.PLANE_ID = p.loadURDF("plane.urdf", physicsClientId=self.CLIENT)
-        self.DRONE_IDS = np.array([p.loadURDF(os.path.dirname(os.path.abspath(__file__))+"/../assets/"+self.URDF,
-                                              self.INIT_XYZS[i,:],
-                                              p.getQuaternionFromEuler(self.INIT_RPYS[i,:]),
-                                              physicsClientId=self.CLIENT
-                                              ) for i in range(self.NUM_DRONES)])
-        for i in range(self.NUM_DRONES):
-            #### Show the frame of reference of the drone, note that ###
-            #### It severly slows down the GUI #########################
-            if self.GUI and self.USER_DEBUG:
-                self._showDroneLocalAxes(i)
-            #### Disable collisions between drones' and the ground plane
-            #### E.g., to start a drone at [0,0,0] #####################
-            # p.setCollisionFilterPair(bodyUniqueIdA=self.PLANE_ID, bodyUniqueIdB=self.DRONE_IDS[i], linkIndexA=-1, linkIndexB=-1, enableCollision=0, physicsClientId=self.CLIENT)
-        if self.OBSTACLES:
-            self._addObstacles()
-    
-    ################################################################################
-
-    def _updateAndStoreKinematicInformation(self):
-        """Updates and stores the drones kinemaatic information.
-
-        This method is meant to limit the number of calls to PyBullet in each step
-        and improve performance (at the expense of memory).
-
-        """
-        for i in range (self.NUM_DRONES):
-            self.pos[i], self.quat[i] = p.getBasePositionAndOrientation(self.DRONE_IDS[i], physicsClientId=self.CLIENT)
-            self.rpy[i] = p.getEulerFromQuaternion(self.quat[i])
-            self.vel[i], self.ang_v[i] = p.getBaseVelocity(self.DRONE_IDS[i], physicsClientId=self.CLIENT)
-    
-    ################################################################################
-
-    def _startVideoRecording(self):
-        """Starts the recording of a video output.
-
-        The format of the video output is .mp4, if GUI is True, or .png, otherwise.
-        The video is saved under folder `files/videos`.
-
-        """
-        if self.RECORD and self.GUI:
-            self.VIDEO_ID = p.startStateLogging(loggingType=p.STATE_LOGGING_VIDEO_MP4,
-                                                fileName=os.path.dirname(os.path.abspath(__file__))+"/../../files/videos/video-"+datetime.now().strftime("%m.%d.%Y_%H.%M.%S")+".mp4",
-                                                physicsClientId=self.CLIENT
-                                                )
-        if self.RECORD and not self.GUI:
-            self.FRAME_NUM = 0
-            self.IMG_PATH = os.path.dirname(os.path.abspath(__file__))+"/../../files/videos/video-"+datetime.now().strftime("%m.%d.%Y_%H.%M.%S")+"/"
-            os.makedirs(os.path.dirname(self.IMG_PATH), exist_ok=True)
-    
-    ################################################################################
-
-    def _getDroneStateVector(self,
-                             nth_drone
-                             ):
-        """Returns the state vector of the n-th drone.
-
-        Parameters
-        ----------
-        nth_drone : int
-            The ordinal number/position of the desired drone in list self.DRONE_IDS.
-
-        Returns
-        -------
-        ndarray 
-            (20,)-shaped array of floats containing the state vector of the n-th drone.
-            Check the only line in this method and `_updateAndStoreKinematicInformation()`
-            to understand its format.
-
-        """
-        state = np.hstack([self.pos[nth_drone, :], self.quat[nth_drone, :], self.rpy[nth_drone, :],
-                           self.vel[nth_drone, :], self.ang_v[nth_drone, :], self.last_clipped_action[nth_drone, :]])
-        return state.reshape(20,)
-
-    ################################################################################
-
-    def _getDroneImages(self,
-                        nth_drone,
-                        segmentation: bool=True
-                        ):
-        """Returns camera captures from the n-th drone POV.
-
-        Parameters
-        ----------
-        nth_drone : int
-            The ordinal number/position of the desired drone in list self.DRONE_IDS.
-        segmentation : bool, optional
-            Whehter to compute the compute the segmentation mask.
-            It affects performance.
-
-        Returns
-        -------
-        ndarray 
-            (h, w, 4)-shaped array of uint8's containing the RBG(A) image captured from the n-th drone's POV.
-        ndarray
-            (h, w)-shaped array of uint8's containing the depth image captured from the n-th drone's POV.
-        ndarray
-            (h, w)-shaped array of uint8's containing the segmentation image captured from the n-th drone's POV.
-
-        """
-        if self.IMG_RES is None:
-            print("[ERROR] in BaseAviary._getDroneImages(), remember to set self.IMG_RES to np.array([width, height])")
-            exit()
-        rot_mat = np.array(p.getMatrixFromQuaternion(self.quat[nth_drone, :])).reshape(3, 3)
-        #### Set target point, camera view and projection matrices #
-        target = np.dot(rot_mat,np.array([1000, 0, 0])) + np.array(self.pos[nth_drone, :])
-        DRONE_CAM_VIEW = p.computeViewMatrix(cameraEyePosition=self.pos[nth_drone, :]+np.array([0, 0, self.L]),
-                                             cameraTargetPosition=target,
-                                             cameraUpVector=[0, 0, 1],
-                                             physicsClientId=self.CLIENT
-                                             )
-        DRONE_CAM_PRO =  p.computeProjectionMatrixFOV(fov=60.0,
-                                                      aspect=1.0,
-                                                      nearVal=self.L,
-                                                      farVal=1000.0
-                                                      )
-        SEG_FLAG = p.ER_SEGMENTATION_MASK_OBJECT_AND_LINKINDEX if segmentation else p.ER_NO_SEGMENTATION_MASK
-        [w, h, rgb, dep, seg] = p.getCameraImage(width=self.IMG_RES[0],
-                                                 height=self.IMG_RES[1],
-                                                 shadow=1,
-                                                 viewMatrix=DRONE_CAM_VIEW,
-                                                 projectionMatrix=DRONE_CAM_PRO,
-                                                 flags=SEG_FLAG,
-                                                 physicsClientId=self.CLIENT
-                                                 )
-        rgb = np.reshape(rgb, (h, w, 4))
-        dep = np.reshape(dep, (h, w))
-        seg = np.reshape(seg, (h, w))
-        return rgb, dep, seg
-
-    ################################################################################
-
-    def _exportImage(self,
-                     img_type: ImageType,
-                     img_input,
-                     path: str,
-                     frame_num: int=0
-                     ):
-        """Returns camera captures from the n-th drone POV.
-
-        Parameters
-        ----------
-        img_type : ImageType
-            The image type: RGB(A), depth, segmentation, or B&W (from RGB).
-        img_input : ndarray
-            (h, w, 4)-shaped array of uint8's for RBG(A) or B&W images.
-            (h, w)-shaped array of uint8's for depth or segmentation images.
-        path : str
-            Path where to save the output as PNG.
-        fram_num: int, optional
-            Frame number to append to the PNG's filename.
-
-        """
-        if img_type == ImageType.RGB:
-            (Image.fromarray(img_input.astype('uint8'), 'RGBA')).save(path+"frame_"+str(frame_num)+".png")
-        elif img_type == ImageType.DEP:
-            temp = ((img_input-np.min(img_input)) * 255 / (np.max(img_input)-np.min(img_input))).astype('uint8')
-        elif img_type == ImageType.SEG:
-            temp = ((img_input-np.min(img_input)) * 255 / (np.max(img_input)-np.min(img_input))).astype('uint8')
-        elif img_type == ImageType.BW:
-            temp = (np.sum(img_input[:, :, 0:2], axis=2) / 3).astype('uint8')
-        else:
-            print("[ERROR] in BaseAviary._exportImage(), unknown ImageType")
-            exit()
-        if img_type != ImageType.RGB:
-            (Image.fromarray(temp)).save(path+"frame_"+str(frame_num)+".png")
-
-    ################################################################################
-
-    def _getAdjacencyMatrix(self):
-        """Computes the adjacency matrix of a multi-drone system.
-
-        Attribute NEIGHBOURHOOD_RADIUS is used to determine neighboring relationships.
-
-        Returns
-        -------
-        ndarray
-            (NUM_DRONES, NUM_DRONES)-shaped array of 0's and 1's representing the adjacency matrix 
-            of the system: adj_mat[i,j] == 1 if (i, j) are neighbors; == 0 otherwise.
-
-        """
-        adjacency_mat = np.identity(self.NUM_DRONES)
-        for i in range(self.NUM_DRONES-1):
-            for j in range(self.NUM_DRONES-i-1):
-                if np.linalg.norm(self.pos[i, :]-self.pos[j+i+1, :]) < self.NEIGHBOURHOOD_RADIUS:
-                    adjacency_mat[i, j+i+1] = adjacency_mat[j+i+1, i] = 1
-        return adjacency_mat
-    
-    ################################################################################
-    
-    def _physics(self,
-                 rpm,
-                 nth_drone
-                 ):
-        """Base PyBullet physics implementation.
-
-        Parameters
-        ----------
-        rpm : ndarray
-            (4)-shaped array of ints containing the RPMs values of the 4 motors.
-        nth_drone : int
-            The ordinal number/position of the desired drone in list self.DRONE_IDS.
-
-        """
-        forces = np.array(rpm**2)*self.KF
-        torques = np.array(rpm**2)*self.KM
-        z_torque = (-torques[0] + torques[1] - torques[2] + torques[3])
-        for i in range(4):
-            p.applyExternalForce(self.DRONE_IDS[nth_drone],
-                                 i,
-                                 forceObj=[0, 0, forces[i]],
-                                 posObj=[0, 0, 0],
-                                 flags=p.LINK_FRAME,
-                                 physicsClientId=self.CLIENT
-                                 )
-        p.applyExternalTorque(self.DRONE_IDS[nth_drone],
-                              4,
-                              torqueObj=[0, 0, z_torque],
-                              flags=p.LINK_FRAME,
-                              physicsClientId=self.CLIENT
-                              )
-
-    ################################################################################
-
-    def _groundEffect(self,
-                      rpm,
-                      nth_drone
-                      ):
-        """PyBullet implementation of a ground effect model.
-
-        Inspired by the analytical model used for comparison in (Shi et al., 2019).
-
-        Parameters
-        ----------
-        rpm : ndarray
-            (4)-shaped array of ints containing the RPMs values of the 4 motors.
-        nth_drone : int
-            The ordinal number/position of the desired drone in list self.DRONE_IDS.
-
-        """
-        #### Kin. info of all links (propellers and center of mass)
-        link_states = np.array(p.getLinkStates(self.DRONE_IDS[nth_drone],
-                                               linkIndices=[0, 1, 2, 3, 4],
-                                               computeLinkVelocity=1,
-                                               computeForwardKinematics=1,
-                                               physicsClientId=self.CLIENT
-                                               ))
-        #### Simple, per-propeller ground effects ##################
-        prop_heights = np.array([link_states[0, 0][2], link_states[1, 0][2], link_states[2, 0][2], link_states[3, 0][2]])
-        prop_heights = np.clip(prop_heights, self.GND_EFF_H_CLIP, np.inf)
-        gnd_effects = np.array(rpm**2) * self.KF * self.GND_EFF_COEFF * (self.PROP_RADIUS/(4 * prop_heights))**2
-        if np.abs(self.rpy[nth_drone,0]) < np.pi/2 and np.abs(self.rpy[nth_drone,1]) < np.pi/2:
-            for i in range(4):
-                p.applyExternalForce(self.DRONE_IDS[nth_drone],
-                                     i,
-                                     forceObj=[0, 0, gnd_effects[i]],
-                                     posObj=[0, 0, 0],
-                                     flags=p.LINK_FRAME,
-                                     physicsClientId=self.CLIENT
-                                     )
-        #### TODO: a more realistic model accounting for the drone's
-        #### Attitude and its z-axis velocity in the world frame ###
-    
-    ################################################################################
-
-    def _drag(self,
-              rpm,
-              nth_drone
-              ):
-        """PyBullet implementation of a drag model.
-
-        Based on the the system identification in (Forster, 2015).
-
-        Parameters
-        ----------
-        rpm : ndarray
-            (4)-shaped array of ints containing the RPMs values of the 4 motors.
-        nth_drone : int
-            The ordinal number/position of the desired drone in list self.DRONE_IDS.
-
-        """
-        #### Rotation matrix of the base ###########################
-        base_rot = np.array(p.getMatrixFromQuaternion(self.quat[nth_drone, :])).reshape(3, 3)
-        #### Simple draft model applied to the base/center of mass #
-        drag_factors = -1 * self.DRAG_COEFF * np.sum(np.array(2*np.pi*rpm/60))
-        drag = np.dot(base_rot, drag_factors*np.array(self.vel[nth_drone, :]))
-        p.applyExternalForce(self.DRONE_IDS[nth_drone],
-                             4,
-                             forceObj=drag,
-                             posObj=[0, 0, 0],
-                             flags=p.LINK_FRAME,
-                             physicsClientId=self.CLIENT
-                             )
-    
-    ################################################################################
-
-    def _downwash(self,
-                  nth_drone
-                  ):
-        """PyBullet implementation of a ground effect model.
-
-        Based on experiments conducted at the Dynamic Systems Lab by SiQi Zhou.
-
-        Parameters
-        ----------
-        nth_drone : int
-            The ordinal number/position of the desired drone in list self.DRONE_IDS.
-
-        """
-        for i in range(self.NUM_DRONES):
-            delta_z = self.pos[i, 2] - self.pos[nth_drone, 2]
-            delta_xy = np.linalg.norm(np.array(self.pos[i, 0:2]) - np.array(self.pos[nth_drone, 0:2]))
-            if delta_z > 0 and delta_xy < 10: # Ignore drones more than 10 meters away
-                alpha = self.DW_COEFF_1 * (self.PROP_RADIUS/(4*delta_z))**2
-                beta = self.DW_COEFF_2 * delta_z + self.DW_COEFF_3
-                downwash = [0, 0, -alpha * np.exp(-.5*(delta_xy/beta)**2)]
-                p.applyExternalForce(self.DRONE_IDS[nth_drone],
-                                     4,
-                                     forceObj=downwash,
-                                     posObj=[0, 0, 0],
-                                     flags=p.LINK_FRAME,
-                                     physicsClientId=self.CLIENT
-                                     )
-
-    ################################################################################
-
-    def _dynamics(self,
-                  rpm,
-                  nth_drone
-                  ):
-        """Explicit dynamics implementation.
-
-        Based on code written at the Dynamic Systems Lab by James Xu.
-
-        Parameters
-        ----------
-        rpm : ndarray
-            (4)-shaped array of ints containing the RPMs values of the 4 motors.
-        nth_drone : int
-            The ordinal number/position of the desired drone in list self.DRONE_IDS.
-
-        """
-        #### Current state #########################################
-        pos = self.pos[nth_drone,:]
-        quat = self.quat[nth_drone,:]
-        rpy = self.rpy[nth_drone,:]
-        vel = self.vel[nth_drone,:]
-        rpy_rates = self.rpy_rates[nth_drone,:]
-        rotation = np.array(p.getMatrixFromQuaternion(quat)).reshape(3, 3)
-        #### Compute forces and torques ############################
-        forces = np.array(rpm**2) * self.KF
-        thrust = np.array([0, 0, np.sum(forces)])
-        thrust_world_frame = np.dot(rotation, thrust)
-        force_world_frame = thrust_world_frame - np.array([0, 0, self.GRAVITY])
-        z_torques = np.array(rpm**2)*self.KM
-        z_torque = (-z_torques[0] + z_torques[1] - z_torques[2] + z_torques[3])
-        if self.DRONE_MODEL==DroneModel.CF2X:
-            x_torque = (forces[0] + forces[1] - forces[2] - forces[3]) * (self.L/np.sqrt(2))
-            y_torque = (- forces[0] + forces[1] + forces[2] - forces[3]) * (self.L/np.sqrt(2))
-        elif self.DRONE_MODEL==DroneModel.CF2P or self.DRONE_MODEL==DroneModel.HB:
-            x_torque = (forces[1] - forces[3]) * self.L
-            y_torque = (-forces[0] + forces[2]) * self.L
-        torques = np.array([x_torque, y_torque, z_torque])
-        torques = torques - np.cross(rpy_rates, np.dot(self.J, rpy_rates))
-        rpy_rates_deriv = np.dot(self.J_INV, torques)
-        no_pybullet_dyn_accs = force_world_frame / self.M
-        #### Update state ##########################################
-        vel = vel + self.TIMESTEP * no_pybullet_dyn_accs
-        rpy_rates = rpy_rates + self.TIMESTEP * rpy_rates_deriv
-        pos = pos + self.TIMESTEP * vel
-        rpy = rpy + self.TIMESTEP * rpy_rates
-        #### Set PyBullet's state ##################################
-        p.resetBasePositionAndOrientation(self.DRONE_IDS[nth_drone],
-                                          pos,
-                                          p.getQuaternionFromEuler(rpy),
-                                          physicsClientId=self.CLIENT
-                                          )
-        #### Note: the base's velocity only stored and not used ####
-        p.resetBaseVelocity(self.DRONE_IDS[nth_drone],
-                            vel,
-                            [-1, -1, -1], # ang_vel not computed by DYN
-                            physicsClientId=self.CLIENT
-                            )
-        #### Store the roll, pitch, yaw rates for the next step ####
-        self.rpy_rates[nth_drone,:] = rpy_rates
-    
-    ################################################################################
-
-    def _normalizedActionToRPM(self,
-                               action
-                               ):
-        """De-normalizes the [-1, 1] range to the [0, MAX_RPM] range.
-
-        Parameters
-        ----------
-        action : ndarray
-            (4)-shaped array of ints containing an input in the [-1, 1] range.
-
-        Returns
-        -------
-        ndarray
-            (4)-shaped array of ints containing RPMs for the 4 motors in the [0, MAX_RPM] range.
-
-        """
-        if np.any(np.abs(action)) > 1:
-            print("\n[ERROR] it", self.step_counter, "in BaseAviary._normalizedActionToRPM(), out-of-bound action")
-        return np.where(action <= 0, (action+1)*self.HOVER_RPM, action*self.MAX_RPM) # Non-linear mapping: -1 -> 0, 0 -> HOVER_RPM, 1 -> MAX_RPM
-    
-    ################################################################################
-
-    def _saveLastAction(self,
-                        action
-                        ):
-        """Stores the most recent action into attribute `self.last_action`.
-
-        The last action can be used to compute aerodynamic effects.
-        The method disambiguates between array and dict inputs 
-        (for single or multi-agent aviaries, respectively).
-
-        Parameters
-        ----------
-        action : ndarray | dict
-            (4)-shaped array of ints (or dictionary of arrays) containing the current RPMs input.
-
-        """
-        if isinstance(action, collections.Mapping):
-            for k, v in action.items(): 
-                res_v = np.resize(v, (1, 4)) # Resize, possibly with repetition, to cope with different action spaces in RL subclasses
-                self.last_action[int(k), :] = res_v
-        else: 
-            res_action = np.resize(action, (1, 4)) # Resize, possibly with repetition, to cope with different action spaces in RL subclasses
-            self.last_action = np.reshape(res_action, (self.NUM_DRONES, 4))
-    
-    ################################################################################
-
-    def _showDroneLocalAxes(self,
-                            nth_drone
-                            ):
-        """Draws the local frame of the n-th drone in PyBullet's GUI.
-
-        Parameters
-        ----------
-        nth_drone : int
-            The ordinal number/position of the desired drone in list self.DRONE_IDS.
-
-        """
-        if self.GUI:
-            AXIS_LENGTH = 2*self.L
-            self.X_AX[nth_drone] = p.addUserDebugLine(lineFromXYZ=[0, 0, 0],
-                                                      lineToXYZ=[AXIS_LENGTH, 0, 0],
-                                                      lineColorRGB=[1, 0, 0],
-                                                      parentObjectUniqueId=self.DRONE_IDS[nth_drone],
-                                                      parentLinkIndex=-1,
-                                                      replaceItemUniqueId=int(self.X_AX[nth_drone]),
-                                                      physicsClientId=self.CLIENT
-                                                      )
-            self.Y_AX[nth_drone] = p.addUserDebugLine(lineFromXYZ=[0, 0, 0],
-                                                      lineToXYZ=[0, AXIS_LENGTH, 0],
-                                                      lineColorRGB=[0, 1, 0],
-                                                      parentObjectUniqueId=self.DRONE_IDS[nth_drone],
-                                                      parentLinkIndex=-1,
-                                                      replaceItemUniqueId=int(self.Y_AX[nth_drone]),
-                                                      physicsClientId=self.CLIENT
-                                                      )
-            self.Z_AX[nth_drone] = p.addUserDebugLine(lineFromXYZ=[0, 0, 0],
-                                                      lineToXYZ=[0, 0, AXIS_LENGTH],
-                                                      lineColorRGB=[0, 0, 1],
-                                                      parentObjectUniqueId=self.DRONE_IDS[nth_drone],
-                                                      parentLinkIndex=-1,
-                                                      replaceItemUniqueId=int(self.Z_AX[nth_drone]),
-                                                      physicsClientId=self.CLIENT
-                                                      )
-    
-    ################################################################################
-
-    def _addObstacles(self):
-        """Add obstacles to the environment.
-
-        These obstacles are loaded from standard URDF files included in Bullet.
-
-        """
-        p.loadURDF("samurai.urdf",
-                   physicsClientId=self.CLIENT
-                   )
-        p.loadURDF("duck_vhacd.urdf",
-                   [-.5, -.5, .05],
-                   p.getQuaternionFromEuler([0, 0, 0]),
-                   physicsClientId=self.CLIENT
-                   )
-        p.loadURDF("cube_no_rotation.urdf",
-                   [-.5, -2.5, .5],
-                   p.getQuaternionFromEuler([0, 0, 0]),
-                   physicsClientId=self.CLIENT
-                   )
-        p.loadURDF("sphere2.urdf",
-                   [0, 2, .5],
-                   p.getQuaternionFromEuler([0,0,0]),
-                   physicsClientId=self.CLIENT
-                   )
-    
-    ################################################################################
-    
-    def _parseURDFParameters(self):
-        """Loads parameters from an URDF file.
-
-        This method is nothing more than a custom XML parser for the .urdf
-        files in folder `assets/`.
-
-        """
-        URDF_TREE = etxml.parse(os.path.dirname(os.path.abspath(__file__))+"/../assets/"+self.URDF).getroot()
-        M = float(URDF_TREE[1][0][1].attrib['value'])
-        L = float(URDF_TREE[0].attrib['arm'])
-        THRUST2WEIGHT_RATIO = float(URDF_TREE[0].attrib['thrust2weight'])
-        IXX = float(URDF_TREE[1][0][2].attrib['ixx'])
-        IYY = float(URDF_TREE[1][0][2].attrib['iyy'])
-        IZZ = float(URDF_TREE[1][0][2].attrib['izz'])
-        J = np.diag([IXX, IYY, IZZ])
-        J_INV = np.linalg.inv(J)
-        KF = float(URDF_TREE[0].attrib['kf'])
-        KM = float(URDF_TREE[0].attrib['km'])
-        COLLISION_H = float(URDF_TREE[1][2][1][0].attrib['length'])
-        COLLISION_R = float(URDF_TREE[1][2][1][0].attrib['radius'])
-        COLLISION_SHAPE_OFFSETS = [float(s) for s in URDF_TREE[1][2][0].attrib['xyz'].split(' ')]
-        COLLISION_Z_OFFSET = COLLISION_SHAPE_OFFSETS[2]
-        MAX_SPEED_KMH = float(URDF_TREE[0].attrib['max_speed_kmh'])
-        GND_EFF_COEFF = float(URDF_TREE[0].attrib['gnd_eff_coeff'])
-        PROP_RADIUS = float(URDF_TREE[0].attrib['prop_radius'])
-        DRAG_COEFF_XY = float(URDF_TREE[0].attrib['drag_coeff_xy'])
-        DRAG_COEFF_Z = float(URDF_TREE[0].attrib['drag_coeff_z'])
-        DRAG_COEFF = np.array([DRAG_COEFF_XY, DRAG_COEFF_XY, DRAG_COEFF_Z])
-        DW_COEFF_1 = float(URDF_TREE[0].attrib['dw_coeff_1'])
-        DW_COEFF_2 = float(URDF_TREE[0].attrib['dw_coeff_2'])
-        DW_COEFF_3 = float(URDF_TREE[0].attrib['dw_coeff_3'])
-        return M, L, THRUST2WEIGHT_RATIO, J, J_INV, KF, KM, COLLISION_H, COLLISION_R, COLLISION_Z_OFFSET, MAX_SPEED_KMH, \
-               GND_EFF_COEFF, PROP_RADIUS, DRAG_COEFF, DW_COEFF_1, DW_COEFF_2, DW_COEFF_3
-    
-    ################################################################################
-    
-    def _actionSpace(self):
-        """Returns the action space of the environment.
-
-        Must be implemented in a subclass.
-
-        """
-        raise NotImplementedError
-    
-    ################################################################################
-
-    def _observationSpace(self):
-        """Returns the observation space of the environment.
-
-        Must be implemented in a subclass.
-
-        """
-        raise NotImplementedError
-    
-    ################################################################################
-    
-    def _computeObs(self):
-        """Returns the current observation of the environment.
-
-        Must be implemented in a subclass.
-
-        """
-        raise NotImplementedError
-    
-    ################################################################################
-
-    def _preprocessAction(self,
-                          action
-                          ):
-        """Pre-processes the action passed to `.step()` into motors' RPMs.
-
-        Must be implemented in a subclass.
-
-        Parameters
-        ----------
-        action : ndarray | dict[..]
-            The input action for one or more drones, to be translated into RPMs.
-
-        """
-        raise NotImplementedError
-
-    ################################################################################
-
-    def _computeReward(self):
-        """Computes the current reward value(s).
-
-        Must be implemented in a subclass.
-
-        """
-        raise NotImplementedError
-
-    ################################################################################
-
-    def _computeDone(self):
-        """Computes the current done value(s).
-
-        Must be implemented in a subclass.
-
-        """
-        raise NotImplementedError
-
-    ################################################################################
-
-    def _computeInfo(self):
-        """Computes the current info dict(s).
-
-        Must be implemented in a subclass.
-
-        """
-        raise NotImplementedError
+import os
+from sys import platform
+import time
+import collections
+from datetime import datetime
+from enum import Enum
+import xml.etree.ElementTree as etxml
+from PIL import Image
+import pkgutil
+egl = pkgutil.get_loader('eglRenderer')
+import numpy as np
+import pybullet as p
+import pybullet_data
+import gymnasium as gym
+
+class DroneModel(Enum):
+    """Drone models enumeration class."""
+
+    CF2X = "cf2x"   # Bitcraze Craziflie 2.0 in the X configuration
+    CF2P = "cf2p"   # Bitcraze Craziflie 2.0 in the + configuration
+    HB = "hb"       # Generic quadrotor (with AscTec Hummingbird inertial properties)
+
+################################################################################
+
+class Physics(Enum):
+    """Physics implementations enumeration class."""
+
+    PYB = "pyb"                         # Base PyBullet physics update
+    DYN = "dyn"                         # Update with an explicit model of the dynamics
+    PYB_GND = "pyb_gnd"                 # PyBullet physics update with ground effect
+    PYB_DRAG = "pyb_drag"               # PyBullet physics update with drag
+    PYB_DW = "pyb_dw"                   # PyBullet physics update with downwash
+    PYB_GND_DRAG_DW = "pyb_gnd_drag_dw" # PyBullet physics update with ground effect, drag, and downwash
+
+################################################################################
+
+class ImageType(Enum):
+    """Camera capture image type enumeration class."""
+
+    RGB = 0     # Red, green, blue (and alpha)
+    DEP = 1     # Depth
+    SEG = 2     # Segmentation by object id
+    BW = 3      # Black and white
+
+################################################################################
+
+class BaseAviary(gym.Env):
+    """Base class for "drone aviary" Gym environments."""
+
+    metadata = {'render.modes': ['human']}
+    
+    ################################################################################
+
+    def __init__(self,
+                 drone_model: DroneModel=DroneModel.CF2X,
+                 num_drones: int=1,
+                 neighbourhood_radius: float=np.inf,
+                 initial_xyzs=None,
+                 initial_rpys=None,
+                 physics: Physics=Physics.PYB,
+                 freq: int=240,
+                 aggregate_phy_steps: int=1,
+                 gui=False,
+                 record=False,
+                 obstacles=False,
+                 user_debug_gui=True,
+                 vision_attributes=False,
+                 dynamics_attributes=False
+                 ):
+        """Initialization of a generic aviary environment.
+
+        Parameters
+        ----------
+        drone_model : DroneModel, optional
+            The desired drone type (detailed in an .urdf file in folder `assets`).
+        num_drones : int, optional
+            The desired number of drones in the aviary.
+        neighbourhood_radius : float, optional
+            Radius used to compute the drones' adjacency matrix, in meters.
+        initial_xyzs: ndarray | None, optional
+            (NUM_DRONES, 3)-shaped array containing the initial XYZ position of the drones.
+        initial_rpys: ndarray | None, optional
+            (NUM_DRONES, 3)-shaped array containing the initial orientations of the drones (in radians).
+        physics : Physics, optional
+            The desired implementation of PyBullet physics/custom dynamics.
+        freq : int, optional
+            The frequency (Hz) at which the physics engine steps.
+        aggregate_phy_steps : int, optional
+            The number of physics steps within one call to `BaseAviary.step()`.
+        gui : bool, optional
+            Whether to use PyBullet's GUI.
+        record : bool, optional
+            Whether to save a video of the simulation in folder `files/videos/`.
+        obstacles : bool, optional
+            Whether to add obstacles to the simulation.
+        user_debug_gui : bool, optional
+            Whether to draw the drones' axes and the GUI RPMs sliders.
+        vision_attributes : bool, optional
+            Whether to allocate the attributes needed by vision-based aviary subclasses.
+        dynamics_attributes : bool, optional
+            Whether to allocate the attributes needed by subclasses accepting thrust and torques inputs.
+
+        """
+        #### Constants #############################################
+        self.G = 9.8
+        self.RAD2DEG = 180/np.pi
+        self.DEG2RAD = np.pi/180
+        self.SIM_FREQ = freq
+        self.TIMESTEP = 1./self.SIM_FREQ
+        self.AGGR_PHY_STEPS = aggregate_phy_steps
+        #### Parameters ############################################
+        self.NUM_DRONES = num_drones
+        self.NEIGHBOURHOOD_RADIUS = neighbourhood_radius
+        #### Options ###############################################
+        self.DRONE_MODEL = drone_model
+        self.GUI = gui
+        self.RECORD = record
+        self.PHYSICS = physics
+        self.OBSTACLES = obstacles
+        self.USER_DEBUG = user_debug_gui
+        self.URDF = "cf2x" + ".urdf"
+        #### Load the drone properties from the .urdf file #########
+        self.M, \
+        self.L, \
+        self.THRUST2WEIGHT_RATIO, \
+        self.J, \
+        self.J_INV, \
+        self.KF, \
+        self.KM, \
+        self.COLLISION_H,\
+        self.COLLISION_R, \
+        self.COLLISION_Z_OFFSET, \
+        self.MAX_SPEED_KMH, \
+        self.GND_EFF_COEFF, \
+        self.PROP_RADIUS, \
+        self.DRAG_COEFF, \
+        self.DW_COEFF_1, \
+        self.DW_COEFF_2, \
+        self.DW_COEFF_3 = self._parseURDFParameters()
+        print("[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:\n[INFO] m {:f}, L {:f},\n[INFO] ixx {:f}, iyy {:f}, izz {:f},\n[INFO] kf {:f}, km {:f},\n[INFO] t2w {:f}, max_speed_kmh {:f},\n[INFO] gnd_eff_coeff {:f}, prop_radius {:f},\n[INFO] drag_xy_coeff {:f}, drag_z_coeff {:f},\n[INFO] dw_coeff_1 {:f}, dw_coeff_2 {:f}, dw_coeff_3 {:f}".format(
+            self.M, self.L, self.J[0,0], self.J[1,1], self.J[2,2], self.KF, self.KM, self.THRUST2WEIGHT_RATIO, self.MAX_SPEED_KMH, self.GND_EFF_COEFF, self.PROP_RADIUS, self.DRAG_COEFF[0], self.DRAG_COEFF[2], self.DW_COEFF_1, self.DW_COEFF_2, self.DW_COEFF_3))
+        #### Compute constants #####################################
+        self.GRAVITY = self.G*self.M
+        self.HOVER_RPM = np.sqrt(self.GRAVITY / (4*self.KF))
+        self.MAX_RPM = np.sqrt((self.THRUST2WEIGHT_RATIO*self.GRAVITY) / (4*self.KF))
+        self.MAX_THRUST = (4*self.KF*self.MAX_RPM**2)
+        if self.DRONE_MODEL == DroneModel.CF2X:
+            self.MAX_XY_TORQUE = (2*self.L*self.KF*self.MAX_RPM**2)/np.sqrt(2)
+        elif self.DRONE_MODEL in [DroneModel.CF2P, DroneModel.HB]:
+            self.MAX_XY_TORQUE = (self.L*self.KF*self.MAX_RPM**2)
+        self.MAX_Z_TORQUE = (2*self.KM*self.MAX_RPM**2)
+        self.GND_EFF_H_CLIP = 0.25 * self.PROP_RADIUS * np.sqrt((15 * self.MAX_RPM**2 * self.KF * self.GND_EFF_COEFF) / self.MAX_THRUST)
+        #### Create attributes for vision tasks ####################
+        self.VISION_ATTR = vision_attributes
+        if self.VISION_ATTR:
+            self.IMG_RES = np.array([64, 48])
+            self.IMG_FRAME_PER_SEC = 24
+            self.IMG_CAPTURE_FREQ = int(self.SIM_FREQ/self.IMG_FRAME_PER_SEC)
+            self.rgb = np.zeros(((self.NUM_DRONES, self.IMG_RES[1], self.IMG_RES[0], 4)))
+            self.dep = np.ones(((self.NUM_DRONES, self.IMG_RES[1], self.IMG_RES[0])))
+            self.seg = np.zeros(((self.NUM_DRONES, self.IMG_RES[1], self.IMG_RES[0])))
+            if self.IMG_CAPTURE_FREQ%self.AGGR_PHY_STEPS != 0:
+                print("[ERROR] in BaseAviary.__init__(), aggregate_phy_steps incompatible with the desired video capture frame rate ({:f}Hz)".format(self.IMG_FRAME_PER_SEC))
+                exit()
+            if self.RECORD:
+                self.ONBOARD_IMG_PATH = os.path.dirname(os.path.abspath(__file__))+"/../../files/videos/onboard-"+datetime.now().strftime("%m.%d.%Y_%H.%M.%S")+"/"
+                os.makedirs(os.path.dirname(self.ONBOARD_IMG_PATH), exist_ok=True)
+        #### Create attributes for dynamics control inputs #########
+        self.DYNAMICS_ATTR = dynamics_attributes
+        if self.DYNAMICS_ATTR:
+            if self.DRONE_MODEL == DroneModel.CF2X:
+                self.A = np.array([ [1, 1, 1, 1], [1/np.sqrt(2), 1/np.sqrt(2), -1/np.sqrt(2), -1/np.sqrt(2)], [-1/np.sqrt(2), 1/np.sqrt(2), 1/np.sqrt(2), -1/np.sqrt(2)], [-1, 1, -1, 1] ])
+            elif self.DRONE_MODEL in [DroneModel.CF2P, DroneModel.HB]:
+                self.A = np.array([ [1, 1, 1, 1], [0, 1, 0, -1], [-1, 0, 1, 0], [-1, 1, -1, 1] ])
+            self.INV_A = np.linalg.inv(self.A)
+            self.B_COEFF = np.array([1/self.KF, 1/(self.KF*self.L), 1/(self.KF*self.L), 1/self.KM])
+        #### Connect to PyBullet ###################################
+        if self.GUI:
+            #### With debug GUI ########################################
+            self.CLIENT = p.connect(p.GUI) # p.connect(p.GUI, options="--opengl2")
+            for i in [p.COV_ENABLE_RGB_BUFFER_PREVIEW, p.COV_ENABLE_DEPTH_BUFFER_PREVIEW, p.COV_ENABLE_SEGMENTATION_MARK_PREVIEW]:
+                p.configureDebugVisualizer(i, 0, physicsClientId=self.CLIENT)
+            p.resetDebugVisualizerCamera(cameraDistance=3,
+                                         cameraYaw=-30,
+                                         cameraPitch=-30,
+                                         cameraTargetPosition=[0, 0, 0],
+                                         physicsClientId=self.CLIENT
+                                         )
+            ret = p.getDebugVisualizerCamera(physicsClientId=self.CLIENT)
+            print("viewMatrix", ret[2])
+            print("projectionMatrix", ret[3])
+            if self.USER_DEBUG:
+                #### Add input sliders to the GUI ##########################
+                self.SLIDERS = -1*np.ones(4)
+                for i in range(4):
+                    self.SLIDERS[i] = p.addUserDebugParameter("Propeller "+str(i)+" RPM", 0, self.MAX_RPM, self.HOVER_RPM, physicsClientId=self.CLIENT)
+                self.INPUT_SWITCH = p.addUserDebugParameter("Use GUI RPM", 9999, -1, 0, physicsClientId=self.CLIENT)
+        else:
+            #### Without debug GUI #####################################
+            self.CLIENT = p.connect(p.DIRECT)
+            #### Uncomment the following line to use EGL Render Plugin #
+            #### Instead of TinyRender (CPU-based) in PYB's Direct mode
+            # if platform == "linux": p.setAdditionalSearchPath(pybullet_data.getDataPath()); plugin = p.loadPlugin(egl.get_filename(), "_eglRendererPlugin"); print("plugin=", plugin)
+            if self.RECORD:
+                #### Set the camera parameters to save frames in DIRECT mode
+                self.VID_WIDTH=int(640)
+                self.VID_HEIGHT=int(480)
+                self.FRAME_PER_SEC = 24
+                self.CAPTURE_FREQ = int(self.SIM_FREQ/self.FRAME_PER_SEC)
+                self.CAM_VIEW = p.computeViewMatrixFromYawPitchRoll(distance=3,
+                                                                    yaw=-30,
+                                                                    pitch=-30,
+                                                                    roll=0,
+                                                                    cameraTargetPosition=[0, 0, 0],
+                                                                    upAxisIndex=2,
+                                                                    physicsClientId=self.CLIENT
+                                                                    )
+                self.CAM_PRO = p.computeProjectionMatrixFOV(fov=60.0,
+                                                            aspect=self.VID_WIDTH/self.VID_HEIGHT,
+                                                            nearVal=0.1,
+                                                            farVal=1000.0
+                                                            )
+        #### Set initial poses #####################################
+        if initial_xyzs is None:
+            self.INIT_XYZS = np.vstack([np.array([x*4*self.L for x in range(self.NUM_DRONES)]), \
+                                        np.array([y*4*self.L for y in range(self.NUM_DRONES)]), \
+                                        np.ones(self.NUM_DRONES) * (self.COLLISION_H/2-self.COLLISION_Z_OFFSET+.1)]).transpose().reshape(self.NUM_DRONES, 3)
+        elif np.array(initial_xyzs).shape == (self.NUM_DRONES,3):
+            self.INIT_XYZS = initial_xyzs
+        else:
+            print("[ERROR] invalid initial_xyzs in BaseAviary.__init__(), try initial_xyzs.reshape(NUM_DRONES,3)")
+        if initial_rpys is None:
+            self.INIT_RPYS = np.zeros((self.NUM_DRONES, 3))
+        elif np.array(initial_rpys).shape == (self.NUM_DRONES, 3):
+            self.INIT_RPYS = initial_rpys
+        else:
+            print("[ERROR] invalid initial_rpys in BaseAviary.__init__(), try initial_rpys.reshape(NUM_DRONES,3)")
+        #### Create action and observation spaces ##################
+        self.action_space = self._actionSpace()
+        self.observation_space = self._observationSpace()
+        #### Housekeeping ##########################################
+        self._housekeeping()
+        #### Update and store the drones kinematic information #####
+        self._updateAndStoreKinematicInformation()
+        #### Start video recording #################################
+        self._startVideoRecording()
+    
+    ################################################################################
+
+    def reset(self):
+        """Resets the environment.
+
+        Returns
+        -------
+        ndarray | dict[..]
+            The initial observation, check the specific implementation of `_computeObs()`
+            in each subclass for its format.
+
+        """
+        p.resetSimulation(physicsClientId=self.CLIENT)
+        #### Housekeeping ##########################################
+        self._housekeeping()
+        #### Update and store the drones kinematic information #####
+        self._updateAndStoreKinematicInformation()
+        #### Start video recording #################################
+        self._startVideoRecording()
+        #### Return the initial observation ########################
+        return self._computeObs()
+    
+    ################################################################################
+
+    def step(self,
+             action
+             ):
+        """Advances the environment by one simulation step.
+
+        Parameters
+        ----------
+        action : ndarray | dict[..]
+            The input action for one or more drones, translated into RPMs by
+            the specific implementation of `_preprocessAction()` in each subclass.
+
+        Returns
+        -------
+        ndarray | dict[..]
+            The step's observation, check the specific implementation of `_computeObs()`
+            in each subclass for its format.
+        float | dict[..]
+            The step's reward value(s), check the specific implementation of `_computeReward()`
+            in each subclass for its format.
+        bool | dict[..]
+            Whether the current epoisode is over, check the specific implementation of `_computeDone()`
+            in each subclass for its format.
+        dict[..]
+            Additional information as a dictionary, check the specific implementation of `_computeInfo()`
+            in each subclass for its format.
+
+        """
+        #### Save PNG video frames if RECORD=True and GUI=False ####
+        if self.RECORD and not self.GUI and self.step_counter%self.CAPTURE_FREQ == 0:
+            [w, h, rgb, dep, seg] = p.getCameraImage(width=self.VID_WIDTH,
+                                                     height=self.VID_HEIGHT,
+                                                     shadow=1,
+                                                     viewMatrix=self.CAM_VIEW,
+                                                     projectionMatrix=self.CAM_PRO,
+                                                     renderer=p.ER_TINY_RENDERER,
+                                                     flags=p.ER_SEGMENTATION_MASK_OBJECT_AND_LINKINDEX,
+                                                     physicsClientId=self.CLIENT
+                                                     )
+            (Image.fromarray(np.reshape(rgb, (h, w, 4)), 'RGBA')).save(self.IMG_PATH+"frame_"+str(self.FRAME_NUM)+".png")
+            #### Save the depth or segmentation view instead #######
+            # dep = ((dep-np.min(dep)) * 255 / (np.max(dep)-np.min(dep))).astype('uint8')
+            # (Image.fromarray(np.reshape(dep, (h, w)))).save(self.IMG_PATH+"frame_"+str(self.FRAME_NUM)+".png")
+            # seg = ((seg-np.min(seg)) * 255 / (np.max(seg)-np.min(seg))).astype('uint8')
+            # (Image.fromarray(np.reshape(seg, (h, w)))).save(self.IMG_PATH+"frame_"+str(self.FRAME_NUM)+".png")
+            self.FRAME_NUM += 1
+        #### Read the GUI's input parameters #######################
+        if self.GUI and self.USER_DEBUG:
+            current_input_switch = p.readUserDebugParameter(self.INPUT_SWITCH, physicsClientId=self.CLIENT)
+            if current_input_switch > self.last_input_switch:
+                self.last_input_switch = current_input_switch
+                self.USE_GUI_RPM = True if self.USE_GUI_RPM == False else False
+        if self.USE_GUI_RPM:
+            for i in range(4):
+                self.gui_input[i] = p.readUserDebugParameter(int(self.SLIDERS[i]), physicsClientId=self.CLIENT)
+            clipped_action = np.tile(self.gui_input, (self.NUM_DRONES, 1))
+            if self.step_counter%(self.SIM_FREQ/2) == 0:
+                self.GUI_INPUT_TEXT = [p.addUserDebugText("Using GUI RPM",
+                                                          textPosition=[0, 0, 0],
+                                                          textColorRGB=[1, 0, 0],
+                                                          lifeTime=1,
+                                                          textSize=2,
+                                                          parentObjectUniqueId=self.DRONE_IDS[i],
+                                                          parentLinkIndex=-1,
+                                                          replaceItemUniqueId=int(self.GUI_INPUT_TEXT[i]),
+                                                          physicsClientId=self.CLIENT
+                                                          ) for i in range(self.NUM_DRONES)]
+        #### Save, preprocess, and clip the action to the max. RPM #
+        else:
+            self._saveLastAction(action)
+            clipped_action = np.reshape(self._preprocessAction(action), (self.NUM_DRONES, 4))
+        #### Repeat for as many as the aggregate physics steps #####
+        for _ in range(self.AGGR_PHY_STEPS):
+            #### Update and store the drones kinematic info for certain
+            #### Between aggregate steps for certain types of update ###
+            if self.AGGR_PHY_STEPS > 1 and self.PHYSICS in [Physics.DYN, Physics.PYB_GND, Physics.PYB_DRAG, Physics.PYB_DW, Physics.PYB_GND_DRAG_DW]:
+                self._updateAndStoreKinematicInformation()
+            #### Step the simulation using the desired physics update ##
+            for i in range (self.NUM_DRONES):
+                if self.PHYSICS == Physics.PYB:
+                    self._physics(clipped_action[i, :], i)
+                elif self.PHYSICS == Physics.DYN:
+                    self._dynamics(clipped_action[i, :], i)
+                elif self.PHYSICS == Physics.PYB_GND:
+                    self._physics(clipped_action[i, :], i)
+                    self._groundEffect(clipped_action[i, :], i)
+                elif self.PHYSICS == Physics.PYB_DRAG:
+                    self._physics(clipped_action[i, :], i)
+                    self._drag(self.last_clipped_action[i, :], i)
+                elif self.PHYSICS == Physics.PYB_DW:
+                    self._physics(clipped_action[i, :], i)
+                    self._downwash(i)
+                elif self.PHYSICS == Physics.PYB_GND_DRAG_DW:
+                    self._physics(clipped_action[i, :], i)
+                    self._groundEffect(clipped_action[i, :], i)
+                    self._drag(self.last_clipped_action[i, :], i)
+                    self._downwash(i)
+            #### PyBullet computes the new state, unless Physics.DYN ###
+            if self.PHYSICS != Physics.DYN:
+                p.stepSimulation(physicsClientId=self.CLIENT)
+            #### Save the last applied action (e.g. to compute drag) ###
+            self.last_clipped_action = clipped_action
+        #### Update and store the drones kinematic information #####
+        self._updateAndStoreKinematicInformation()
+        #### Prepare the return values #############################
+        obs = self._computeObs()
+        reward = self._computeReward()
+        done = self._computeDone()
+        truncated = self._computeTruncated()
+        info = self._computeInfo()
+        #### Advance the step counter ##############################
+        self.step_counter = self.step_counter + (1 * self.AGGR_PHY_STEPS)
+        return obs, reward, done, truncated, info
+    
+    ################################################################################
+    
+    def render(self,
+               mode='human',
+               close=False
+               ):
+        """Prints a textual output of the environment.
+
+        Parameters
+        ----------
+        mode : str, optional
+            Unused.
+        close : bool, optional
+            Unused.
+
+        """
+        if self.first_render_call and not self.GUI:
+            print("[WARNING] BaseAviary.render() is implemented as text-only, re-initialize the environment using Aviary(gui=True) to use PyBullet's graphical interface")
+            self.first_render_call = False
+        print("\n[INFO] BaseAviary.render() ——— it {:04d}".format(self.step_counter),
+              "——— wall-clock time {:.1f}s,".format(time.time()-self.RESET_TIME),
+              "simulation time {:.1f}s@{:d}Hz ({:.2f}x)".format(self.step_counter*self.TIMESTEP, self.SIM_FREQ, (self.step_counter*self.TIMESTEP)/(time.time()-self.RESET_TIME)))
+        for i in range (self.NUM_DRONES):
+            print("[INFO] BaseAviary.render() ——— drone {:d}".format(i),
+                  "——— x {:+06.2f}, y {:+06.2f}, z {:+06.2f}".format(self.pos[i, 0], self.pos[i, 1], self.pos[i, 2]),
+                  "——— velocity {:+06.2f}, {:+06.2f}, {:+06.2f}".format(self.vel[i, 0], self.vel[i, 1], self.vel[i, 2]),
+                  "——— roll {:+06.2f}, pitch {:+06.2f}, yaw {:+06.2f}".format(self.rpy[i, 0]*self.RAD2DEG, self.rpy[i, 1]*self.RAD2DEG, self.rpy[i, 2]*self.RAD2DEG),
+                  "——— angular velocity {:+06.4f}, {:+06.4f}, {:+06.4f} ——— ".format(self.ang_v[i, 0], self.ang_v[i, 1], self.ang_v[i, 2]))
+    
+    ################################################################################
+
+    def close(self):
+        """Terminates the environment.
+        """
+        if self.RECORD and self.GUI:
+            p.stopStateLogging(self.VIDEO_ID, physicsClientId=self.CLIENT)
+        p.disconnect(physicsClientId=self.CLIENT)
+    
+    ################################################################################
+
+    def getPyBulletClient(self):
+        """Returns the PyBullet Client Id.
+
+        Returns
+        -------
+        int:
+            The PyBullet Client Id.
+
+        """
+        return self.CLIENT
+    
+    ################################################################################
+
+    def getDroneIds(self):
+        """Return the Drone Ids.
+
+        Returns
+        -------
+        ndarray:
+            (NUM_DRONES,)-shaped array of ints containing the drones' ids.
+
+        """
+        return self.DRONE_IDS
+    
+    ################################################################################
+
+    def _housekeeping(self):
+        """Housekeeping function.
+
+        Allocation and zero-ing of the variables and PyBullet's parameters/objects
+        in the `reset()` function.
+
+        """
+        #### Initialize/reset counters and zero-valued variables ###
+        self.RESET_TIME = time.time()
+        self.step_counter = 0
+        self.first_render_call = True
+        self.X_AX = -1*np.ones(self.NUM_DRONES)
+        self.Y_AX = -1*np.ones(self.NUM_DRONES)
+        self.Z_AX = -1*np.ones(self.NUM_DRONES);
+        self.GUI_INPUT_TEXT = -1*np.ones(self.NUM_DRONES)
+        self.USE_GUI_RPM=False
+        self.last_input_switch = 0
+        self.last_action = -1*np.ones((self.NUM_DRONES, 4))
+        self.last_clipped_action = np.zeros((self.NUM_DRONES, 4))
+        self.gui_input = np.zeros(4)
+        #### Initialize the drones kinemaatic information ##########
+        self.pos = np.zeros((self.NUM_DRONES, 3))
+        self.quat = np.zeros((self.NUM_DRONES, 4))
+        self.rpy = np.zeros((self.NUM_DRONES, 3))
+        self.vel = np.zeros((self.NUM_DRONES, 3))
+        self.ang_v = np.zeros((self.NUM_DRONES, 3))
+        if self.PHYSICS == Physics.DYN:
+            self.rpy_rates = np.zeros((self.NUM_DRONES, 3))
+        #### Set PyBullet's parameters #############################
+        p.setGravity(0, 0, -self.G, physicsClientId=self.CLIENT)
+        p.setRealTimeSimulation(0, physicsClientId=self.CLIENT)
+        p.setTimeStep(self.TIMESTEP, physicsClientId=self.CLIENT)
+        p.setAdditionalSearchPath(pybullet_data.getDataPath(), physicsClientId=self.CLIENT)
+        #### Load ground plane, drone and obstacles models #########
+        self.PLANE_ID = p.loadURDF("plane.urdf", physicsClientId=self.CLIENT)
+        self.DRONE_IDS = np.array([p.loadURDF(os.path.dirname(os.path.abspath(__file__))+"/../assets/"+self.URDF,
+                                              self.INIT_XYZS[i,:],
+                                              p.getQuaternionFromEuler(self.INIT_RPYS[i,:]),
+                                              physicsClientId=self.CLIENT
+                                              ) for i in range(self.NUM_DRONES)])
+        for i in range(self.NUM_DRONES):
+            #### Show the frame of reference of the drone, note that ###
+            #### It severly slows down the GUI #########################
+            if self.GUI and self.USER_DEBUG:
+                self._showDroneLocalAxes(i)
+            #### Disable collisions between drones' and the ground plane
+            #### E.g., to start a drone at [0,0,0] #####################
+            # p.setCollisionFilterPair(bodyUniqueIdA=self.PLANE_ID, bodyUniqueIdB=self.DRONE_IDS[i], linkIndexA=-1, linkIndexB=-1, enableCollision=0, physicsClientId=self.CLIENT)
+        if self.OBSTACLES:
+            self._addObstacles()
+    
+    ################################################################################
+
+    def _updateAndStoreKinematicInformation(self):
+        """Updates and stores the drones kinemaatic information.
+
+        This method is meant to limit the number of calls to PyBullet in each step
+        and improve performance (at the expense of memory).
+
+        """
+        for i in range (self.NUM_DRONES):
+            self.pos[i], self.quat[i] = p.getBasePositionAndOrientation(self.DRONE_IDS[i], physicsClientId=self.CLIENT)
+            self.rpy[i] = p.getEulerFromQuaternion(self.quat[i])
+            self.vel[i], self.ang_v[i] = p.getBaseVelocity(self.DRONE_IDS[i], physicsClientId=self.CLIENT)
+    
+    ################################################################################
+
+    def _startVideoRecording(self):
+        """Starts the recording of a video output.
+
+        The format of the video output is .mp4, if GUI is True, or .png, otherwise.
+        The video is saved under folder `files/videos`.
+
+        """
+        if self.RECORD and self.GUI:
+            self.VIDEO_ID = p.startStateLogging(loggingType=p.STATE_LOGGING_VIDEO_MP4,
+                                                fileName=os.path.dirname(os.path.abspath(__file__))+"/../../files/videos/video-"+datetime.now().strftime("%m.%d.%Y_%H.%M.%S")+".mp4",
+                                                physicsClientId=self.CLIENT
+                                                )
+        if self.RECORD and not self.GUI:
+            self.FRAME_NUM = 0
+            self.IMG_PATH = os.path.dirname(os.path.abspath(__file__))+"/../../files/videos/video-"+datetime.now().strftime("%m.%d.%Y_%H.%M.%S")+"/"
+            os.makedirs(os.path.dirname(self.IMG_PATH), exist_ok=True)
+    
+    ################################################################################
+
+    def _getDroneStateVector(self,
+                             nth_drone
+                             ):
+        """Returns the state vector of the n-th drone.
+
+        Parameters
+        ----------
+        nth_drone : int
+            The ordinal number/position of the desired drone in list self.DRONE_IDS.
+
+        Returns
+        -------
+        ndarray 
+            (20,)-shaped array of floats containing the state vector of the n-th drone.
+            Check the only line in this method and `_updateAndStoreKinematicInformation()`
+            to understand its format.
+
+        """
+        state = np.hstack([self.pos[nth_drone, :], self.quat[nth_drone, :], self.rpy[nth_drone, :],
+                           self.vel[nth_drone, :], self.ang_v[nth_drone, :], self.last_clipped_action[nth_drone, :]])
+        return state.reshape(20,)
+
+    ################################################################################
+
+    def _getDroneImages(self,
+                        nth_drone,
+                        segmentation: bool=True
+                        ):
+        """Returns camera captures from the n-th drone POV.
+
+        Parameters
+        ----------
+        nth_drone : int
+            The ordinal number/position of the desired drone in list self.DRONE_IDS.
+        segmentation : bool, optional
+            Whehter to compute the compute the segmentation mask.
+            It affects performance.
+
+        Returns
+        -------
+        ndarray 
+            (h, w, 4)-shaped array of uint8's containing the RBG(A) image captured from the n-th drone's POV.
+        ndarray
+            (h, w)-shaped array of uint8's containing the depth image captured from the n-th drone's POV.
+        ndarray
+            (h, w)-shaped array of uint8's containing the segmentation image captured from the n-th drone's POV.
+
+        """
+        if self.IMG_RES is None:
+            print("[ERROR] in BaseAviary._getDroneImages(), remember to set self.IMG_RES to np.array([width, height])")
+            exit()
+        rot_mat = np.array(p.getMatrixFromQuaternion(self.quat[nth_drone, :])).reshape(3, 3)
+        #### Set target point, camera view and projection matrices #
+        target = np.dot(rot_mat,np.array([1000, 0, 0])) + np.array(self.pos[nth_drone, :])
+        DRONE_CAM_VIEW = p.computeViewMatrix(cameraEyePosition=self.pos[nth_drone, :]+np.array([0, 0, self.L]),
+                                             cameraTargetPosition=target,
+                                             cameraUpVector=[0, 0, 1],
+                                             physicsClientId=self.CLIENT
+                                             )
+        DRONE_CAM_PRO =  p.computeProjectionMatrixFOV(fov=60.0,
+                                                      aspect=1.0,
+                                                      nearVal=self.L,
+                                                      farVal=1000.0
+                                                      )
+        SEG_FLAG = p.ER_SEGMENTATION_MASK_OBJECT_AND_LINKINDEX if segmentation else p.ER_NO_SEGMENTATION_MASK
+        [w, h, rgb, dep, seg] = p.getCameraImage(width=self.IMG_RES[0],
+                                                 height=self.IMG_RES[1],
+                                                 shadow=1,
+                                                 viewMatrix=DRONE_CAM_VIEW,
+                                                 projectionMatrix=DRONE_CAM_PRO,
+                                                 flags=SEG_FLAG,
+                                                 physicsClientId=self.CLIENT
+                                                 )
+        rgb = np.reshape(rgb, (h, w, 4))
+        dep = np.reshape(dep, (h, w))
+        seg = np.reshape(seg, (h, w))
+        return rgb, dep, seg
+
+    ################################################################################
+
+    def _exportImage(self,
+                     img_type: ImageType,
+                     img_input,
+                     path: str,
+                     frame_num: int=0
+                     ):
+        """Returns camera captures from the n-th drone POV.
+
+        Parameters
+        ----------
+        img_type : ImageType
+            The image type: RGB(A), depth, segmentation, or B&W (from RGB).
+        img_input : ndarray
+            (h, w, 4)-shaped array of uint8's for RBG(A) or B&W images.
+            (h, w)-shaped array of uint8's for depth or segmentation images.
+        path : str
+            Path where to save the output as PNG.
+        fram_num: int, optional
+            Frame number to append to the PNG's filename.
+
+        """
+        if img_type == ImageType.RGB:
+            (Image.fromarray(img_input.astype('uint8'), 'RGBA')).save(path+"frame_"+str(frame_num)+".png")
+        elif img_type == ImageType.DEP:
+            temp = ((img_input-np.min(img_input)) * 255 / (np.max(img_input)-np.min(img_input))).astype('uint8')
+        elif img_type == ImageType.SEG:
+            temp = ((img_input-np.min(img_input)) * 255 / (np.max(img_input)-np.min(img_input))).astype('uint8')
+        elif img_type == ImageType.BW:
+            temp = (np.sum(img_input[:, :, 0:2], axis=2) / 3).astype('uint8')
+        else:
+            print("[ERROR] in BaseAviary._exportImage(), unknown ImageType")
+            exit()
+        if img_type != ImageType.RGB:
+            (Image.fromarray(temp)).save(path+"frame_"+str(frame_num)+".png")
+
+    ################################################################################
+
+    def _getAdjacencyMatrix(self):
+        """Computes the adjacency matrix of a multi-drone system.
+
+        Attribute NEIGHBOURHOOD_RADIUS is used to determine neighboring relationships.
+
+        Returns
+        -------
+        ndarray
+            (NUM_DRONES, NUM_DRONES)-shaped array of 0's and 1's representing the adjacency matrix 
+            of the system: adj_mat[i,j] == 1 if (i, j) are neighbors; == 0 otherwise.
+
+        """
+        adjacency_mat = np.identity(self.NUM_DRONES)
+        for i in range(self.NUM_DRONES-1):
+            for j in range(self.NUM_DRONES-i-1):
+                if np.linalg.norm(self.pos[i, :]-self.pos[j+i+1, :]) < self.NEIGHBOURHOOD_RADIUS:
+                    adjacency_mat[i, j+i+1] = adjacency_mat[j+i+1, i] = 1
+        return adjacency_mat
+    
+    ################################################################################
+    
+    def _physics(self,
+                 rpm,
+                 nth_drone
+                 ):
+        """Base PyBullet physics implementation.
+
+        Parameters
+        ----------
+        rpm : ndarray
+            (4)-shaped array of ints containing the RPMs values of the 4 motors.
+        nth_drone : int
+            The ordinal number/position of the desired drone in list self.DRONE_IDS.
+
+        """
+        forces = np.array(rpm**2)*self.KF
+        torques = np.array(rpm**2)*self.KM
+        z_torque = (-torques[0] + torques[1] - torques[2] + torques[3])
+        for i in range(4):
+            p.applyExternalForce(self.DRONE_IDS[nth_drone],
+                                 i,
+                                 forceObj=[0, 0, forces[i]],
+                                 posObj=[0, 0, 0],
+                                 flags=p.LINK_FRAME,
+                                 physicsClientId=self.CLIENT
+                                 )
+        p.applyExternalTorque(self.DRONE_IDS[nth_drone],
+                              4,
+                              torqueObj=[0, 0, z_torque],
+                              flags=p.LINK_FRAME,
+                              physicsClientId=self.CLIENT
+                              )
+
+    ################################################################################
+
+    def _groundEffect(self,
+                      rpm,
+                      nth_drone
+                      ):
+        """PyBullet implementation of a ground effect model.
+
+        Inspired by the analytical model used for comparison in (Shi et al., 2019).
+
+        Parameters
+        ----------
+        rpm : ndarray
+            (4)-shaped array of ints containing the RPMs values of the 4 motors.
+        nth_drone : int
+            The ordinal number/position of the desired drone in list self.DRONE_IDS.
+
+        """
+        #### Kin. info of all links (propellers and center of mass)
+        link_states = np.array(p.getLinkStates(self.DRONE_IDS[nth_drone],
+                                               linkIndices=[0, 1, 2, 3, 4],
+                                               computeLinkVelocity=1,
+                                               computeForwardKinematics=1,
+                                               physicsClientId=self.CLIENT
+                                               ))
+        #### Simple, per-propeller ground effects ##################
+        prop_heights = np.array([link_states[0, 0][2], link_states[1, 0][2], link_states[2, 0][2], link_states[3, 0][2]])
+        prop_heights = np.clip(prop_heights, self.GND_EFF_H_CLIP, np.inf)
+        gnd_effects = np.array(rpm**2) * self.KF * self.GND_EFF_COEFF * (self.PROP_RADIUS/(4 * prop_heights))**2
+        if np.abs(self.rpy[nth_drone,0]) < np.pi/2 and np.abs(self.rpy[nth_drone,1]) < np.pi/2:
+            for i in range(4):
+                p.applyExternalForce(self.DRONE_IDS[nth_drone],
+                                     i,
+                                     forceObj=[0, 0, gnd_effects[i]],
+                                     posObj=[0, 0, 0],
+                                     flags=p.LINK_FRAME,
+                                     physicsClientId=self.CLIENT
+                                     )
+        #### TODO: a more realistic model accounting for the drone's
+        #### Attitude and its z-axis velocity in the world frame ###
+    
+    ################################################################################
+
+    def _drag(self,
+              rpm,
+              nth_drone
+              ):
+        """PyBullet implementation of a drag model.
+
+        Based on the the system identification in (Forster, 2015).
+
+        Parameters
+        ----------
+        rpm : ndarray
+            (4)-shaped array of ints containing the RPMs values of the 4 motors.
+        nth_drone : int
+            The ordinal number/position of the desired drone in list self.DRONE_IDS.
+
+        """
+        #### Rotation matrix of the base ###########################
+        base_rot = np.array(p.getMatrixFromQuaternion(self.quat[nth_drone, :])).reshape(3, 3)
+        #### Simple draft model applied to the base/center of mass #
+        drag_factors = -1 * self.DRAG_COEFF * np.sum(np.array(2*np.pi*rpm/60))
+        drag = np.dot(base_rot, drag_factors*np.array(self.vel[nth_drone, :]))
+        p.applyExternalForce(self.DRONE_IDS[nth_drone],
+                             4,
+                             forceObj=drag,
+                             posObj=[0, 0, 0],
+                             flags=p.LINK_FRAME,
+                             physicsClientId=self.CLIENT
+                             )
+    
+    ################################################################################
+
+    def _downwash(self,
+                  nth_drone
+                  ):
+        """PyBullet implementation of a ground effect model.
+
+        Based on experiments conducted at the Dynamic Systems Lab by SiQi Zhou.
+
+        Parameters
+        ----------
+        nth_drone : int
+            The ordinal number/position of the desired drone in list self.DRONE_IDS.
+
+        """
+        for i in range(self.NUM_DRONES):
+            delta_z = self.pos[i, 2] - self.pos[nth_drone, 2]
+            delta_xy = np.linalg.norm(np.array(self.pos[i, 0:2]) - np.array(self.pos[nth_drone, 0:2]))
+            if delta_z > 0 and delta_xy < 10: # Ignore drones more than 10 meters away
+                alpha = self.DW_COEFF_1 * (self.PROP_RADIUS/(4*delta_z))**2
+                beta = self.DW_COEFF_2 * delta_z + self.DW_COEFF_3
+                downwash = [0, 0, -alpha * np.exp(-.5*(delta_xy/beta)**2)]
+                p.applyExternalForce(self.DRONE_IDS[nth_drone],
+                                     4,
+                                     forceObj=downwash,
+                                     posObj=[0, 0, 0],
+                                     flags=p.LINK_FRAME,
+                                     physicsClientId=self.CLIENT
+                                     )
+
+    ################################################################################
+
+    def _dynamics(self,
+                  rpm,
+                  nth_drone
+                  ):
+        """Explicit dynamics implementation.
+
+        Based on code written at the Dynamic Systems Lab by James Xu.
+
+        Parameters
+        ----------
+        rpm : ndarray
+            (4)-shaped array of ints containing the RPMs values of the 4 motors.
+        nth_drone : int
+            The ordinal number/position of the desired drone in list self.DRONE_IDS.
+
+        """
+        #### Current state #########################################
+        pos = self.pos[nth_drone,:]
+        quat = self.quat[nth_drone,:]
+        rpy = self.rpy[nth_drone,:]
+        vel = self.vel[nth_drone,:]
+        rpy_rates = self.rpy_rates[nth_drone,:]
+        rotation = np.array(p.getMatrixFromQuaternion(quat)).reshape(3, 3)
+        #### Compute forces and torques ############################
+        forces = np.array(rpm**2) * self.KF
+        thrust = np.array([0, 0, np.sum(forces)])
+        thrust_world_frame = np.dot(rotation, thrust)
+        force_world_frame = thrust_world_frame - np.array([0, 0, self.GRAVITY])
+        z_torques = np.array(rpm**2)*self.KM
+        z_torque = (-z_torques[0] + z_torques[1] - z_torques[2] + z_torques[3])
+        if self.DRONE_MODEL==DroneModel.CF2X:
+            x_torque = (forces[0] + forces[1] - forces[2] - forces[3]) * (self.L/np.sqrt(2))
+            y_torque = (- forces[0] + forces[1] + forces[2] - forces[3]) * (self.L/np.sqrt(2))
+        elif self.DRONE_MODEL==DroneModel.CF2P or self.DRONE_MODEL==DroneModel.HB:
+            x_torque = (forces[1] - forces[3]) * self.L
+            y_torque = (-forces[0] + forces[2]) * self.L
+        torques = np.array([x_torque, y_torque, z_torque])
+        torques = torques - np.cross(rpy_rates, np.dot(self.J, rpy_rates))
+        rpy_rates_deriv = np.dot(self.J_INV, torques)
+        no_pybullet_dyn_accs = force_world_frame / self.M
+        #### Update state ##########################################
+        vel = vel + self.TIMESTEP * no_pybullet_dyn_accs
+        rpy_rates = rpy_rates + self.TIMESTEP * rpy_rates_deriv
+        pos = pos + self.TIMESTEP * vel
+        rpy = rpy + self.TIMESTEP * rpy_rates
+        #### Set PyBullet's state ##################################
+        p.resetBasePositionAndOrientation(self.DRONE_IDS[nth_drone],
+                                          pos,
+                                          p.getQuaternionFromEuler(rpy),
+                                          physicsClientId=self.CLIENT
+                                          )
+        #### Note: the base's velocity only stored and not used ####
+        p.resetBaseVelocity(self.DRONE_IDS[nth_drone],
+                            vel,
+                            [-1, -1, -1], # ang_vel not computed by DYN
+                            physicsClientId=self.CLIENT
+                            )
+        #### Store the roll, pitch, yaw rates for the next step ####
+        self.rpy_rates[nth_drone,:] = rpy_rates
+    
+    ################################################################################
+
+    def _normalizedActionToRPM(self,
+                               action
+                               ):
+        """De-normalizes the [-1, 1] range to the [0, MAX_RPM] range.
+
+        Parameters
+        ----------
+        action : ndarray
+            (4)-shaped array of ints containing an input in the [-1, 1] range.
+
+        Returns
+        -------
+        ndarray
+            (4)-shaped array of ints containing RPMs for the 4 motors in the [0, MAX_RPM] range.
+
+        """
+        if np.any(np.abs(action)) > 1:
+            print("\n[ERROR] it", self.step_counter, "in BaseAviary._normalizedActionToRPM(), out-of-bound action")
+        return np.where(action <= 0, (action+1)*self.HOVER_RPM, action*self.MAX_RPM) # Non-linear mapping: -1 -> 0, 0 -> HOVER_RPM, 1 -> MAX_RPM
+    
+    ################################################################################
+
+    def _saveLastAction(self,
+                        action
+                        ):
+        """Stores the most recent action into attribute `self.last_action`.
+
+        The last action can be used to compute aerodynamic effects.
+        The method disambiguates between array and dict inputs 
+        (for single or multi-agent aviaries, respectively).
+
+        Parameters
+        ----------
+        action : ndarray | dict
+            (4)-shaped array of ints (or dictionary of arrays) containing the current RPMs input.
+
+        """
+        if isinstance(action, collections.Mapping):
+            for k, v in action.items(): 
+                self.last_action[int(k), :] = np.array(v).reshape(4,)
+        else: 
+            self.last_action[int(k), :] = np.array(v).reshape(4,)
+    
+    ################################################################################
+
+    def _showDroneLocalAxes(self,
+                            nth_drone
+                            ):
+        """Draws the local frame of the n-th drone in PyBullet's GUI.
+
+        Parameters
+        ----------
+        nth_drone : int
+            The ordinal number/position of the desired drone in list self.DRONE_IDS.
+
+        """
+        if self.GUI:
+            AXIS_LENGTH = 2*self.L
+            self.X_AX[nth_drone] = p.addUserDebugLine(lineFromXYZ=[0, 0, 0],
+                                                      lineToXYZ=[AXIS_LENGTH, 0, 0],
+                                                      lineColorRGB=[1, 0, 0],
+                                                      parentObjectUniqueId=self.DRONE_IDS[nth_drone],
+                                                      parentLinkIndex=-1,
+                                                      replaceItemUniqueId=int(self.X_AX[nth_drone]),
+                                                      physicsClientId=self.CLIENT
+                                                      )
+            self.Y_AX[nth_drone] = p.addUserDebugLine(lineFromXYZ=[0, 0, 0],
+                                                      lineToXYZ=[0, AXIS_LENGTH, 0],
+                                                      lineColorRGB=[0, 1, 0],
+                                                      parentObjectUniqueId=self.DRONE_IDS[nth_drone],
+                                                      parentLinkIndex=-1,
+                                                      replaceItemUniqueId=int(self.Y_AX[nth_drone]),
+                                                      physicsClientId=self.CLIENT
+                                                      )
+            self.Z_AX[nth_drone] = p.addUserDebugLine(lineFromXYZ=[0, 0, 0],
+                                                      lineToXYZ=[0, 0, AXIS_LENGTH],
+                                                      lineColorRGB=[0, 0, 1],
+                                                      parentObjectUniqueId=self.DRONE_IDS[nth_drone],
+                                                      parentLinkIndex=-1,
+                                                      replaceItemUniqueId=int(self.Z_AX[nth_drone]),
+                                                      physicsClientId=self.CLIENT
+                                                      )
+    
+    ################################################################################
+
+    def _addObstacles(self):
+        """Add obstacles to the environment.
+
+        These obstacles are loaded from standard URDF files included in Bullet.
+
+        """
+        p.loadURDF("samurai.urdf",
+                   physicsClientId=self.CLIENT
+                   )
+        p.loadURDF("duck_vhacd.urdf",
+                   [-.5, -.5, .05],
+                   p.getQuaternionFromEuler([0, 0, 0]),
+                   physicsClientId=self.CLIENT
+                   )
+        p.loadURDF("cube_no_rotation.urdf",
+                   [-.5, -2.5, .5],
+                   p.getQuaternionFromEuler([0, 0, 0]),
+                   physicsClientId=self.CLIENT
+                   )
+        p.loadURDF("sphere2.urdf",
+                   [0, 2, .5],
+                   p.getQuaternionFromEuler([0,0,0]),
+                   physicsClientId=self.CLIENT
+                   )
+    
+    ################################################################################
+    
+    def _parseURDFParameters(self):
+        """Loads parameters from an URDF file.
+
+        This method is nothing more than a custom XML parser for the .urdf
+        files in folder `assets/`.
+
+        """
+        URDF_TREE = etxml.parse(os.path.dirname(os.path.abspath(__file__))+"/../assets/"+self.URDF).getroot()
+        M = float(URDF_TREE[1][0][1].attrib['value'])
+        L = float(URDF_TREE[0].attrib['arm'])
+        THRUST2WEIGHT_RATIO = float(URDF_TREE[0].attrib['thrust2weight'])
+        IXX = float(URDF_TREE[1][0][2].attrib['ixx'])
+        IYY = float(URDF_TREE[1][0][2].attrib['iyy'])
+        IZZ = float(URDF_TREE[1][0][2].attrib['izz'])
+        J = np.diag([IXX, IYY, IZZ])
+        J_INV = np.linalg.inv(J)
+        KF = float(URDF_TREE[0].attrib['kf'])
+        KM = float(URDF_TREE[0].attrib['km'])
+        COLLISION_H = float(URDF_TREE[1][2][1][0].attrib['length'])
+        COLLISION_R = float(URDF_TREE[1][2][1][0].attrib['radius'])
+        COLLISION_SHAPE_OFFSETS = [float(s) for s in URDF_TREE[1][2][0].attrib['xyz'].split(' ')]
+        COLLISION_Z_OFFSET = COLLISION_SHAPE_OFFSETS[2]
+        MAX_SPEED_KMH = float(URDF_TREE[0].attrib['max_speed_kmh'])
+        GND_EFF_COEFF = float(URDF_TREE[0].attrib['gnd_eff_coeff'])
+        PROP_RADIUS = float(URDF_TREE[0].attrib['prop_radius'])
+        DRAG_COEFF_XY = float(URDF_TREE[0].attrib['drag_coeff_xy'])
+        DRAG_COEFF_Z = float(URDF_TREE[0].attrib['drag_coeff_z'])
+        DRAG_COEFF = np.array([DRAG_COEFF_XY, DRAG_COEFF_XY, DRAG_COEFF_Z])
+        DW_COEFF_1 = float(URDF_TREE[0].attrib['dw_coeff_1'])
+        DW_COEFF_2 = float(URDF_TREE[0].attrib['dw_coeff_2'])
+        DW_COEFF_3 = float(URDF_TREE[0].attrib['dw_coeff_3'])
+        return M, L, THRUST2WEIGHT_RATIO, J, J_INV, KF, KM, COLLISION_H, COLLISION_R, COLLISION_Z_OFFSET, MAX_SPEED_KMH, \
+               GND_EFF_COEFF, PROP_RADIUS, DRAG_COEFF, DW_COEFF_1, DW_COEFF_2, DW_COEFF_3
+    
+    ################################################################################
+    
+    def _actionSpace(self):
+        """Returns the action space of the environment.
+
+        Must be implemented in a subclass.
+
+        """
+        raise NotImplementedError
+    
+    ################################################################################
+
+    def _observationSpace(self):
+        """Returns the observation space of the environment.
+
+        Must be implemented in a subclass.
+
+        """
+        raise NotImplementedError
+    
+    ################################################################################
+    
+    def _computeObs(self):
+        """Returns the current observation of the environment.
+
+        Must be implemented in a subclass.
+
+        """
+        raise NotImplementedError
+    
+    ################################################################################
+
+    def _preprocessAction(self,
+                          action
+                          ):
+        """Pre-processes the action passed to `.step()` into motors' RPMs.
+
+        Must be implemented in a subclass.
+
+        Parameters
+        ----------
+        action : ndarray | dict[..]
+            The input action for one or more drones, to be translated into RPMs.
+
+        """
+        raise NotImplementedError
+
+    ################################################################################
+
+    def _computeReward(self):
+        """Computes the current reward value(s).
+
+        Must be implemented in a subclass.
+
+        """
+        raise NotImplementedError
+
+    ################################################################################
+
+    def _computeDone(self):
+        """Computes the current done value(s).
+
+        Must be implemented in a subclass.
+
+        """
+        raise NotImplementedError
+
+    def _computeTruncated(self):
+        """Computes the current done value(s).
+
+        Must be implemented in a subclass.
+
+        """
+        raise NotImplementedError
+    ################################################################################
+
+    def _computeInfo(self):
+        """Computes the current info dict(s).
+
+        Must be implemented in a subclass.
+
+        """
+        raise NotImplementedError
diff --git a/gym_pybullet_drones/envs/multi_agent_rl/LeaderFollowerAviary.py b/gym_pybullet_drones/envs/multi_agent_rl/LeaderFollowerAviary.py
index cab8ed3..fa999e1 100644
--- a/gym_pybullet_drones/envs/multi_agent_rl/LeaderFollowerAviary.py
+++ b/gym_pybullet_drones/envs/multi_agent_rl/LeaderFollowerAviary.py
@@ -81,30 +81,45 @@ class LeaderFollowerAviary(BaseMultiagentAviary):
     
     def _computeReward(self):
         """
-            Computes the reward for each drone in the environment.
-
-            The reward is based on the distance of each drone from its target position:
-            - The leader drone is rewarded based on how close it is to a target position.
-            - The follower drones are rewarded based on how close they are to their respective target positions.
-
-            Returns:
-            --------
-            dict[int, float]
-                A dictionary where keys are drone indices and values are the computed rewards.
+        Computes rewards for each drone based on their distances to target positions.
+        Uses reward shaping weights for leader and followers.
         """
         rewards = {}
         states = np.array([self._getDroneStateVector(i) for i in range(self.NUM_DRONES)])
 
-        # Reward for the leader drone
-        rewards[0] = -1 * np.linalg.norm(np.array([0, 0, 0.5]) - states[0, 0:3])**2
+        reward_shaping = self.get_default_reward_shaping()
+        leader_weight = reward_shaping["leader_target_penalty"]
+        follower_weight = reward_shaping["follower_target_penalty"]
+
+        leader_target = np.array([0, 0, 1])
+        rewards[0] = -1 * np.linalg.norm(np.array([0, 0, 1]) - states[0, 0:3])**2
 
-        # rewards[1] = -1 * np.linalg.norm(np.array([states[1, 0], states[1, 1], 0.5]) - states[1, 0:3])**2 # DEBUG WITH INDEPENDENT REWARD 
-        
-        # Reward for follower drones
         for i in range(1, self.NUM_DRONES):
             rewards[i] = -(1/self.NUM_DRONES) * np.linalg.norm(np.array([states[i, 0], states[i, 1], states[0, 2]]) - states[i, 0:3])**2
         return rewards
 
+    def _compute_rewards(self, obs_0, obs_1):
+        """
+        Compute rewards for the leader and follower drones.
+
+        Parameters:
+        - obs_0: ndarray -> Observation of drone 0 (leader).
+        - obs_1: ndarray -> Observation of drone 1 (follower).
+
+        Returns:
+        - rewards: list -> A list of rewards for each drone [reward_0, reward_1].
+        """
+        pos_0 = obs_0[0:3]  # Leader position
+        pos_1 = obs_1[0:3]  # Follower position
+
+        # Target position for leader drone (example: at [0, 0, 0.5])
+        leader_target = np.array([0, 0, 1])
+
+        reward_0 = 1.0 / (1.0 + np.linalg.norm(leader_target - pos_0))  
+        reward_1 = 1.0 / (1.0 + np.linalg.norm(pos_0 - pos_1)) 
+
+        return [reward_0, reward_1]
+
     ################################################################################
     
     def _computeDone(self):
@@ -129,19 +144,22 @@ class LeaderFollowerAviary(BaseMultiagentAviary):
     
     def _computeInfo(self):
         """
-            Returns additional information about the current state of the environment.
-            
-            Currently not used, but a placeholder for future implementation.
+        Returns additional information about the current state of the environment.
 
-            Returns:
-            --------
-            dict[int, dict]
-                A dictionary where keys are drone indices and the values are empty dictionaries (unused).
+        Returns:
+        --------
+        dict[int, dict]
+            A dictionary where keys are drone indices (agent IDs) and values are dictionaries 
+            containing agent-specific information.
         """
-        return {i: {} for i in range(self.NUM_DRONES)}
+        return {
+            i: {"is_active": True} for i in range(self.NUM_DRONES)
+        }
 
-    ################################################################################
 
+    ################################################################################
+    def _computeTruncated(self):       
+        return {i: False for i in range(self.NUM_DRONES)}
     def _clipAndNormalizeState(self, state):
         """
             Normalizes and clips the state of a drone to ensure it falls within predefined limits.
